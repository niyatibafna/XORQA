/usr/bin/python3: Error while finding spec for 'torch.distributed.launch' (ImportError: No module named 'torch')
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 1; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 256, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/CE/nbafna/anaconda3/envs/story_recom/bin/python3', '-u', '../baselines/DPR/train_dense_encoder.py', '--local_rank=1', '--max_grad_norm', '2.0', '--encoder_model_type', 'hf_bert', '--pretrained_model_cfg', 'bert-base-multilingual-uncased', '--model_file', '../biencoders/pretrained/dpr_biencoder_B', '--seed', '12345', '--sequence_length', '256', '--warmup_steps', '1237', '--batch_size', '12', '--do_lower_case', '--train_file', '../data/NQ/biencoder-nq-train.json', '--dev_file', '../data/NQ/biencoder-nq-dev.json', '--output_dir', '../biencoders/trained/B+A/', '--learning_rate', '2e-05', '--num_train_epochs', '40', '--dev_batch_size', '12', '--val_av_rank_start_epoch', '10', '--restart']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 1; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 256, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/CE/nbafna/anaconda3/envs/story_recom/bin/python3', '-u', '../baselines/DPR/train_dense_encoder.py', '--local_rank=1', '--max_grad_norm', '2.0', '--encoder_model_type', 'hf_bert', '--pretrained_model_cfg', 'bert-base-multilingual-uncased', '--model_file', '../biencoders/pretrained/dpr_biencoder_B', '--seed', '12345', '--sequence_length', '256', '--warmup_steps', '1237', '--batch_size', '12', '--do_lower_case', '--train_file', '../data/NQ/biencoder-nq-train.json', '--dev_file', '../data/NQ/biencoder-nq-dev.json', '--output_dir', '../biencoders/trained/B+A/', '--learning_rate', '2e-05', '--num_train_epochs', '40', '--dev_batch_size', '12', '--val_av_rank_start_epoch', '10', '--restart']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
16-bits training: False 
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
***** Initializing components for training *****
adam_eps                       -->   1e-08
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 1; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 256, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/CE/nbafna/anaconda3/envs/story_recom/bin/python3', '-u', '../baselines/DPR/train_dense_encoder.py', '--local_rank=1', '--max_grad_norm', '2.0', '--encoder_model_type', 'hf_bert', '--pretrained_model_cfg', 'bert-base-multilingual-uncased', '--model_file', '../biencoders/pretrained/dpr_biencoder_B', '--seed', '12345', '--sequence_length', '256', '--warmup_steps', '1237', '--batch_size', '12', '--do_lower_case', '--train_file', '../data/NQ/biencoder-nq-train.json', '--dev_file', '../data/NQ/biencoder-nq-dev.json', '--output_dir', '../biencoders/trained/B+A/', '--learning_rate', '2e-05', '--num_train_epochs', '40', '--dev_batch_size', '12', '--val_av_rank_start_epoch', '10', '--restart']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 1; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 256, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/CE/nbafna/anaconda3/envs/story_recom/bin/python3', '-u', '../baselines/DPR/train_dense_encoder.py', '--local_rank=1', '--max_grad_norm', '2.0', '--encoder_model_type', 'hf_bert', '--pretrained_model_cfg', 'bert-base-multilingual-uncased', '--model_file', '../biencoders/pretrained/dpr_biencoder_B', '--seed', '12345', '--sequence_length', '256', '--warmup_steps', '1237', '--batch_size', '12', '--do_lower_case', '--train_file', '../data/NQ/biencoder-nq-train.json', '--dev_file', '../data/NQ/biencoder-nq-dev.json', '--output_dir', '../biencoders/trained/B+A/', '--learning_rate', '2e-05', '--num_train_epochs', '40', '--dev_batch_size', '12', '--val_av_rank_start_epoch', '10', '--restart']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   6
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   6
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   6
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   6
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=4907
 Total updates=196280
  Eval step = 4907
***** Training *****
***** Epoch 0 *****
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=4907
 Total updates=196280
  Eval step = 4907
***** Training *****
***** Epoch 0 *****
Epoch: 0: Step: 1/4907, loss=4.017465, lr=0.000000
Epoch: 0: Step: 1/4907, loss=4.017465, lr=0.000000
Train batch 100
Avg. loss per last 100 batches: 3.069843
Train batch 100
Avg. loss per last 100 batches: 3.069843
Epoch: 0: Step: 101/4907, loss=2.570882, lr=0.000002
Epoch: 0: Step: 101/4907, loss=2.570882, lr=0.000002
Train batch 200
Avg. loss per last 100 batches: 1.891513
Train batch 200
Avg. loss per last 100 batches: 1.891513
Epoch: 0: Step: 201/4907, loss=3.002283, lr=0.000003
Epoch: 0: Step: 201/4907, loss=3.002283, lr=0.000003
Train batch 300
Avg. loss per last 100 batches: 1.091184
Train batch 300
Avg. loss per last 100 batches: 1.091184
Epoch: 0: Step: 301/4907, loss=1.667616, lr=0.000005
Epoch: 0: Step: 301/4907, loss=1.667616, lr=0.000005
Train batch 400
Avg. loss per last 100 batches: 0.762433
Train batch 400
Avg. loss per last 100 batches: 0.762433
Epoch: 0: Step: 401/4907, loss=0.057895, lr=0.000006
Epoch: 0: Step: 401/4907, loss=0.057895, lr=0.000006
Train batch 500
Avg. loss per last 100 batches: 0.670715
Train batch 500
Avg. loss per last 100 batches: 0.670715
Epoch: 0: Step: 501/4907, loss=0.748637, lr=0.000008
Epoch: 0: Step: 501/4907, loss=0.748637, lr=0.000008
Train batch 600
Avg. loss per last 100 batches: 0.561317
Train batch 600
Avg. loss per last 100 batches: 0.561317
Epoch: 0: Step: 601/4907, loss=0.506405, lr=0.000010
Epoch: 0: Step: 601/4907, loss=0.506405, lr=0.000010
Train batch 700
Avg. loss per last 100 batches: 0.518331
Train batch 700
Avg. loss per last 100 batches: 0.518331
Epoch: 0: Step: 701/4907, loss=0.313789, lr=0.000011
Epoch: 0: Step: 701/4907, loss=0.313789, lr=0.000011
Train batch 800
Avg. loss per last 100 batches: 0.438257
Train batch 800
Avg. loss per last 100 batches: 0.438257
Epoch: 0: Step: 801/4907, loss=0.160320, lr=0.000013
Epoch: 0: Step: 801/4907, loss=0.160320, lr=0.000013
Train batch 900
Avg. loss per last 100 batches: 0.392031
Train batch 900
Avg. loss per last 100 batches: 0.392031
Epoch: 0: Step: 901/4907, loss=0.186262, lr=0.000015
Epoch: 0: Step: 901/4907, loss=0.186262, lr=0.000015
Train batch 1000
Avg. loss per last 100 batches: 0.407528
Train batch 1000
Avg. loss per last 100 batches: 0.407528
Epoch: 0: Step: 1001/4907, loss=0.228110, lr=0.000016
Epoch: 0: Step: 1001/4907, loss=0.228110, lr=0.000016
Train batch 1100
Avg. loss per last 100 batches: 0.371221
Train batch 1100
Avg. loss per last 100 batches: 0.371221
Epoch: 0: Step: 1101/4907, loss=0.075700, lr=0.000018
Epoch: 0: Step: 1101/4907, loss=0.075700, lr=0.000018
Train batch 1200
Avg. loss per last 100 batches: 0.327214
Train batch 1200
Avg. loss per last 100 batches: 0.327214
Epoch: 0: Step: 1201/4907, loss=0.375097, lr=0.000019
Epoch: 0: Step: 1201/4907, loss=0.375097, lr=0.000019
Train batch 1300
Avg. loss per last 100 batches: 0.360195
Train batch 1300
Avg. loss per last 100 batches: 0.360195
Epoch: 0: Step: 1301/4907, loss=1.685537, lr=0.000020
Epoch: 0: Step: 1301/4907, loss=1.685537, lr=0.000020
Train batch 1400
Avg. loss per last 100 batches: 0.370966
Train batch 1400
Avg. loss per last 100 batches: 0.370966
Epoch: 0: Step: 1401/4907, loss=0.022527, lr=0.000020
Epoch: 0: Step: 1401/4907, loss=0.022527, lr=0.000020
Train batch 1500
Avg. loss per last 100 batches: 0.309958
Train batch 1500
Avg. loss per last 100 batches: 0.309958
Epoch: 0: Step: 1501/4907, loss=0.430437, lr=0.000020
Epoch: 0: Step: 1501/4907, loss=0.430437, lr=0.000020
Train batch 1600
Avg. loss per last 100 batches: 0.391048
Train batch 1600
Avg. loss per last 100 batches: 0.391048
Epoch: 0: Step: 1601/4907, loss=0.556172, lr=0.000020
Epoch: 0: Step: 1601/4907, loss=0.556172, lr=0.000020
Train batch 1700
Avg. loss per last 100 batches: 0.357360
Train batch 1700
Avg. loss per last 100 batches: 0.357360
Epoch: 0: Step: 1701/4907, loss=0.567616, lr=0.000020
Epoch: 0: Step: 1701/4907, loss=0.567616, lr=0.000020
Train batch 1800
Avg. loss per last 100 batches: 0.332552
Train batch 1800
Avg. loss per last 100 batches: 0.332552
Epoch: 0: Step: 1801/4907, loss=0.077064, lr=0.000020
Epoch: 0: Step: 1801/4907, loss=0.077064, lr=0.000020
Train batch 1900
Avg. loss per last 100 batches: 0.323447
Train batch 1900
Avg. loss per last 100 batches: 0.323447
Epoch: 0: Step: 1901/4907, loss=0.194578, lr=0.000020
Epoch: 0: Step: 1901/4907, loss=0.194578, lr=0.000020
Train batch 2000
Avg. loss per last 100 batches: 0.302242
Train batch 2000
Avg. loss per last 100 batches: 0.302242
Epoch: 0: Step: 2001/4907, loss=0.318219, lr=0.000020
Epoch: 0: Step: 2001/4907, loss=0.318219, lr=0.000020
Train batch 2100
Avg. loss per last 100 batches: 0.300839
Train batch 2100
Avg. loss per last 100 batches: 0.300839
Epoch: 0: Step: 2101/4907, loss=0.251361, lr=0.000020
Epoch: 0: Step: 2101/4907, loss=0.251361, lr=0.000020
Train batch 2200
Avg. loss per last 100 batches: 0.272917
Train batch 2200
Avg. loss per last 100 batches: 0.272917
Epoch: 0: Step: 2201/4907, loss=0.256415, lr=0.000020
Epoch: 0: Step: 2201/4907, loss=0.256415, lr=0.000020
Train batch 2300
Avg. loss per last 100 batches: 0.290550
Train batch 2300
Avg. loss per last 100 batches: 0.290550
Epoch: 0: Step: 2301/4907, loss=0.183895, lr=0.000020
Epoch: 0: Step: 2301/4907, loss=0.183895, lr=0.000020
Train batch 2400
Avg. loss per last 100 batches: 0.295779
Train batch 2400
Avg. loss per last 100 batches: 0.295779
Epoch: 0: Step: 2401/4907, loss=0.358316, lr=0.000020
Epoch: 0: Step: 2401/4907, loss=0.358316, lr=0.000020
Train batch 2500
Avg. loss per last 100 batches: 0.276079
Train batch 2500
Avg. loss per last 100 batches: 0.276079
Epoch: 0: Step: 2501/4907, loss=0.020416, lr=0.000020
Epoch: 0: Step: 2501/4907, loss=0.020416, lr=0.000020
Train batch 2600
Avg. loss per last 100 batches: 0.265472
Train batch 2600
Avg. loss per last 100 batches: 0.265472
Epoch: 0: Step: 2601/4907, loss=0.395620, lr=0.000020
Epoch: 0: Step: 2601/4907, loss=0.395620, lr=0.000020
Train batch 2700
Avg. loss per last 100 batches: 0.331890
Train batch 2700
Avg. loss per last 100 batches: 0.331890
Epoch: 0: Step: 2701/4907, loss=0.070595, lr=0.000020
Epoch: 0: Step: 2701/4907, loss=0.070595, lr=0.000020
Train batch 2800
Avg. loss per last 100 batches: 0.270311
Train batch 2800
Avg. loss per last 100 batches: 0.270311
Epoch: 0: Step: 2801/4907, loss=0.271623, lr=0.000020
Epoch: 0: Step: 2801/4907, loss=0.271623, lr=0.000020
Train batch 2900
Avg. loss per last 100 batches: 0.287199
Train batch 2900
Avg. loss per last 100 batches: 0.287199
Epoch: 0: Step: 2901/4907, loss=1.242602, lr=0.000020
Epoch: 0: Step: 2901/4907, loss=1.242602, lr=0.000020
Train batch 3000
Avg. loss per last 100 batches: 0.248387
Train batch 3000
Avg. loss per last 100 batches: 0.248387
Epoch: 0: Step: 3001/4907, loss=0.696428, lr=0.000020
Epoch: 0: Step: 3001/4907, loss=0.696428, lr=0.000020
Train batch 3100
Avg. loss per last 100 batches: 0.259377
Train batch 3100
Avg. loss per last 100 batches: 0.259377
Epoch: 0: Step: 3101/4907, loss=0.037212, lr=0.000020
Epoch: 0: Step: 3101/4907, loss=0.037212, lr=0.000020
Train batch 3200
Avg. loss per last 100 batches: 0.260392
Train batch 3200
Avg. loss per last 100 batches: 0.260392
Epoch: 0: Step: 3201/4907, loss=0.031090, lr=0.000020
Epoch: 0: Step: 3201/4907, loss=0.031090, lr=0.000020
Train batch 3300
Avg. loss per last 100 batches: 0.252486
Train batch 3300
Avg. loss per last 100 batches: 0.252486
Epoch: 0: Step: 3301/4907, loss=0.574335, lr=0.000020
Epoch: 0: Step: 3301/4907, loss=0.574335, lr=0.000020
Train batch 3400
Avg. loss per last 100 batches: 0.272031
Train batch 3400
Avg. loss per last 100 batches: 0.272031
Epoch: 0: Step: 3401/4907, loss=0.304338, lr=0.000020
Epoch: 0: Step: 3401/4907, loss=0.304338, lr=0.000020
Train batch 3500
Avg. loss per last 100 batches: 0.246026
Train batch 3500
Avg. loss per last 100 batches: 0.246026
Epoch: 0: Step: 3501/4907, loss=0.126953, lr=0.000020
Epoch: 0: Step: 3501/4907, loss=0.126953, lr=0.000020
Train batch 3600
Avg. loss per last 100 batches: 0.230955
Train batch 3600
Avg. loss per last 100 batches: 0.230955
Epoch: 0: Step: 3601/4907, loss=0.253241, lr=0.000020
Epoch: 0: Step: 3601/4907, loss=0.253241, lr=0.000020
Train batch 3700
Avg. loss per last 100 batches: 0.269894
Train batch 3700
Avg. loss per last 100 batches: 0.269894
Epoch: 0: Step: 3701/4907, loss=0.625531, lr=0.000020
Epoch: 0: Step: 3701/4907, loss=0.625531, lr=0.000020
Train batch 3800
Train batch 3800
Avg. loss per last 100 batches: 0.258112
Avg. loss per last 100 batches: 0.258112
Epoch: 0: Step: 3801/4907, loss=0.008255, lr=0.000020
Epoch: 0: Step: 3801/4907, loss=0.008255, lr=0.000020
Train batch 3900
Avg. loss per last 100 batches: 0.269410
Train batch 3900
Avg. loss per last 100 batches: 0.269410
Epoch: 0: Step: 3901/4907, loss=0.162754, lr=0.000020
Epoch: 0: Step: 3901/4907, loss=0.162754, lr=0.000020
Train batch 4000
Avg. loss per last 100 batches: 0.270165
Train batch 4000
Avg. loss per last 100 batches: 0.270165
Epoch: 0: Step: 4001/4907, loss=0.414500, lr=0.000020
Epoch: 0: Step: 4001/4907, loss=0.414500, lr=0.000020
Train batch 4100
Avg. loss per last 100 batches: 0.274964
Train batch 4100
Avg. loss per last 100 batches: 0.274964
Epoch: 0: Step: 4101/4907, loss=0.030356, lr=0.000020
Epoch: 0: Step: 4101/4907, loss=0.030356, lr=0.000020
Train batch 4200
Avg. loss per last 100 batches: 0.219227
Train batch 4200
Avg. loss per last 100 batches: 0.219227
Epoch: 0: Step: 4201/4907, loss=0.061458, lr=0.000020
Epoch: 0: Step: 4201/4907, loss=0.061458, lr=0.000020
Train batch 4300
Avg. loss per last 100 batches: 0.264812
Train batch 4300
Avg. loss per last 100 batches: 0.264812
Epoch: 0: Step: 4301/4907, loss=0.342663, lr=0.000020
Epoch: 0: Step: 4301/4907, loss=0.342663, lr=0.000020
Train batch 4400
Avg. loss per last 100 batches: 0.205112
Train batch 4400
Avg. loss per last 100 batches: 0.205112
Epoch: 0: Step: 4401/4907, loss=0.049997, lr=0.000020
Epoch: 0: Step: 4401/4907, loss=0.049997, lr=0.000020
Train batch 4500
Avg. loss per last 100 batches: 0.292821
Train batch 4500
Avg. loss per last 100 batches: 0.292821
Epoch: 0: Step: 4501/4907, loss=0.194680, lr=0.000020
Epoch: 0: Step: 4501/4907, loss=0.194680, lr=0.000020
Train batch 4600
Avg. loss per last 100 batches: 0.238678
Train batch 4600
Avg. loss per last 100 batches: 0.238678
Epoch: 0: Step: 4601/4907, loss=0.343834, lr=0.000020
Epoch: 0: Step: 4601/4907, loss=0.343834, lr=0.000020
Train batch 4700
Avg. loss per last 100 batches: 0.239998
Train batch 4700
Avg. loss per last 100 batches: 0.239998
Epoch: 0: Step: 4701/4907, loss=0.055941, lr=0.000020
Epoch: 0: Step: 4701/4907, loss=0.055941, lr=0.000020
Train batch 4800
Avg. loss per last 100 batches: 0.280769
Train batch 4800
Avg. loss per last 100 batches: 0.280769
Epoch: 0: Step: 4801/4907, loss=0.031332, lr=0.000020
Epoch: 0: Step: 4801/4907, loss=0.031332, lr=0.000020
Train batch 4900
Avg. loss per last 100 batches: 0.231245
Train batch 4900
Avg. loss per last 100 batches: 0.231245
Epoch: 0: Step: 4901/4907, loss=0.142841, lr=0.000020
Epoch: 0: Step: 4901/4907, loss=0.142841, lr=0.000020
Validation: Epoch: 0 Step: 4907/4907
NLL validation ...
Validation: Epoch: 0 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.382221 sec., loss=0.136667 
Eval step: 99 , used_time=28.478279 sec., loss=0.136667 
Eval step: 199 , used_time=57.527405 sec., loss=0.107963 
Eval step: 199 , used_time=57.431280 sec., loss=0.107963 
Eval step: 299 , used_time=86.464754 sec., loss=0.710855 
Eval step: 299 , used_time=86.560935 sec., loss=0.710855 
Eval step: 399 , used_time=115.580472 sec., loss=0.194009 
Eval step: 399 , used_time=115.484415 sec., loss=0.194009 
Eval step: 499 , used_time=144.562332 sec., loss=0.030770 
Eval step: 499 , used_time=144.466206 sec., loss=0.030770 
NLL Validation: loss = 0.442775. correct prediction ratio  5534/6516 ~  0.849294
NLL Validation: loss = 0.442775. correct prediction ratio  5534/6516 ~  0.849294
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.
  warnings.warn(SAVE_STATE_WARNING, UserWarning)
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.0.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.0.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.0.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=37.166745 sec., loss=0.136667 
Eval step: 99 , used_time=27.652208 sec., loss=0.136667 
Eval step: 199 , used_time=56.626096 sec., loss=0.107963 
Eval step: 199 , used_time=66.140613 sec., loss=0.107963 
Eval step: 299 , used_time=85.609547 sec., loss=0.710855 
Eval step: 299 , used_time=95.124134 sec., loss=0.710855 
Eval step: 399 , used_time=124.126125 sec., loss=0.194009 
Eval step: 399 , used_time=114.611578 sec., loss=0.194009 
Eval step: 499 , used_time=143.638715 sec., loss=0.030770 
Eval step: 499 , used_time=153.153268 sec., loss=0.030770 
NLL Validation: loss = 0.442775. correct prediction ratio  5534/6516 ~  0.849294
NLL Validation: loss = 0.442775. correct prediction ratio  5534/6516 ~  0.849294
Av Loss per epoch=0.426650
epoch total correct predictions=51744
***** Epoch 1 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.0.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.0.4907
Av Loss per epoch=0.426650
epoch total correct predictions=51744
***** Epoch 1 *****
Epoch: 1: Step: 1/4907, loss=0.001258, lr=0.000020
Epoch: 1: Step: 1/4907, loss=0.001258, lr=0.000020
Train batch 100
Train batch 100
Avg. loss per last 100 batches: 0.179459
Avg. loss per last 100 batches: 0.179459
Epoch: 1: Step: 101/4907, loss=0.000992, lr=0.000020
Epoch: 1: Step: 101/4907, loss=0.000992, lr=0.000020
Train batch 200
Avg. loss per last 100 batches: 0.159920
Train batch 200
Avg. loss per last 100 batches: 0.159920
Epoch: 1: Step: 201/4907, loss=0.027029, lr=0.000020
Epoch: 1: Step: 201/4907, loss=0.027029, lr=0.000020
Train batch 300
Avg. loss per last 100 batches: 0.190505
Train batch 300
Avg. loss per last 100 batches: 0.190505
Epoch: 1: Step: 301/4907, loss=0.008841, lr=0.000020
Epoch: 1: Step: 301/4907, loss=0.008841, lr=0.000020
Train batch 400
Avg. loss per last 100 batches: 0.194797
Train batch 400
Avg. loss per last 100 batches: 0.194797
Epoch: 1: Step: 401/4907, loss=0.404120, lr=0.000020
Epoch: 1: Step: 401/4907, loss=0.404120, lr=0.000020
Train batch 500
Avg. loss per last 100 batches: 0.168945
Train batch 500
Avg. loss per last 100 batches: 0.168945
Epoch: 1: Step: 501/4907, loss=0.289099, lr=0.000020
Epoch: 1: Step: 501/4907, loss=0.289099, lr=0.000020
Train batch 600
Avg. loss per last 100 batches: 0.197387
Train batch 600
Avg. loss per last 100 batches: 0.197387
Epoch: 1: Step: 601/4907, loss=0.193249, lr=0.000020
Epoch: 1: Step: 601/4907, loss=0.193249, lr=0.000020
Train batch 700
Avg. loss per last 100 batches: 0.223751
Train batch 700
Avg. loss per last 100 batches: 0.223751
Epoch: 1: Step: 701/4907, loss=0.401367, lr=0.000020
Epoch: 1: Step: 701/4907, loss=0.401367, lr=0.000020
Train batch 800
Avg. loss per last 100 batches: 0.176679
Train batch 800
Avg. loss per last 100 batches: 0.176679
Epoch: 1: Step: 801/4907, loss=0.041093, lr=0.000020
Epoch: 1: Step: 801/4907, loss=0.041093, lr=0.000020
Train batch 900
Avg. loss per last 100 batches: 0.205234
Train batch 900
Avg. loss per last 100 batches: 0.205234
Epoch: 1: Step: 901/4907, loss=0.020002, lr=0.000020
Epoch: 1: Step: 901/4907, loss=0.020002, lr=0.000020
Train batch 1000
Avg. loss per last 100 batches: 0.156478
Train batch 1000
Avg. loss per last 100 batches: 0.156478
Epoch: 1: Step: 1001/4907, loss=0.005093, lr=0.000020
Epoch: 1: Step: 1001/4907, loss=0.005093, lr=0.000020
Train batch 1100
Avg. loss per last 100 batches: 0.201711
Train batch 1100
Avg. loss per last 100 batches: 0.201711
Epoch: 1: Step: 1101/4907, loss=0.097390, lr=0.000020
Epoch: 1: Step: 1101/4907, loss=0.097390, lr=0.000020
Train batch 1200
Avg. loss per last 100 batches: 0.174939
Train batch 1200
Avg. loss per last 100 batches: 0.174939
Epoch: 1: Step: 1201/4907, loss=0.405383, lr=0.000020
Epoch: 1: Step: 1201/4907, loss=0.405383, lr=0.000020
Train batch 1300
Avg. loss per last 100 batches: 0.167789
Train batch 1300
Avg. loss per last 100 batches: 0.167789
Epoch: 1: Step: 1301/4907, loss=0.006983, lr=0.000019
Epoch: 1: Step: 1301/4907, loss=0.006983, lr=0.000019
Train batch 1400
Avg. loss per last 100 batches: 0.193839
Train batch 1400
Avg. loss per last 100 batches: 0.193839
Epoch: 1: Step: 1401/4907, loss=0.333481, lr=0.000019
Epoch: 1: Step: 1401/4907, loss=0.333481, lr=0.000019
Train batch 1500
Avg. loss per last 100 batches: 0.193488
Train batch 1500
Avg. loss per last 100 batches: 0.193488
Epoch: 1: Step: 1501/4907, loss=0.459582, lr=0.000019
Epoch: 1: Step: 1501/4907, loss=0.459582, lr=0.000019
Train batch 1600
Avg. loss per last 100 batches: 0.177727
Train batch 1600
Avg. loss per last 100 batches: 0.177727
Epoch: 1: Step: 1601/4907, loss=0.063317, lr=0.000019
Epoch: 1: Step: 1601/4907, loss=0.063317, lr=0.000019
Train batch 1700
Avg. loss per last 100 batches: 0.200063
Train batch 1700
Avg. loss per last 100 batches: 0.200063
Epoch: 1: Step: 1701/4907, loss=0.005366, lr=0.000019
Epoch: 1: Step: 1701/4907, loss=0.005366, lr=0.000019
Train batch 1800
Avg. loss per last 100 batches: 0.172874
Train batch 1800
Avg. loss per last 100 batches: 0.172874
Epoch: 1: Step: 1801/4907, loss=0.231990, lr=0.000019
Epoch: 1: Step: 1801/4907, loss=0.231990, lr=0.000019
Train batch 1900
Avg. loss per last 100 batches: 0.196664
Train batch 1900
Avg. loss per last 100 batches: 0.196664
Epoch: 1: Step: 1901/4907, loss=0.151664, lr=0.000019
Epoch: 1: Step: 1901/4907, loss=0.151664, lr=0.000019
Train batch 2000
Avg. loss per last 100 batches: 0.174421
Train batch 2000
Avg. loss per last 100 batches: 0.174421
Epoch: 1: Step: 2001/4907, loss=0.040840, lr=0.000019
Epoch: 1: Step: 2001/4907, loss=0.040840, lr=0.000019
Train batch 2100
Avg. loss per last 100 batches: 0.186479
Train batch 2100
Avg. loss per last 100 batches: 0.186479
Epoch: 1: Step: 2101/4907, loss=0.108022, lr=0.000019
Epoch: 1: Step: 2101/4907, loss=0.108022, lr=0.000019
Train batch 2200
Avg. loss per last 100 batches: 0.215635
Train batch 2200
Avg. loss per last 100 batches: 0.215635
Epoch: 1: Step: 2201/4907, loss=0.197594, lr=0.000019
Epoch: 1: Step: 2201/4907, loss=0.197594, lr=0.000019
Train batch 2300
Avg. loss per last 100 batches: 0.146376
Train batch 2300
Avg. loss per last 100 batches: 0.146376
Epoch: 1: Step: 2301/4907, loss=0.030966, lr=0.000019
Epoch: 1: Step: 2301/4907, loss=0.030966, lr=0.000019
Train batch 2400
Avg. loss per last 100 batches: 0.232441
Train batch 2400
Avg. loss per last 100 batches: 0.232441
Epoch: 1: Step: 2401/4907, loss=0.598759, lr=0.000019
Epoch: 1: Step: 2401/4907, loss=0.598759, lr=0.000019
Train batch 2500
Avg. loss per last 100 batches: 0.159647
Train batch 2500
Avg. loss per last 100 batches: 0.159647
Epoch: 1: Step: 2501/4907, loss=0.023987, lr=0.000019
Epoch: 1: Step: 2501/4907, loss=0.023987, lr=0.000019
Train batch 2600
Avg. loss per last 100 batches: 0.220390
Train batch 2600
Avg. loss per last 100 batches: 0.220390
Epoch: 1: Step: 2601/4907, loss=0.047156, lr=0.000019
Epoch: 1: Step: 2601/4907, loss=0.047156, lr=0.000019
Train batch 2700
Avg. loss per last 100 batches: 0.176452
Train batch 2700
Avg. loss per last 100 batches: 0.176452
Epoch: 1: Step: 2701/4907, loss=0.061378, lr=0.000019
Epoch: 1: Step: 2701/4907, loss=0.061378, lr=0.000019
Train batch 2800
Avg. loss per last 100 batches: 0.196884
Train batch 2800
Avg. loss per last 100 batches: 0.196884
Epoch: 1: Step: 2801/4907, loss=0.456783, lr=0.000019
Epoch: 1: Step: 2801/4907, loss=0.456783, lr=0.000019
Train batch 2900
Avg. loss per last 100 batches: 0.214743
Train batch 2900
Avg. loss per last 100 batches: 0.214743
Epoch: 1: Step: 2901/4907, loss=0.076379, lr=0.000019
Epoch: 1: Step: 2901/4907, loss=0.076379, lr=0.000019
Train batch 3000
Train batch 3000
Avg. loss per last 100 batches: 0.224168
Avg. loss per last 100 batches: 0.224168
Epoch: 1: Step: 3001/4907, loss=0.383368, lr=0.000019
Epoch: 1: Step: 3001/4907, loss=0.383368, lr=0.000019
Train batch 3100
Avg. loss per last 100 batches: 0.188676
Train batch 3100
Avg. loss per last 100 batches: 0.188676
Epoch: 1: Step: 3101/4907, loss=0.028601, lr=0.000019
Epoch: 1: Step: 3101/4907, loss=0.028601, lr=0.000019
Train batch 3200
Avg. loss per last 100 batches: 0.185303
Train batch 3200
Avg. loss per last 100 batches: 0.185303
Epoch: 1: Step: 3201/4907, loss=0.043866, lr=0.000019
Epoch: 1: Step: 3201/4907, loss=0.043866, lr=0.000019
Train batch 3300
Avg. loss per last 100 batches: 0.172942
Train batch 3300
Avg. loss per last 100 batches: 0.172942
Epoch: 1: Step: 3301/4907, loss=0.475438, lr=0.000019
Epoch: 1: Step: 3301/4907, loss=0.475438, lr=0.000019
Train batch 3400
Avg. loss per last 100 batches: 0.176894
Train batch 3400
Avg. loss per last 100 batches: 0.176894
Epoch: 1: Step: 3401/4907, loss=0.177587, lr=0.000019
Epoch: 1: Step: 3401/4907, loss=0.177587, lr=0.000019
Train batch 3500
Avg. loss per last 100 batches: 0.169071
Train batch 3500
Avg. loss per last 100 batches: 0.169071
Epoch: 1: Step: 3501/4907, loss=0.501168, lr=0.000019
Epoch: 1: Step: 3501/4907, loss=0.501168, lr=0.000019
Train batch 3600
Avg. loss per last 100 batches: 0.199180
Train batch 3600
Avg. loss per last 100 batches: 0.199180
Epoch: 1: Step: 3601/4907, loss=0.033487, lr=0.000019
Epoch: 1: Step: 3601/4907, loss=0.033487, lr=0.000019
Train batch 3700
Avg. loss per last 100 batches: 0.177761
Train batch 3700
Avg. loss per last 100 batches: 0.177761
Epoch: 1: Step: 3701/4907, loss=0.121475, lr=0.000019
Epoch: 1: Step: 3701/4907, loss=0.121475, lr=0.000019
Train batch 3800
Avg. loss per last 100 batches: 0.207319
Train batch 3800
Avg. loss per last 100 batches: 0.207319
Epoch: 1: Step: 3801/4907, loss=0.425595, lr=0.000019
Epoch: 1: Step: 3801/4907, loss=0.425595, lr=0.000019
Train batch 3900
Avg. loss per last 100 batches: 0.180728
Train batch 3900
Avg. loss per last 100 batches: 0.180728
Epoch: 1: Step: 3901/4907, loss=0.032794, lr=0.000019
Epoch: 1: Step: 3901/4907, loss=0.032794, lr=0.000019
Train batch 4000
Avg. loss per last 100 batches: 0.173951
Train batch 4000
Avg. loss per last 100 batches: 0.173951
Epoch: 1: Step: 4001/4907, loss=0.337232, lr=0.000019
Epoch: 1: Step: 4001/4907, loss=0.337232, lr=0.000019
Train batch 4100
Avg. loss per last 100 batches: 0.219733
Train batch 4100
Avg. loss per last 100 batches: 0.219733
Epoch: 1: Step: 4101/4907, loss=0.003933, lr=0.000019
Epoch: 1: Step: 4101/4907, loss=0.003933, lr=0.000019
Train batch 4200
Avg. loss per last 100 batches: 0.193375
Train batch 4200
Avg. loss per last 100 batches: 0.193375
Epoch: 1: Step: 4201/4907, loss=0.121112, lr=0.000019
Epoch: 1: Step: 4201/4907, loss=0.121112, lr=0.000019
Train batch 4300
Avg. loss per last 100 batches: 0.154189
Train batch 4300
Avg. loss per last 100 batches: 0.154189
Epoch: 1: Step: 4301/4907, loss=0.417406, lr=0.000019
Epoch: 1: Step: 4301/4907, loss=0.417406, lr=0.000019
Train batch 4400
Avg. loss per last 100 batches: 0.165795
Train batch 4400
Avg. loss per last 100 batches: 0.165795
Epoch: 1: Step: 4401/4907, loss=0.012326, lr=0.000019
Epoch: 1: Step: 4401/4907, loss=0.012326, lr=0.000019
Train batch 4500
Avg. loss per last 100 batches: 0.152986
Train batch 4500
Avg. loss per last 100 batches: 0.152986
Epoch: 1: Step: 4501/4907, loss=0.000347, lr=0.000019
Epoch: 1: Step: 4501/4907, loss=0.000347, lr=0.000019
Train batch 4600
Avg. loss per last 100 batches: 0.171363
Train batch 4600
Avg. loss per last 100 batches: 0.171363
Epoch: 1: Step: 4601/4907, loss=0.285035, lr=0.000019
Epoch: 1: Step: 4601/4907, loss=0.285035, lr=0.000019
Train batch 4700
Avg. loss per last 100 batches: 0.155556
Train batch 4700
Avg. loss per last 100 batches: 0.155556
Epoch: 1: Step: 4701/4907, loss=0.000043, lr=0.000019
Epoch: 1: Step: 4701/4907, loss=0.000043, lr=0.000019
Train batch 4800
Avg. loss per last 100 batches: 0.203998
Train batch 4800
Avg. loss per last 100 batches: 0.203998
Epoch: 1: Step: 4801/4907, loss=0.348933, lr=0.000019
Epoch: 1: Step: 4801/4907, loss=0.348933, lr=0.000019
Train batch 4900
Avg. loss per last 100 batches: 0.175091
Train batch 4900
Avg. loss per last 100 batches: 0.175091
Epoch: 1: Step: 4901/4907, loss=0.148549, lr=0.000019
Epoch: 1: Step: 4901/4907, loss=0.148549, lr=0.000019
Validation: Epoch: 1 Step: 4907/4907
NLL validation ...
Validation: Epoch: 1 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.605858 sec., loss=0.077826 
Eval step: 99 , used_time=28.699984 sec., loss=0.077826 
Eval step: 199 , used_time=57.754295 sec., loss=0.201883 
Eval step: 199 , used_time=57.660103 sec., loss=0.201883 
Eval step: 299 , used_time=86.670137 sec., loss=0.305515 
Eval step: 299 , used_time=86.764333 sec., loss=0.305515 
Eval step: 399 , used_time=115.788094 sec., loss=0.316863 
Eval step: 399 , used_time=115.693875 sec., loss=0.316863 
Eval step: 499 , used_time=144.806786 sec., loss=0.417448 
Eval step: 499 , used_time=144.712615 sec., loss=0.417448 
NLL Validation: loss = 0.451227. correct prediction ratio  5657/6516 ~  0.868171
NLL Validation: loss = 0.451227. correct prediction ratio  5657/6516 ~  0.868171
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.1.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.1.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.935342 sec., loss=0.077826 
Eval step: 99 , used_time=36.567600 sec., loss=0.077826 
Eval step: 199 , used_time=56.944395 sec., loss=0.201883 
Eval step: 199 , used_time=65.576672 sec., loss=0.201883 
Eval step: 299 , used_time=85.960128 sec., loss=0.305515 
Eval step: 299 , used_time=94.592399 sec., loss=0.305515 
Eval step: 399 , used_time=123.672327 sec., loss=0.316863 
Eval step: 399 , used_time=115.040100 sec., loss=0.316863 
Eval step: 499 , used_time=144.065049 sec., loss=0.417448 
Eval step: 499 , used_time=152.697338 sec., loss=0.417448 
NLL Validation: loss = 0.451227. correct prediction ratio  5657/6516 ~  0.868171
NLL Validation: loss = 0.451227. correct prediction ratio  5657/6516 ~  0.868171
Av Loss per epoch=0.185641
epoch total correct predictions=55291
***** Epoch 2 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.1.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.1.4907
Av Loss per epoch=0.185641
epoch total correct predictions=55291
***** Epoch 2 *****
Epoch: 2: Step: 1/4907, loss=0.000268, lr=0.000019
Epoch: 2: Step: 1/4907, loss=0.000268, lr=0.000019
Train batch 100
Avg. loss per last 100 batches: 0.130475
Train batch 100
Avg. loss per last 100 batches: 0.130475
Epoch: 2: Step: 101/4907, loss=0.010334, lr=0.000019
Epoch: 2: Step: 101/4907, loss=0.010334, lr=0.000019
Train batch 200
Avg. loss per last 100 batches: 0.127704
Train batch 200
Avg. loss per last 100 batches: 0.127704
Epoch: 2: Step: 201/4907, loss=0.000625, lr=0.000019
Epoch: 2: Step: 201/4907, loss=0.000625, lr=0.000019
Train batch 300
Avg. loss per last 100 batches: 0.151365
Train batch 300
Avg. loss per last 100 batches: 0.151365
Epoch: 2: Step: 301/4907, loss=0.043701, lr=0.000019
Epoch: 2: Step: 301/4907, loss=0.043701, lr=0.000019
Train batch 400
Avg. loss per last 100 batches: 0.101573
Train batch 400
Avg. loss per last 100 batches: 0.101573
Epoch: 2: Step: 401/4907, loss=0.147218, lr=0.000019
Epoch: 2: Step: 401/4907, loss=0.147218, lr=0.000019
Train batch 500
Avg. loss per last 100 batches: 0.119681
Train batch 500
Avg. loss per last 100 batches: 0.119681
Epoch: 2: Step: 501/4907, loss=0.148791, lr=0.000019
Epoch: 2: Step: 501/4907, loss=0.148791, lr=0.000019
Train batch 600
Avg. loss per last 100 batches: 0.133358
Train batch 600
Avg. loss per last 100 batches: 0.133358
Epoch: 2: Step: 601/4907, loss=0.001056, lr=0.000019
Epoch: 2: Step: 601/4907, loss=0.001056, lr=0.000019
Train batch 700
Avg. loss per last 100 batches: 0.168708
Train batch 700
Avg. loss per last 100 batches: 0.168708
Epoch: 2: Step: 701/4907, loss=0.029978, lr=0.000019
Epoch: 2: Step: 701/4907, loss=0.029978, lr=0.000019
Train batch 800
Avg. loss per last 100 batches: 0.163624
Train batch 800
Avg. loss per last 100 batches: 0.163624
Epoch: 2: Step: 801/4907, loss=0.153192, lr=0.000019
Epoch: 2: Step: 801/4907, loss=0.153192, lr=0.000019
Train batch 900
Avg. loss per last 100 batches: 0.169148
Train batch 900
Avg. loss per last 100 batches: 0.169148
Epoch: 2: Step: 901/4907, loss=0.033535, lr=0.000019
Epoch: 2: Step: 901/4907, loss=0.033535, lr=0.000019
Train batch 1000
Avg. loss per last 100 batches: 0.132535
Train batch 1000
Avg. loss per last 100 batches: 0.132535
Epoch: 2: Step: 1001/4907, loss=0.002488, lr=0.000019
Epoch: 2: Step: 1001/4907, loss=0.002488, lr=0.000019
Train batch 1100
Avg. loss per last 100 batches: 0.125052
Train batch 1100
Avg. loss per last 100 batches: 0.125052
Epoch: 2: Step: 1101/4907, loss=0.132047, lr=0.000019
Epoch: 2: Step: 1101/4907, loss=0.132047, lr=0.000019
Train batch 1200
Avg. loss per last 100 batches: 0.135744
Train batch 1200
Avg. loss per last 100 batches: 0.135744
Epoch: 2: Step: 1201/4907, loss=0.007177, lr=0.000019
Epoch: 2: Step: 1201/4907, loss=0.007177, lr=0.000019
Train batch 1300
Avg. loss per last 100 batches: 0.160538
Train batch 1300
Avg. loss per last 100 batches: 0.160538
Epoch: 2: Step: 1301/4907, loss=0.053374, lr=0.000019
Epoch: 2: Step: 1301/4907, loss=0.053374, lr=0.000019
Train batch 1400
Avg. loss per last 100 batches: 0.105453
Train batch 1400
Avg. loss per last 100 batches: 0.105453
Epoch: 2: Step: 1401/4907, loss=0.084464, lr=0.000019
Epoch: 2: Step: 1401/4907, loss=0.084464, lr=0.000019
Train batch 1500
Avg. loss per last 100 batches: 0.142406
Train batch 1500
Avg. loss per last 100 batches: 0.142406
Epoch: 2: Step: 1501/4907, loss=0.658203, lr=0.000019
Epoch: 2: Step: 1501/4907, loss=0.658203, lr=0.000019
Train batch 1600
Avg. loss per last 100 batches: 0.162034
Train batch 1600
Avg. loss per last 100 batches: 0.162034
Epoch: 2: Step: 1601/4907, loss=0.009362, lr=0.000019
Epoch: 2: Step: 1601/4907, loss=0.009362, lr=0.000019
Train batch 1700
Avg. loss per last 100 batches: 0.127299
Train batch 1700
Avg. loss per last 100 batches: 0.127299
Epoch: 2: Step: 1701/4907, loss=0.004520, lr=0.000019
Epoch: 2: Step: 1701/4907, loss=0.004520, lr=0.000019
Train batch 1800
Avg. loss per last 100 batches: 0.145849
Train batch 1800
Avg. loss per last 100 batches: 0.145849
Epoch: 2: Step: 1801/4907, loss=0.398162, lr=0.000019
Epoch: 2: Step: 1801/4907, loss=0.398162, lr=0.000019
Train batch 1900
Avg. loss per last 100 batches: 0.174043
Train batch 1900
Avg. loss per last 100 batches: 0.174043
Epoch: 2: Step: 1901/4907, loss=0.473691, lr=0.000019
Epoch: 2: Step: 1901/4907, loss=0.473691, lr=0.000019
Train batch 2000
Avg. loss per last 100 batches: 0.157203
Train batch 2000
Avg. loss per last 100 batches: 0.157203
Epoch: 2: Step: 2001/4907, loss=0.024653, lr=0.000019
Epoch: 2: Step: 2001/4907, loss=0.024653, lr=0.000019
Train batch 2100
Avg. loss per last 100 batches: 0.177066
Train batch 2100
Avg. loss per last 100 batches: 0.177066
Epoch: 2: Step: 2101/4907, loss=0.009065, lr=0.000019
Epoch: 2: Step: 2101/4907, loss=0.009065, lr=0.000019
Train batch 2200
Avg. loss per last 100 batches: 0.151937
Train batch 2200
Avg. loss per last 100 batches: 0.151937
Epoch: 2: Step: 2201/4907, loss=0.262000, lr=0.000019
Epoch: 2: Step: 2201/4907, loss=0.262000, lr=0.000019
Train batch 2300
Avg. loss per last 100 batches: 0.135962
Train batch 2300
Avg. loss per last 100 batches: 0.135962
Epoch: 2: Step: 2301/4907, loss=0.043347, lr=0.000019
Epoch: 2: Step: 2301/4907, loss=0.043347, lr=0.000019
Train batch 2400
Avg. loss per last 100 batches: 0.146038
Train batch 2400
Avg. loss per last 100 batches: 0.146038
Epoch: 2: Step: 2401/4907, loss=0.238134, lr=0.000019
Epoch: 2: Step: 2401/4907, loss=0.238134, lr=0.000019
Train batch 2500
Avg. loss per last 100 batches: 0.149174
Train batch 2500
Avg. loss per last 100 batches: 0.149174
Epoch: 2: Step: 2501/4907, loss=0.123742, lr=0.000019
Epoch: 2: Step: 2501/4907, loss=0.123742, lr=0.000019
Train batch 2600
Avg. loss per last 100 batches: 0.137475
Train batch 2600
Avg. loss per last 100 batches: 0.137475
Epoch: 2: Step: 2601/4907, loss=0.000033, lr=0.000019
Epoch: 2: Step: 2601/4907, loss=0.000033, lr=0.000019
Train batch 2700
Avg. loss per last 100 batches: 0.164814
Train batch 2700
Avg. loss per last 100 batches: 0.164814
Epoch: 2: Step: 2701/4907, loss=0.076965, lr=0.000019
Epoch: 2: Step: 2701/4907, loss=0.076965, lr=0.000019
Train batch 2800
Avg. loss per last 100 batches: 0.142691
Train batch 2800
Avg. loss per last 100 batches: 0.142691
Epoch: 2: Step: 2801/4907, loss=0.007685, lr=0.000019
Epoch: 2: Step: 2801/4907, loss=0.007685, lr=0.000019
Train batch 2900
Avg. loss per last 100 batches: 0.132201
Train batch 2900
Avg. loss per last 100 batches: 0.132201
Epoch: 2: Step: 2901/4907, loss=0.006690, lr=0.000019
Epoch: 2: Step: 2901/4907, loss=0.006690, lr=0.000019
Train batch 3000
Avg. loss per last 100 batches: 0.131705
Train batch 3000
Avg. loss per last 100 batches: 0.131705
Epoch: 2: Step: 3001/4907, loss=0.001988, lr=0.000019
Epoch: 2: Step: 3001/4907, loss=0.001988, lr=0.000019
Train batch 3100
Avg. loss per last 100 batches: 0.134688
Train batch 3100
Avg. loss per last 100 batches: 0.134688
Epoch: 2: Step: 3101/4907, loss=0.245513, lr=0.000019
Epoch: 2: Step: 3101/4907, loss=0.245513, lr=0.000019
Train batch 3200
Avg. loss per last 100 batches: 0.123418
Train batch 3200
Avg. loss per last 100 batches: 0.123418
Epoch: 2: Step: 3201/4907, loss=0.066106, lr=0.000019
Epoch: 2: Step: 3201/4907, loss=0.066106, lr=0.000019
Train batch 3300
Avg. loss per last 100 batches: 0.133523
Train batch 3300
Avg. loss per last 100 batches: 0.133523
Epoch: 2: Step: 3301/4907, loss=0.002442, lr=0.000019
Epoch: 2: Step: 3301/4907, loss=0.002442, lr=0.000019
Train batch 3400
Avg. loss per last 100 batches: 0.146097
Train batch 3400
Avg. loss per last 100 batches: 0.146097
Epoch: 2: Step: 3401/4907, loss=0.098527, lr=0.000019
Epoch: 2: Step: 3401/4907, loss=0.098527, lr=0.000019
Train batch 3500
Avg. loss per last 100 batches: 0.177713
Train batch 3500
Avg. loss per last 100 batches: 0.177713
Epoch: 2: Step: 3501/4907, loss=0.647211, lr=0.000019
Epoch: 2: Step: 3501/4907, loss=0.647211, lr=0.000019
Train batch 3600
Avg. loss per last 100 batches: 0.161815
Train batch 3600
Avg. loss per last 100 batches: 0.161815
Epoch: 2: Step: 3601/4907, loss=0.353080, lr=0.000019
Epoch: 2: Step: 3601/4907, loss=0.353080, lr=0.000019
Train batch 3700
Avg. loss per last 100 batches: 0.125213
Train batch 3700
Avg. loss per last 100 batches: 0.125213
Epoch: 2: Step: 3701/4907, loss=0.000555, lr=0.000019
Epoch: 2: Step: 3701/4907, loss=0.000555, lr=0.000019
Train batch 3800
Avg. loss per last 100 batches: 0.164101
Train batch 3800
Avg. loss per last 100 batches: 0.164101
Epoch: 2: Step: 3801/4907, loss=0.044440, lr=0.000019
Epoch: 2: Step: 3801/4907, loss=0.044440, lr=0.000019
Train batch 3900
Avg. loss per last 100 batches: 0.121745
Train batch 3900
Avg. loss per last 100 batches: 0.121745
Epoch: 2: Step: 3901/4907, loss=0.387297, lr=0.000019
Epoch: 2: Step: 3901/4907, loss=0.387297, lr=0.000019
Train batch 4000
Avg. loss per last 100 batches: 0.128386
Train batch 4000
Avg. loss per last 100 batches: 0.128386
Epoch: 2: Step: 4001/4907, loss=0.055780, lr=0.000019
Epoch: 2: Step: 4001/4907, loss=0.055780, lr=0.000019
Train batch 4100
Avg. loss per last 100 batches: 0.126807
Train batch 4100
Avg. loss per last 100 batches: 0.126807
Epoch: 2: Step: 4101/4907, loss=0.018521, lr=0.000019
Epoch: 2: Step: 4101/4907, loss=0.018521, lr=0.000019
Train batch 4200
Avg. loss per last 100 batches: 0.134794
Train batch 4200
Avg. loss per last 100 batches: 0.134794
Epoch: 2: Step: 4201/4907, loss=0.015150, lr=0.000019
Epoch: 2: Step: 4201/4907, loss=0.015150, lr=0.000019
Train batch 4300
Avg. loss per last 100 batches: 0.175592
Train batch 4300
Avg. loss per last 100 batches: 0.175592
Epoch: 2: Step: 4301/4907, loss=0.018430, lr=0.000019
Epoch: 2: Step: 4301/4907, loss=0.018430, lr=0.000019
Train batch 4400
Avg. loss per last 100 batches: 0.129347
Train batch 4400
Avg. loss per last 100 batches: 0.129347
Epoch: 2: Step: 4401/4907, loss=0.214990, lr=0.000019
Epoch: 2: Step: 4401/4907, loss=0.214990, lr=0.000019
Train batch 4500
Avg. loss per last 100 batches: 0.184668
Train batch 4500
Avg. loss per last 100 batches: 0.184668
Epoch: 2: Step: 4501/4907, loss=0.209033, lr=0.000019
Epoch: 2: Step: 4501/4907, loss=0.209033, lr=0.000019
Train batch 4600
Avg. loss per last 100 batches: 0.143202
Train batch 4600
Avg. loss per last 100 batches: 0.143202
Epoch: 2: Step: 4601/4907, loss=0.139282, lr=0.000019
Epoch: 2: Step: 4601/4907, loss=0.139282, lr=0.000019
Train batch 4700
Avg. loss per last 100 batches: 0.148346
Train batch 4700
Avg. loss per last 100 batches: 0.148346
Epoch: 2: Step: 4701/4907, loss=0.144561, lr=0.000019
Epoch: 2: Step: 4701/4907, loss=0.144561, lr=0.000019
Train batch 4800
Avg. loss per last 100 batches: 0.165761
Train batch 4800
Avg. loss per last 100 batches: 0.165761
Epoch: 2: Step: 4801/4907, loss=0.009912, lr=0.000019
Epoch: 2: Step: 4801/4907, loss=0.009912, lr=0.000019
Train batch 4900
Avg. loss per last 100 batches: 0.129788
Train batch 4900
Avg. loss per last 100 batches: 0.129788
Epoch: 2: Step: 4901/4907, loss=0.165693, lr=0.000019
Epoch: 2: Step: 4901/4907, loss=0.165693, lr=0.000019
Validation: Epoch: 2 Step: 4907/4907
NLL validation ...
Validation: Epoch: 2 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.335539 sec., loss=0.122542 
Eval step: 99 , used_time=28.230158 sec., loss=0.122542 
Eval step: 199 , used_time=57.362612 sec., loss=0.061032 
Eval step: 199 , used_time=57.257202 sec., loss=0.061032 
Eval step: 299 , used_time=86.297840 sec., loss=1.003866 
Eval step: 299 , used_time=86.403234 sec., loss=1.003866 
Eval step: 399 , used_time=115.336743 sec., loss=0.220949 
Eval step: 399 , used_time=115.442150 sec., loss=0.220949 
Eval step: 499 , used_time=144.530285 sec., loss=0.433617 
Eval step: 499 , used_time=144.424981 sec., loss=0.433617 
NLL Validation: loss = 0.462244. correct prediction ratio  5725/6516 ~  0.878607
NLL Validation: loss = 0.462244. correct prediction ratio  5725/6516 ~  0.878607
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.2.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.2.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.933107 sec., loss=0.122542 
Eval step: 99 , used_time=36.510980 sec., loss=0.122542 
Eval step: 199 , used_time=65.536556 sec., loss=0.061032 
Eval step: 199 , used_time=56.958689 sec., loss=0.061032 
Eval step: 299 , used_time=85.982447 sec., loss=1.003866 
Eval step: 299 , used_time=94.560378 sec., loss=1.003866 
Eval step: 399 , used_time=115.018286 sec., loss=0.220949 
Eval step: 399 , used_time=123.596190 sec., loss=0.220949 
Eval step: 499 , used_time=144.037237 sec., loss=0.433617 
Eval step: 499 , used_time=152.615137 sec., loss=0.433617 
NLL Validation: loss = 0.462244. correct prediction ratio  5725/6516 ~  0.878607
NLL Validation: loss = 0.462244. correct prediction ratio  5725/6516 ~  0.878607
Av Loss per epoch=0.143918
epoch total correct predictions=56122
***** Epoch 3 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.2.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.2.4907
Av Loss per epoch=0.143918
epoch total correct predictions=56122
***** Epoch 3 *****
Epoch: 3: Step: 1/4907, loss=0.004473, lr=0.000019
Epoch: 3: Step: 1/4907, loss=0.004473, lr=0.000019
Train batch 100
Avg. loss per last 100 batches: 0.119629
Train batch 100
Avg. loss per last 100 batches: 0.119629
Epoch: 3: Step: 101/4907, loss=0.093036, lr=0.000019
Epoch: 3: Step: 101/4907, loss=0.093036, lr=0.000019
Train batch 200
Avg. loss per last 100 batches: 0.102527
Train batch 200
Avg. loss per last 100 batches: 0.102527
Epoch: 3: Step: 201/4907, loss=0.040410, lr=0.000019
Epoch: 3: Step: 201/4907, loss=0.040410, lr=0.000019
Train batch 300
Avg. loss per last 100 batches: 0.111630
Train batch 300
Avg. loss per last 100 batches: 0.111630
Epoch: 3: Step: 301/4907, loss=0.189625, lr=0.000019
Epoch: 3: Step: 301/4907, loss=0.189625, lr=0.000019
Train batch 400
Avg. loss per last 100 batches: 0.066411
Train batch 400
Avg. loss per last 100 batches: 0.066411
Epoch: 3: Step: 401/4907, loss=0.055362, lr=0.000019
Epoch: 3: Step: 401/4907, loss=0.055362, lr=0.000019
Train batch 500
Avg. loss per last 100 batches: 0.146594
Train batch 500
Avg. loss per last 100 batches: 0.146594
Epoch: 3: Step: 501/4907, loss=0.003542, lr=0.000019
Epoch: 3: Step: 501/4907, loss=0.003542, lr=0.000019
Train batch 600
Avg. loss per last 100 batches: 0.106360
Train batch 600
Avg. loss per last 100 batches: 0.106360
Epoch: 3: Step: 601/4907, loss=0.158010, lr=0.000019
Epoch: 3: Step: 601/4907, loss=0.158010, lr=0.000019
Train batch 700
Avg. loss per last 100 batches: 0.117489
Train batch 700
Avg. loss per last 100 batches: 0.117489
Epoch: 3: Step: 701/4907, loss=0.004962, lr=0.000019
Epoch: 3: Step: 701/4907, loss=0.004962, lr=0.000019
Train batch 800
Avg. loss per last 100 batches: 0.123142
Train batch 800
Avg. loss per last 100 batches: 0.123142
Epoch: 3: Step: 801/4907, loss=0.001598, lr=0.000019
Epoch: 3: Step: 801/4907, loss=0.001598, lr=0.000019
Train batch 900
Avg. loss per last 100 batches: 0.096027
Train batch 900
Avg. loss per last 100 batches: 0.096027
Epoch: 3: Step: 901/4907, loss=0.084808, lr=0.000019
Epoch: 3: Step: 901/4907, loss=0.084808, lr=0.000019
Train batch 1000
Avg. loss per last 100 batches: 0.125055
Train batch 1000
Avg. loss per last 100 batches: 0.125055
Epoch: 3: Step: 1001/4907, loss=0.000018, lr=0.000019
Epoch: 3: Step: 1001/4907, loss=0.000018, lr=0.000019
Train batch 1100
Avg. loss per last 100 batches: 0.151193
Train batch 1100
Avg. loss per last 100 batches: 0.151193
Epoch: 3: Step: 1101/4907, loss=0.574116, lr=0.000019
Epoch: 3: Step: 1101/4907, loss=0.574116, lr=0.000019
Train batch 1200
Avg. loss per last 100 batches: 0.140426
Train batch 1200
Avg. loss per last 100 batches: 0.140426
Epoch: 3: Step: 1201/4907, loss=0.000392, lr=0.000018
Epoch: 3: Step: 1201/4907, loss=0.000392, lr=0.000018
Train batch 1300
Avg. loss per last 100 batches: 0.128039
Train batch 1300
Avg. loss per last 100 batches: 0.128039
Epoch: 3: Step: 1301/4907, loss=0.117565, lr=0.000018
Epoch: 3: Step: 1301/4907, loss=0.117565, lr=0.000018
Train batch 1400
Avg. loss per last 100 batches: 0.130077
Train batch 1400
Avg. loss per last 100 batches: 0.130077
Epoch: 3: Step: 1401/4907, loss=0.229384, lr=0.000018
Epoch: 3: Step: 1401/4907, loss=0.229384, lr=0.000018
Train batch 1500
Avg. loss per last 100 batches: 0.107464
Train batch 1500
Avg. loss per last 100 batches: 0.107464
Epoch: 3: Step: 1501/4907, loss=0.010459, lr=0.000018
Epoch: 3: Step: 1501/4907, loss=0.010459, lr=0.000018
Train batch 1600
Avg. loss per last 100 batches: 0.121274
Train batch 1600
Avg. loss per last 100 batches: 0.121274
Epoch: 3: Step: 1601/4907, loss=0.963592, lr=0.000018
Epoch: 3: Step: 1601/4907, loss=0.963592, lr=0.000018
Train batch 1700
Avg. loss per last 100 batches: 0.105798
Train batch 1700
Avg. loss per last 100 batches: 0.105798
Epoch: 3: Step: 1701/4907, loss=0.247258, lr=0.000018
Epoch: 3: Step: 1701/4907, loss=0.247258, lr=0.000018
Train batch 1800
Avg. loss per last 100 batches: 0.124541
Train batch 1800
Avg. loss per last 100 batches: 0.124541
Epoch: 3: Step: 1801/4907, loss=0.111277, lr=0.000018
Epoch: 3: Step: 1801/4907, loss=0.111277, lr=0.000018
Train batch 1900
Avg. loss per last 100 batches: 0.150231
Train batch 1900
Avg. loss per last 100 batches: 0.150231
Epoch: 3: Step: 1901/4907, loss=0.001750, lr=0.000018
Epoch: 3: Step: 1901/4907, loss=0.001750, lr=0.000018
Train batch 2000
Avg. loss per last 100 batches: 0.113310
Train batch 2000
Avg. loss per last 100 batches: 0.113310
Epoch: 3: Step: 2001/4907, loss=0.606853, lr=0.000018
Epoch: 3: Step: 2001/4907, loss=0.606853, lr=0.000018
Train batch 2100
Avg. loss per last 100 batches: 0.114196
Train batch 2100
Avg. loss per last 100 batches: 0.114196
Epoch: 3: Step: 2101/4907, loss=0.137080, lr=0.000018
Epoch: 3: Step: 2101/4907, loss=0.137080, lr=0.000018
Train batch 2200
Avg. loss per last 100 batches: 0.128158
Train batch 2200
Avg. loss per last 100 batches: 0.128158
Epoch: 3: Step: 2201/4907, loss=0.138897, lr=0.000018
Epoch: 3: Step: 2201/4907, loss=0.138897, lr=0.000018
Train batch 2300
Avg. loss per last 100 batches: 0.102113
Train batch 2300
Avg. loss per last 100 batches: 0.102113
Epoch: 3: Step: 2301/4907, loss=0.011787, lr=0.000018
Epoch: 3: Step: 2301/4907, loss=0.011787, lr=0.000018
Train batch 2400
Avg. loss per last 100 batches: 0.097110
Train batch 2400
Avg. loss per last 100 batches: 0.097110
Epoch: 3: Step: 2401/4907, loss=0.571753, lr=0.000018
Epoch: 3: Step: 2401/4907, loss=0.571753, lr=0.000018
Train batch 2500
Avg. loss per last 100 batches: 0.123071
Train batch 2500
Avg. loss per last 100 batches: 0.123071
Epoch: 3: Step: 2501/4907, loss=0.000196, lr=0.000018
Epoch: 3: Step: 2501/4907, loss=0.000196, lr=0.000018
Train batch 2600
Avg. loss per last 100 batches: 0.156281
Train batch 2600
Avg. loss per last 100 batches: 0.156281
Epoch: 3: Step: 2601/4907, loss=0.086756, lr=0.000018
Epoch: 3: Step: 2601/4907, loss=0.086756, lr=0.000018
Train batch 2700
Avg. loss per last 100 batches: 0.127812
Train batch 2700
Avg. loss per last 100 batches: 0.127812
Epoch: 3: Step: 2701/4907, loss=0.004001, lr=0.000018
Epoch: 3: Step: 2701/4907, loss=0.004001, lr=0.000018
Train batch 2800
Avg. loss per last 100 batches: 0.104981
Train batch 2800
Avg. loss per last 100 batches: 0.104981
Epoch: 3: Step: 2801/4907, loss=0.002765, lr=0.000018
Epoch: 3: Step: 2801/4907, loss=0.002765, lr=0.000018
Train batch 2900
Avg. loss per last 100 batches: 0.116319
Train batch 2900
Avg. loss per last 100 batches: 0.116319
Epoch: 3: Step: 2901/4907, loss=0.183596, lr=0.000018
Epoch: 3: Step: 2901/4907, loss=0.183596, lr=0.000018
Train batch 3000
Avg. loss per last 100 batches: 0.111848
Train batch 3000
Avg. loss per last 100 batches: 0.111848
Epoch: 3: Step: 3001/4907, loss=0.001847, lr=0.000018
Epoch: 3: Step: 3001/4907, loss=0.001847, lr=0.000018
Train batch 3100
Avg. loss per last 100 batches: 0.134781
Train batch 3100
Avg. loss per last 100 batches: 0.134781
Epoch: 3: Step: 3101/4907, loss=0.011978, lr=0.000018
Epoch: 3: Step: 3101/4907, loss=0.011978, lr=0.000018
Train batch 3200
Avg. loss per last 100 batches: 0.127810
Train batch 3200
Avg. loss per last 100 batches: 0.127810
Epoch: 3: Step: 3201/4907, loss=0.000007, lr=0.000018
Epoch: 3: Step: 3201/4907, loss=0.000007, lr=0.000018
Train batch 3300
Avg. loss per last 100 batches: 0.086554
Train batch 3300
Avg. loss per last 100 batches: 0.086554
Epoch: 3: Step: 3301/4907, loss=0.256230, lr=0.000018
Epoch: 3: Step: 3301/4907, loss=0.256230, lr=0.000018
Train batch 3400
Avg. loss per last 100 batches: 0.129516
Train batch 3400
Avg. loss per last 100 batches: 0.129516
Epoch: 3: Step: 3401/4907, loss=0.006425, lr=0.000018
Epoch: 3: Step: 3401/4907, loss=0.006425, lr=0.000018
Train batch 3500
Avg. loss per last 100 batches: 0.103982
Train batch 3500
Avg. loss per last 100 batches: 0.103982
Epoch: 3: Step: 3501/4907, loss=0.294638, lr=0.000018
Epoch: 3: Step: 3501/4907, loss=0.294638, lr=0.000018
Train batch 3600
Avg. loss per last 100 batches: 0.098667
Train batch 3600
Avg. loss per last 100 batches: 0.098667
Epoch: 3: Step: 3601/4907, loss=0.010094, lr=0.000018
Epoch: 3: Step: 3601/4907, loss=0.010094, lr=0.000018
Train batch 3700
Avg. loss per last 100 batches: 0.109604
Train batch 3700
Avg. loss per last 100 batches: 0.109604
Epoch: 3: Step: 3701/4907, loss=0.000188, lr=0.000018
Epoch: 3: Step: 3701/4907, loss=0.000188, lr=0.000018
Train batch 3800
Avg. loss per last 100 batches: 0.165877
Train batch 3800
Avg. loss per last 100 batches: 0.165877
Epoch: 3: Step: 3801/4907, loss=0.039779, lr=0.000018
Epoch: 3: Step: 3801/4907, loss=0.039779, lr=0.000018
Train batch 3900
Avg. loss per last 100 batches: 0.119423
Train batch 3900
Avg. loss per last 100 batches: 0.119423
Epoch: 3: Step: 3901/4907, loss=0.375582, lr=0.000018
Epoch: 3: Step: 3901/4907, loss=0.375582, lr=0.000018
Train batch 4000
Avg. loss per last 100 batches: 0.142770
Train batch 4000
Avg. loss per last 100 batches: 0.142770
Epoch: 3: Step: 4001/4907, loss=0.433221, lr=0.000018
Epoch: 3: Step: 4001/4907, loss=0.433221, lr=0.000018
Train batch 4100
Avg. loss per last 100 batches: 0.113488
Train batch 4100
Avg. loss per last 100 batches: 0.113488
Epoch: 3: Step: 4101/4907, loss=0.025105, lr=0.000018
Epoch: 3: Step: 4101/4907, loss=0.025105, lr=0.000018
Train batch 4200
Avg. loss per last 100 batches: 0.121757
Train batch 4200
Avg. loss per last 100 batches: 0.121757
Epoch: 3: Step: 4201/4907, loss=0.001382, lr=0.000018
Epoch: 3: Step: 4201/4907, loss=0.001382, lr=0.000018
Train batch 4300
Avg. loss per last 100 batches: 0.121412
Train batch 4300
Avg. loss per last 100 batches: 0.121412
Epoch: 3: Step: 4301/4907, loss=0.002254, lr=0.000018
Epoch: 3: Step: 4301/4907, loss=0.002254, lr=0.000018
Train batch 4400
Avg. loss per last 100 batches: 0.095577
Train batch 4400
Avg. loss per last 100 batches: 0.095577
Epoch: 3: Step: 4401/4907, loss=0.009432, lr=0.000018
Epoch: 3: Step: 4401/4907, loss=0.009432, lr=0.000018
Train batch 4500
Avg. loss per last 100 batches: 0.121860
Train batch 4500
Avg. loss per last 100 batches: 0.121860
Epoch: 3: Step: 4501/4907, loss=0.281079, lr=0.000018
Epoch: 3: Step: 4501/4907, loss=0.281079, lr=0.000018
Train batch 4600
Avg. loss per last 100 batches: 0.129762
Train batch 4600
Avg. loss per last 100 batches: 0.129762
Epoch: 3: Step: 4601/4907, loss=0.016645, lr=0.000018
Epoch: 3: Step: 4601/4907, loss=0.016645, lr=0.000018
Train batch 4700
Avg. loss per last 100 batches: 0.119798
Train batch 4700
Avg. loss per last 100 batches: 0.119798
Epoch: 3: Step: 4701/4907, loss=0.007714, lr=0.000018
Epoch: 3: Step: 4701/4907, loss=0.007714, lr=0.000018
Train batch 4800
Avg. loss per last 100 batches: 0.117641
Train batch 4800
Avg. loss per last 100 batches: 0.117641
Epoch: 3: Step: 4801/4907, loss=0.094617, lr=0.000018
Epoch: 3: Step: 4801/4907, loss=0.094617, lr=0.000018
Train batch 4900
Avg. loss per last 100 batches: 0.111497
Train batch 4900
Avg. loss per last 100 batches: 0.111497
Epoch: 3: Step: 4901/4907, loss=0.078763, lr=0.000018
Epoch: 3: Step: 4901/4907, loss=0.078763, lr=0.000018
Validation: Epoch: 3 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 3 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=29.266317 sec., loss=0.028750 
Eval step: 99 , used_time=28.294732 sec., loss=0.028750 
Eval step: 199 , used_time=57.293950 sec., loss=0.364822 
Eval step: 199 , used_time=58.265505 sec., loss=0.364822 
Eval step: 299 , used_time=87.258734 sec., loss=0.629369 
Eval step: 299 , used_time=86.287220 sec., loss=0.629369 
Eval step: 399 , used_time=115.286576 sec., loss=0.364496 
Eval step: 399 , used_time=116.258183 sec., loss=0.364496 
Eval step: 499 , used_time=145.256582 sec., loss=0.157763 
Eval step: 499 , used_time=144.285015 sec., loss=0.157763 
NLL Validation: loss = 0.500311. correct prediction ratio  5708/6516 ~  0.875998
NLL Validation: loss = 0.500311. correct prediction ratio  5708/6516 ~  0.875998
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.3.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.3.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=35.419217 sec., loss=0.028750 
Eval step: 99 , used_time=27.820044 sec., loss=0.028750 
Eval step: 199 , used_time=56.821861 sec., loss=0.364822 
Eval step: 199 , used_time=64.421064 sec., loss=0.364822 
Eval step: 299 , used_time=93.427894 sec., loss=0.629369 
Eval step: 299 , used_time=85.830885 sec., loss=0.629369 
Eval step: 399 , used_time=114.843012 sec., loss=0.364496 
Eval step: 399 , used_time=122.442214 sec., loss=0.364496 
Eval step: 499 , used_time=143.876518 sec., loss=0.157763 
Eval step: 499 , used_time=151.475744 sec., loss=0.157763 
NLL Validation: loss = 0.500311. correct prediction ratio  5708/6516 ~  0.875998
NLL Validation: loss = 0.500311. correct prediction ratio  5708/6516 ~  0.875998
Av Loss per epoch=0.119083
epoch total correct predictions=56624
***** Epoch 4 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.3.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.3.4907
Av Loss per epoch=0.119083
epoch total correct predictions=56624
***** Epoch 4 *****
Epoch: 4: Step: 1/4907, loss=0.079615, lr=0.000018
Epoch: 4: Step: 1/4907, loss=0.079615, lr=0.000018
Train batch 100
Avg. loss per last 100 batches: 0.079507
Train batch 100
Avg. loss per last 100 batches: 0.079507
Epoch: 4: Step: 101/4907, loss=0.000878, lr=0.000018
Epoch: 4: Step: 101/4907, loss=0.000878, lr=0.000018
Train batch 200
Avg. loss per last 100 batches: 0.128135
Train batch 200
Avg. loss per last 100 batches: 0.128135
Epoch: 4: Step: 201/4907, loss=0.423955, lr=0.000018
Epoch: 4: Step: 201/4907, loss=0.423955, lr=0.000018
Train batch 300
Avg. loss per last 100 batches: 0.100475
Train batch 300
Avg. loss per last 100 batches: 0.100475
Epoch: 4: Step: 301/4907, loss=0.608137, lr=0.000018
Epoch: 4: Step: 301/4907, loss=0.608137, lr=0.000018
Train batch 400
Avg. loss per last 100 batches: 0.115109
Train batch 400
Avg. loss per last 100 batches: 0.115109
Epoch: 4: Step: 401/4907, loss=0.000062, lr=0.000018
Epoch: 4: Step: 401/4907, loss=0.000062, lr=0.000018
Train batch 500
Avg. loss per last 100 batches: 0.103152
Train batch 500
Avg. loss per last 100 batches: 0.103152
Epoch: 4: Step: 501/4907, loss=0.141809, lr=0.000018
Epoch: 4: Step: 501/4907, loss=0.141809, lr=0.000018
Train batch 600
Avg. loss per last 100 batches: 0.116558
Train batch 600
Avg. loss per last 100 batches: 0.116558
Epoch: 4: Step: 601/4907, loss=0.009834, lr=0.000018
Epoch: 4: Step: 601/4907, loss=0.009834, lr=0.000018
Train batch 700
Avg. loss per last 100 batches: 0.099209
Train batch 700
Avg. loss per last 100 batches: 0.099209
Epoch: 4: Step: 701/4907, loss=0.089348, lr=0.000018
Epoch: 4: Step: 701/4907, loss=0.089348, lr=0.000018
Train batch 800
Avg. loss per last 100 batches: 0.154910
Train batch 800
Avg. loss per last 100 batches: 0.154910
Epoch: 4: Step: 801/4907, loss=0.295065, lr=0.000018
Epoch: 4: Step: 801/4907, loss=0.295065, lr=0.000018
Train batch 900
Avg. loss per last 100 batches: 0.086626
Train batch 900
Avg. loss per last 100 batches: 0.086626
Epoch: 4: Step: 901/4907, loss=0.007776, lr=0.000018
Epoch: 4: Step: 901/4907, loss=0.007776, lr=0.000018
Train batch 1000
Avg. loss per last 100 batches: 0.148632
Train batch 1000
Avg. loss per last 100 batches: 0.148632
Epoch: 4: Step: 1001/4907, loss=0.007279, lr=0.000018
Epoch: 4: Step: 1001/4907, loss=0.007279, lr=0.000018
Train batch 1100
Avg. loss per last 100 batches: 0.071484
Train batch 1100
Avg. loss per last 100 batches: 0.071484
Epoch: 4: Step: 1101/4907, loss=0.156648, lr=0.000018
Epoch: 4: Step: 1101/4907, loss=0.156648, lr=0.000018
Train batch 1200
Avg. loss per last 100 batches: 0.100159
Train batch 1200
Avg. loss per last 100 batches: 0.100159
Epoch: 4: Step: 1201/4907, loss=0.063571, lr=0.000018
Epoch: 4: Step: 1201/4907, loss=0.063571, lr=0.000018
Train batch 1300
Avg. loss per last 100 batches: 0.102197
Train batch 1300
Avg. loss per last 100 batches: 0.102197
Epoch: 4: Step: 1301/4907, loss=0.032887, lr=0.000018
Epoch: 4: Step: 1301/4907, loss=0.032887, lr=0.000018
Train batch 1400
Avg. loss per last 100 batches: 0.111199
Train batch 1400
Avg. loss per last 100 batches: 0.111199
Epoch: 4: Step: 1401/4907, loss=0.083662, lr=0.000018
Epoch: 4: Step: 1401/4907, loss=0.083662, lr=0.000018
Train batch 1500
Avg. loss per last 100 batches: 0.081717
Train batch 1500
Avg. loss per last 100 batches: 0.081717
Epoch: 4: Step: 1501/4907, loss=0.078136, lr=0.000018
Epoch: 4: Step: 1501/4907, loss=0.078136, lr=0.000018
Train batch 1600
Avg. loss per last 100 batches: 0.096330
Train batch 1600
Avg. loss per last 100 batches: 0.096330
Epoch: 4: Step: 1601/4907, loss=0.046333, lr=0.000018
Epoch: 4: Step: 1601/4907, loss=0.046333, lr=0.000018
Train batch 1700
Avg. loss per last 100 batches: 0.138445
Train batch 1700
Avg. loss per last 100 batches: 0.138445
Epoch: 4: Step: 1701/4907, loss=0.022784, lr=0.000018
Epoch: 4: Step: 1701/4907, loss=0.022784, lr=0.000018
Train batch 1800
Avg. loss per last 100 batches: 0.112658
Train batch 1800
Avg. loss per last 100 batches: 0.112658
Epoch: 4: Step: 1801/4907, loss=0.113855, lr=0.000018
Epoch: 4: Step: 1801/4907, loss=0.113855, lr=0.000018
Train batch 1900
Avg. loss per last 100 batches: 0.081977
Train batch 1900
Avg. loss per last 100 batches: 0.081977
Epoch: 4: Step: 1901/4907, loss=0.077979, lr=0.000018
Epoch: 4: Step: 1901/4907, loss=0.077979, lr=0.000018
Train batch 2000
Avg. loss per last 100 batches: 0.098182
Train batch 2000
Avg. loss per last 100 batches: 0.098182
Epoch: 4: Step: 2001/4907, loss=0.415406, lr=0.000018
Epoch: 4: Step: 2001/4907, loss=0.415406, lr=0.000018
Train batch 2100
Train batch 2100
Avg. loss per last 100 batches: 0.114798
Avg. loss per last 100 batches: 0.114798
Epoch: 4: Step: 2101/4907, loss=0.009244, lr=0.000018
Epoch: 4: Step: 2101/4907, loss=0.009244, lr=0.000018
Train batch 2200
Avg. loss per last 100 batches: 0.130390
Train batch 2200
Avg. loss per last 100 batches: 0.130390
Epoch: 4: Step: 2201/4907, loss=0.000306, lr=0.000018
Epoch: 4: Step: 2201/4907, loss=0.000306, lr=0.000018
Train batch 2300
Avg. loss per last 100 batches: 0.159396
Train batch 2300
Avg. loss per last 100 batches: 0.159396
Epoch: 4: Step: 2301/4907, loss=0.209806, lr=0.000018
Epoch: 4: Step: 2301/4907, loss=0.209806, lr=0.000018
Train batch 2400
Avg. loss per last 100 batches: 0.112158
Train batch 2400
Avg. loss per last 100 batches: 0.112158
Epoch: 4: Step: 2401/4907, loss=0.048326, lr=0.000018
Epoch: 4: Step: 2401/4907, loss=0.048326, lr=0.000018
Train batch 2500
Avg. loss per last 100 batches: 0.092262
Train batch 2500
Avg. loss per last 100 batches: 0.092262
Epoch: 4: Step: 2501/4907, loss=0.080409, lr=0.000018
Epoch: 4: Step: 2501/4907, loss=0.080409, lr=0.000018
Train batch 2600
Avg. loss per last 100 batches: 0.091421
Train batch 2600
Avg. loss per last 100 batches: 0.091421
Epoch: 4: Step: 2601/4907, loss=0.188629, lr=0.000018
Epoch: 4: Step: 2601/4907, loss=0.188629, lr=0.000018
Train batch 2700
Avg. loss per last 100 batches: 0.091957
Train batch 2700
Avg. loss per last 100 batches: 0.091957
Epoch: 4: Step: 2701/4907, loss=0.204399, lr=0.000018
Epoch: 4: Step: 2701/4907, loss=0.204399, lr=0.000018
Train batch 2800
Avg. loss per last 100 batches: 0.101089
Train batch 2800
Avg. loss per last 100 batches: 0.101089
Epoch: 4: Step: 2801/4907, loss=0.001440, lr=0.000018
Epoch: 4: Step: 2801/4907, loss=0.001440, lr=0.000018
Train batch 2900
Avg. loss per last 100 batches: 0.073587
Train batch 2900
Avg. loss per last 100 batches: 0.073587
Epoch: 4: Step: 2901/4907, loss=1.299675, lr=0.000018
Epoch: 4: Step: 2901/4907, loss=1.299675, lr=0.000018
Train batch 3000
Avg. loss per last 100 batches: 0.109971
Train batch 3000
Avg. loss per last 100 batches: 0.109971
Epoch: 4: Step: 3001/4907, loss=0.035141, lr=0.000018
Epoch: 4: Step: 3001/4907, loss=0.035141, lr=0.000018
Train batch 3100
Avg. loss per last 100 batches: 0.108660
Train batch 3100
Avg. loss per last 100 batches: 0.108660
Epoch: 4: Step: 3101/4907, loss=0.395609, lr=0.000018
Epoch: 4: Step: 3101/4907, loss=0.395609, lr=0.000018
Train batch 3200
Avg. loss per last 100 batches: 0.082484
Train batch 3200
Avg. loss per last 100 batches: 0.082484
Epoch: 4: Step: 3201/4907, loss=0.055622, lr=0.000018
Epoch: 4: Step: 3201/4907, loss=0.055622, lr=0.000018
Train batch 3300
Avg. loss per last 100 batches: 0.121746
Train batch 3300
Avg. loss per last 100 batches: 0.121746
Epoch: 4: Step: 3301/4907, loss=0.026974, lr=0.000018
Epoch: 4: Step: 3301/4907, loss=0.026974, lr=0.000018
Train batch 3400
Avg. loss per last 100 batches: 0.104950
Train batch 3400
Avg. loss per last 100 batches: 0.104950
Epoch: 4: Step: 3401/4907, loss=0.034076, lr=0.000018
Epoch: 4: Step: 3401/4907, loss=0.034076, lr=0.000018
Train batch 3500
Avg. loss per last 100 batches: 0.111844
Train batch 3500
Avg. loss per last 100 batches: 0.111844
Epoch: 4: Step: 3501/4907, loss=0.001391, lr=0.000018
Epoch: 4: Step: 3501/4907, loss=0.001391, lr=0.000018
Train batch 3600
Avg. loss per last 100 batches: 0.101029
Train batch 3600
Avg. loss per last 100 batches: 0.101029
Epoch: 4: Step: 3601/4907, loss=0.454105, lr=0.000018
Epoch: 4: Step: 3601/4907, loss=0.454105, lr=0.000018
Train batch 3700
Avg. loss per last 100 batches: 0.106764
Train batch 3700
Avg. loss per last 100 batches: 0.106764
Epoch: 4: Step: 3701/4907, loss=0.000600, lr=0.000018
Epoch: 4: Step: 3701/4907, loss=0.000600, lr=0.000018
Train batch 3800
Avg. loss per last 100 batches: 0.124726
Train batch 3800
Avg. loss per last 100 batches: 0.124726
Epoch: 4: Step: 3801/4907, loss=0.352924, lr=0.000018
Epoch: 4: Step: 3801/4907, loss=0.352924, lr=0.000018
Train batch 3900
Avg. loss per last 100 batches: 0.094739
Train batch 3900
Avg. loss per last 100 batches: 0.094739
Epoch: 4: Step: 3901/4907, loss=0.003445, lr=0.000018
Epoch: 4: Step: 3901/4907, loss=0.003445, lr=0.000018
Train batch 4000
Avg. loss per last 100 batches: 0.090152
Train batch 4000
Avg. loss per last 100 batches: 0.090152
Epoch: 4: Step: 4001/4907, loss=0.000464, lr=0.000018
Epoch: 4: Step: 4001/4907, loss=0.000464, lr=0.000018
Train batch 4100
Avg. loss per last 100 batches: 0.083771
Train batch 4100
Avg. loss per last 100 batches: 0.083771
Epoch: 4: Step: 4101/4907, loss=0.126245, lr=0.000018
Epoch: 4: Step: 4101/4907, loss=0.126245, lr=0.000018
Train batch 4200
Avg. loss per last 100 batches: 0.127945
Train batch 4200
Avg. loss per last 100 batches: 0.127945
Epoch: 4: Step: 4201/4907, loss=0.084400, lr=0.000018
Epoch: 4: Step: 4201/4907, loss=0.084400, lr=0.000018
Train batch 4300
Avg. loss per last 100 batches: 0.128402
Train batch 4300
Avg. loss per last 100 batches: 0.128402
Epoch: 4: Step: 4301/4907, loss=0.015249, lr=0.000018
Epoch: 4: Step: 4301/4907, loss=0.015249, lr=0.000018
Train batch 4400
Avg. loss per last 100 batches: 0.100540
Train batch 4400
Avg. loss per last 100 batches: 0.100540
Epoch: 4: Step: 4401/4907, loss=0.011383, lr=0.000018
Epoch: 4: Step: 4401/4907, loss=0.011383, lr=0.000018
Train batch 4500
Avg. loss per last 100 batches: 0.123554
Train batch 4500
Avg. loss per last 100 batches: 0.123554
Epoch: 4: Step: 4501/4907, loss=0.188090, lr=0.000018
Epoch: 4: Step: 4501/4907, loss=0.188090, lr=0.000018
Train batch 4600
Avg. loss per last 100 batches: 0.125397
Train batch 4600
Avg. loss per last 100 batches: 0.125397
Epoch: 4: Step: 4601/4907, loss=0.305750, lr=0.000018
Epoch: 4: Step: 4601/4907, loss=0.305750, lr=0.000018
Train batch 4700
Avg. loss per last 100 batches: 0.135577
Train batch 4700
Avg. loss per last 100 batches: 0.135577
Epoch: 4: Step: 4701/4907, loss=0.001432, lr=0.000018
Epoch: 4: Step: 4701/4907, loss=0.001432, lr=0.000018
Train batch 4800
Avg. loss per last 100 batches: 0.085784
Train batch 4800
Avg. loss per last 100 batches: 0.085784
Epoch: 4: Step: 4801/4907, loss=0.012584, lr=0.000018
Epoch: 4: Step: 4801/4907, loss=0.012584, lr=0.000018
Train batch 4900
Avg. loss per last 100 batches: 0.095357
Train batch 4900
Avg. loss per last 100 batches: 0.095357
Epoch: 4: Step: 4901/4907, loss=0.000192, lr=0.000018
Epoch: 4: Step: 4901/4907, loss=0.000192, lr=0.000018
Validation: Epoch: 4 Step: 4907/4907
NLL validation ...
Validation: Epoch: 4 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.584026 sec., loss=0.306627 
Eval step: 99 , used_time=28.454802 sec., loss=0.306627 
Eval step: 199 , used_time=57.577928 sec., loss=0.128771 
Eval step: 199 , used_time=57.448717 sec., loss=0.128771 
Eval step: 299 , used_time=86.590071 sec., loss=0.459040 
Eval step: 299 , used_time=86.460883 sec., loss=0.459040 
Eval step: 399 , used_time=115.629482 sec., loss=0.677000 
Eval step: 399 , used_time=115.500278 sec., loss=0.677000 
Eval step: 499 , used_time=144.501823 sec., loss=0.497888 
Eval step: 499 , used_time=144.631071 sec., loss=0.497888 
NLL Validation: loss = 0.515717. correct prediction ratio  5732/6516 ~  0.879681
NLL Validation: loss = 0.515717. correct prediction ratio  5732/6516 ~  0.879681
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.4.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.4.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.848343 sec., loss=0.306627 
Eval step: 99 , used_time=36.264167 sec., loss=0.306627 
Eval step: 199 , used_time=56.854526 sec., loss=0.128771 
Eval step: 199 , used_time=65.270329 sec., loss=0.128771 
Eval step: 299 , used_time=94.264285 sec., loss=0.459040 
Eval step: 299 , used_time=85.848584 sec., loss=0.459040 
Eval step: 399 , used_time=114.952577 sec., loss=0.677000 
Eval step: 399 , used_time=123.368394 sec., loss=0.677000 
Eval step: 499 , used_time=143.947957 sec., loss=0.497888 
Eval step: 499 , used_time=152.363837 sec., loss=0.497888 
NLL Validation: loss = 0.515717. correct prediction ratio  5732/6516 ~  0.879681
NLL Validation: loss = 0.515717. correct prediction ratio  5732/6516 ~  0.879681
Av Loss per epoch=0.107302
epoch total correct predictions=56947
***** Epoch 5 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.4.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.4.4907
Av Loss per epoch=0.107302
epoch total correct predictions=56947
***** Epoch 5 *****
Epoch: 5: Step: 1/4907, loss=0.101990, lr=0.000018
Epoch: 5: Step: 1/4907, loss=0.101990, lr=0.000018
Train batch 100
Avg. loss per last 100 batches: 0.065418
Train batch 100
Avg. loss per last 100 batches: 0.065418
Epoch: 5: Step: 101/4907, loss=0.001004, lr=0.000018
Epoch: 5: Step: 101/4907, loss=0.001004, lr=0.000018
Train batch 200
Avg. loss per last 100 batches: 0.063759
Train batch 200
Avg. loss per last 100 batches: 0.063759
Epoch: 5: Step: 201/4907, loss=0.000130, lr=0.000018
Epoch: 5: Step: 201/4907, loss=0.000130, lr=0.000018
Train batch 300
Avg. loss per last 100 batches: 0.090234
Train batch 300
Avg. loss per last 100 batches: 0.090234
Epoch: 5: Step: 301/4907, loss=0.010588, lr=0.000018
Epoch: 5: Step: 301/4907, loss=0.010588, lr=0.000018
Train batch 400
Avg. loss per last 100 batches: 0.088437
Train batch 400
Avg. loss per last 100 batches: 0.088437
Epoch: 5: Step: 401/4907, loss=0.063138, lr=0.000018
Epoch: 5: Step: 401/4907, loss=0.063138, lr=0.000018
Train batch 500
Avg. loss per last 100 batches: 0.106693
Train batch 500
Avg. loss per last 100 batches: 0.106693
Epoch: 5: Step: 501/4907, loss=0.225402, lr=0.000018
Epoch: 5: Step: 501/4907, loss=0.225402, lr=0.000018
Train batch 600
Avg. loss per last 100 batches: 0.129924
Train batch 600
Avg. loss per last 100 batches: 0.129924
Epoch: 5: Step: 601/4907, loss=0.037995, lr=0.000018
Epoch: 5: Step: 601/4907, loss=0.037995, lr=0.000018
Train batch 700
Avg. loss per last 100 batches: 0.101150
Train batch 700
Avg. loss per last 100 batches: 0.101150
Epoch: 5: Step: 701/4907, loss=0.002110, lr=0.000018
Epoch: 5: Step: 701/4907, loss=0.002110, lr=0.000018
Train batch 800
Avg. loss per last 100 batches: 0.103602
Train batch 800
Avg. loss per last 100 batches: 0.103602
Epoch: 5: Step: 801/4907, loss=0.042831, lr=0.000018
Epoch: 5: Step: 801/4907, loss=0.042831, lr=0.000018
Train batch 900
Train batch 900
Avg. loss per last 100 batches: 0.089868
Avg. loss per last 100 batches: 0.089868
Epoch: 5: Step: 901/4907, loss=0.086378, lr=0.000018
Epoch: 5: Step: 901/4907, loss=0.086378, lr=0.000018
Train batch 1000
Avg. loss per last 100 batches: 0.080897
Train batch 1000
Avg. loss per last 100 batches: 0.080897
Epoch: 5: Step: 1001/4907, loss=0.012027, lr=0.000018
Epoch: 5: Step: 1001/4907, loss=0.012027, lr=0.000018
Train batch 1100
Avg. loss per last 100 batches: 0.072062
Train batch 1100
Avg. loss per last 100 batches: 0.072062
Epoch: 5: Step: 1101/4907, loss=0.189358, lr=0.000017
Epoch: 5: Step: 1101/4907, loss=0.189358, lr=0.000017
Train batch 1200
Avg. loss per last 100 batches: 0.134849
Train batch 1200
Avg. loss per last 100 batches: 0.134849
Epoch: 5: Step: 1201/4907, loss=0.106454, lr=0.000017
Epoch: 5: Step: 1201/4907, loss=0.106454, lr=0.000017
Train batch 1300
Avg. loss per last 100 batches: 0.103551
Train batch 1300
Avg. loss per last 100 batches: 0.103551
Epoch: 5: Step: 1301/4907, loss=0.005159, lr=0.000017
Epoch: 5: Step: 1301/4907, loss=0.005159, lr=0.000017
Train batch 1400
Avg. loss per last 100 batches: 0.088496
Train batch 1400
Avg. loss per last 100 batches: 0.088496
Epoch: 5: Step: 1401/4907, loss=0.014665, lr=0.000017
Epoch: 5: Step: 1401/4907, loss=0.014665, lr=0.000017
Train batch 1500
Avg. loss per last 100 batches: 0.106988
Train batch 1500
Avg. loss per last 100 batches: 0.106988
Epoch: 5: Step: 1501/4907, loss=0.002730, lr=0.000017
Epoch: 5: Step: 1501/4907, loss=0.002730, lr=0.000017
Train batch 1600
Avg. loss per last 100 batches: 0.097487
Train batch 1600
Avg. loss per last 100 batches: 0.097487
Epoch: 5: Step: 1601/4907, loss=0.001664, lr=0.000017
Epoch: 5: Step: 1601/4907, loss=0.001664, lr=0.000017
Train batch 1700
Avg. loss per last 100 batches: 0.085656
Train batch 1700
Avg. loss per last 100 batches: 0.085656
Epoch: 5: Step: 1701/4907, loss=0.001579, lr=0.000017
Epoch: 5: Step: 1701/4907, loss=0.001579, lr=0.000017
Train batch 1800
Avg. loss per last 100 batches: 0.107013
Train batch 1800
Avg. loss per last 100 batches: 0.107013
Epoch: 5: Step: 1801/4907, loss=0.001013, lr=0.000017
Epoch: 5: Step: 1801/4907, loss=0.001013, lr=0.000017
Train batch 1900
Avg. loss per last 100 batches: 0.091308
Train batch 1900
Avg. loss per last 100 batches: 0.091308
Epoch: 5: Step: 1901/4907, loss=0.034151, lr=0.000017
Epoch: 5: Step: 1901/4907, loss=0.034151, lr=0.000017
Train batch 2000
Avg. loss per last 100 batches: 0.090414
Train batch 2000
Avg. loss per last 100 batches: 0.090414
Epoch: 5: Step: 2001/4907, loss=0.090644, lr=0.000017
Epoch: 5: Step: 2001/4907, loss=0.090644, lr=0.000017
Train batch 2100
Avg. loss per last 100 batches: 0.087799
Train batch 2100
Avg. loss per last 100 batches: 0.087799
Epoch: 5: Step: 2101/4907, loss=0.045329, lr=0.000017
Epoch: 5: Step: 2101/4907, loss=0.045329, lr=0.000017
Train batch 2200
Avg. loss per last 100 batches: 0.104915
Train batch 2200
Avg. loss per last 100 batches: 0.104915
Epoch: 5: Step: 2201/4907, loss=0.003649, lr=0.000017
Epoch: 5: Step: 2201/4907, loss=0.003649, lr=0.000017
Train batch 2300
Avg. loss per last 100 batches: 0.068121
Train batch 2300
Avg. loss per last 100 batches: 0.068121
Epoch: 5: Step: 2301/4907, loss=0.001796, lr=0.000017
Epoch: 5: Step: 2301/4907, loss=0.001796, lr=0.000017
Train batch 2400
Avg. loss per last 100 batches: 0.079499
Train batch 2400
Avg. loss per last 100 batches: 0.079499
Epoch: 5: Step: 2401/4907, loss=0.305339, lr=0.000017
Epoch: 5: Step: 2401/4907, loss=0.305339, lr=0.000017
Train batch 2500
Avg. loss per last 100 batches: 0.083937
Train batch 2500
Avg. loss per last 100 batches: 0.083937
Epoch: 5: Step: 2501/4907, loss=0.118258, lr=0.000017
Epoch: 5: Step: 2501/4907, loss=0.118258, lr=0.000017
Train batch 2600
Avg. loss per last 100 batches: 0.066320
Train batch 2600
Avg. loss per last 100 batches: 0.066320
Epoch: 5: Step: 2601/4907, loss=0.001160, lr=0.000017
Epoch: 5: Step: 2601/4907, loss=0.001160, lr=0.000017
Train batch 2700
Avg. loss per last 100 batches: 0.124512
Train batch 2700
Avg. loss per last 100 batches: 0.124512
Epoch: 5: Step: 2701/4907, loss=0.015503, lr=0.000017
Epoch: 5: Step: 2701/4907, loss=0.015503, lr=0.000017
Train batch 2800
Avg. loss per last 100 batches: 0.094713
Train batch 2800
Avg. loss per last 100 batches: 0.094713
Epoch: 5: Step: 2801/4907, loss=0.121204, lr=0.000017
Epoch: 5: Step: 2801/4907, loss=0.121204, lr=0.000017
Train batch 2900
Train batch 2900
Avg. loss per last 100 batches: 0.088459
Avg. loss per last 100 batches: 0.088459
Epoch: 5: Step: 2901/4907, loss=0.266181, lr=0.000017
Epoch: 5: Step: 2901/4907, loss=0.266181, lr=0.000017
Train batch 3000
Avg. loss per last 100 batches: 0.101703
Train batch 3000
Avg. loss per last 100 batches: 0.101703
Epoch: 5: Step: 3001/4907, loss=0.043851, lr=0.000017
Epoch: 5: Step: 3001/4907, loss=0.043851, lr=0.000017
Train batch 3100
Avg. loss per last 100 batches: 0.082841
Train batch 3100
Avg. loss per last 100 batches: 0.082841
Epoch: 5: Step: 3101/4907, loss=0.041893, lr=0.000017
Epoch: 5: Step: 3101/4907, loss=0.041893, lr=0.000017
Train batch 3200
Avg. loss per last 100 batches: 0.146831
Train batch 3200
Avg. loss per last 100 batches: 0.146831
Epoch: 5: Step: 3201/4907, loss=0.039111, lr=0.000017
Epoch: 5: Step: 3201/4907, loss=0.039111, lr=0.000017
Train batch 3300
Avg. loss per last 100 batches: 0.103579
Train batch 3300
Avg. loss per last 100 batches: 0.103579
Epoch: 5: Step: 3301/4907, loss=0.015124, lr=0.000017
Epoch: 5: Step: 3301/4907, loss=0.015124, lr=0.000017
Train batch 3400
Avg. loss per last 100 batches: 0.096956
Train batch 3400
Avg. loss per last 100 batches: 0.096956
Epoch: 5: Step: 3401/4907, loss=0.015886, lr=0.000017
Epoch: 5: Step: 3401/4907, loss=0.015886, lr=0.000017
Train batch 3500
Avg. loss per last 100 batches: 0.102263
Train batch 3500
Avg. loss per last 100 batches: 0.102263
Epoch: 5: Step: 3501/4907, loss=0.010814, lr=0.000017
Epoch: 5: Step: 3501/4907, loss=0.010814, lr=0.000017
Train batch 3600
Avg. loss per last 100 batches: 0.093490
Train batch 3600
Avg. loss per last 100 batches: 0.093490
Epoch: 5: Step: 3601/4907, loss=0.299317, lr=0.000017
Epoch: 5: Step: 3601/4907, loss=0.299317, lr=0.000017
Train batch 3700
Avg. loss per last 100 batches: 0.130583
Train batch 3700
Avg. loss per last 100 batches: 0.130583
Epoch: 5: Step: 3701/4907, loss=0.011145, lr=0.000017
Epoch: 5: Step: 3701/4907, loss=0.011145, lr=0.000017
Train batch 3800
Avg. loss per last 100 batches: 0.101610
Train batch 3800
Avg. loss per last 100 batches: 0.101610
Epoch: 5: Step: 3801/4907, loss=0.023747, lr=0.000017
Epoch: 5: Step: 3801/4907, loss=0.023747, lr=0.000017
Train batch 3900
Avg. loss per last 100 batches: 0.088222
Train batch 3900
Avg. loss per last 100 batches: 0.088222
Epoch: 5: Step: 3901/4907, loss=0.000257, lr=0.000017
Epoch: 5: Step: 3901/4907, loss=0.000257, lr=0.000017
Train batch 4000
Avg. loss per last 100 batches: 0.114950
Train batch 4000
Avg. loss per last 100 batches: 0.114950
Epoch: 5: Step: 4001/4907, loss=0.124926, lr=0.000017
Epoch: 5: Step: 4001/4907, loss=0.124926, lr=0.000017
Train batch 4100
Avg. loss per last 100 batches: 0.083862
Train batch 4100
Avg. loss per last 100 batches: 0.083862
Epoch: 5: Step: 4101/4907, loss=0.001152, lr=0.000017
Epoch: 5: Step: 4101/4907, loss=0.001152, lr=0.000017
Train batch 4200
Avg. loss per last 100 batches: 0.113725
Train batch 4200
Avg. loss per last 100 batches: 0.113725
Epoch: 5: Step: 4201/4907, loss=0.254693, lr=0.000017
Epoch: 5: Step: 4201/4907, loss=0.254693, lr=0.000017
Train batch 4300
Avg. loss per last 100 batches: 0.091935
Train batch 4300
Avg. loss per last 100 batches: 0.091935
Epoch: 5: Step: 4301/4907, loss=0.332722, lr=0.000017
Epoch: 5: Step: 4301/4907, loss=0.332722, lr=0.000017
Train batch 4400
Avg. loss per last 100 batches: 0.109474
Train batch 4400
Avg. loss per last 100 batches: 0.109474
Epoch: 5: Step: 4401/4907, loss=0.040131, lr=0.000017
Epoch: 5: Step: 4401/4907, loss=0.040131, lr=0.000017
Train batch 4500
Avg. loss per last 100 batches: 0.105834
Train batch 4500
Avg. loss per last 100 batches: 0.105834
Epoch: 5: Step: 4501/4907, loss=0.464607, lr=0.000017
Epoch: 5: Step: 4501/4907, loss=0.464607, lr=0.000017
Train batch 4600
Avg. loss per last 100 batches: 0.127676
Train batch 4600
Avg. loss per last 100 batches: 0.127676
Epoch: 5: Step: 4601/4907, loss=0.302285, lr=0.000017
Epoch: 5: Step: 4601/4907, loss=0.302285, lr=0.000017
Train batch 4700
Avg. loss per last 100 batches: 0.107954
Train batch 4700
Avg. loss per last 100 batches: 0.107954
Epoch: 5: Step: 4701/4907, loss=0.038110, lr=0.000017
Epoch: 5: Step: 4701/4907, loss=0.038110, lr=0.000017
Train batch 4800
Avg. loss per last 100 batches: 0.085489
Train batch 4800
Avg. loss per last 100 batches: 0.085489
Epoch: 5: Step: 4801/4907, loss=0.045731, lr=0.000017
Epoch: 5: Step: 4801/4907, loss=0.045731, lr=0.000017
Train batch 4900
Avg. loss per last 100 batches: 0.096616
Train batch 4900
Avg. loss per last 100 batches: 0.096616
Epoch: 5: Step: 4901/4907, loss=0.003633, lr=0.000017
Epoch: 5: Step: 4901/4907, loss=0.003633, lr=0.000017
Validation: Epoch: 5 Step: 4907/4907
NLL validation ...
Validation: Epoch: 5 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=29.714611 sec., loss=0.010149 
Eval step: 99 , used_time=28.326646 sec., loss=0.010149 
Eval step: 199 , used_time=57.394906 sec., loss=0.786330 
Eval step: 199 , used_time=58.782848 sec., loss=0.786330 
Eval step: 299 , used_time=87.867280 sec., loss=1.009255 
Eval step: 299 , used_time=86.479336 sec., loss=1.009255 
Eval step: 399 , used_time=115.560994 sec., loss=0.176336 
Eval step: 399 , used_time=116.948940 sec., loss=0.176336 
Eval step: 499 , used_time=146.061239 sec., loss=0.323168 
Eval step: 499 , used_time=144.673275 sec., loss=0.323168 
NLL Validation: loss = 0.560177. correct prediction ratio  5739/6516 ~  0.880755
NLL Validation: loss = 0.560177. correct prediction ratio  5739/6516 ~  0.880755
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.5.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.5.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.960869 sec., loss=0.010149 
Eval step: 99 , used_time=36.223844 sec., loss=0.010149 
Eval step: 199 , used_time=65.289461 sec., loss=0.786330 
Eval step: 199 , used_time=57.026514 sec., loss=0.786330 
Eval step: 299 , used_time=86.106433 sec., loss=1.009255 
Eval step: 299 , used_time=94.369379 sec., loss=1.009255 
Eval step: 399 , used_time=115.200358 sec., loss=0.176336 
Eval step: 399 , used_time=123.463305 sec., loss=0.176336 
Eval step: 499 , used_time=152.524302 sec., loss=0.323168 
Eval step: 499 , used_time=144.261366 sec., loss=0.323168 
NLL Validation: loss = 0.560177. correct prediction ratio  5739/6516 ~  0.880755
NLL Validation: loss = 0.560177. correct prediction ratio  5739/6516 ~  0.880755
Av Loss per epoch=0.097641
epoch total correct predictions=57142
***** Epoch 6 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.5.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.5.4907
Av Loss per epoch=0.097641
epoch total correct predictions=57142
***** Epoch 6 *****
Epoch: 6: Step: 1/4907, loss=0.378725, lr=0.000017
Epoch: 6: Step: 1/4907, loss=0.378725, lr=0.000017
Train batch 100
Avg. loss per last 100 batches: 0.096737
Train batch 100
Avg. loss per last 100 batches: 0.096737
Epoch: 6: Step: 101/4907, loss=0.000051, lr=0.000017
Epoch: 6: Step: 101/4907, loss=0.000051, lr=0.000017
Train batch 200
Avg. loss per last 100 batches: 0.101894
Train batch 200
Avg. loss per last 100 batches: 0.101894
Epoch: 6: Step: 201/4907, loss=0.277093, lr=0.000017
Epoch: 6: Step: 201/4907, loss=0.277093, lr=0.000017
Train batch 300
Avg. loss per last 100 batches: 0.105500
Train batch 300
Avg. loss per last 100 batches: 0.105500
Epoch: 6: Step: 301/4907, loss=0.146528, lr=0.000017
Epoch: 6: Step: 301/4907, loss=0.146528, lr=0.000017
Train batch 400
Avg. loss per last 100 batches: 0.073031
Train batch 400
Avg. loss per last 100 batches: 0.073031
Epoch: 6: Step: 401/4907, loss=0.000348, lr=0.000017
Epoch: 6: Step: 401/4907, loss=0.000348, lr=0.000017
Train batch 500
Avg. loss per last 100 batches: 0.100630
Train batch 500
Avg. loss per last 100 batches: 0.100630
Epoch: 6: Step: 501/4907, loss=0.000050, lr=0.000017
Epoch: 6: Step: 501/4907, loss=0.000050, lr=0.000017
Train batch 600
Avg. loss per last 100 batches: 0.090587
Train batch 600
Avg. loss per last 100 batches: 0.090587
Epoch: 6: Step: 601/4907, loss=0.012344, lr=0.000017
Epoch: 6: Step: 601/4907, loss=0.012344, lr=0.000017
Train batch 700
Avg. loss per last 100 batches: 0.099817
Train batch 700
Avg. loss per last 100 batches: 0.099817
Epoch: 6: Step: 701/4907, loss=0.001838, lr=0.000017
Epoch: 6: Step: 701/4907, loss=0.001838, lr=0.000017
Train batch 800
Avg. loss per last 100 batches: 0.082657
Train batch 800
Avg. loss per last 100 batches: 0.082657
Epoch: 6: Step: 801/4907, loss=0.000067, lr=0.000017
Epoch: 6: Step: 801/4907, loss=0.000067, lr=0.000017
Train batch 900
Train batch 900
Avg. loss per last 100 batches: 0.086710
Avg. loss per last 100 batches: 0.086710
Epoch: 6: Step: 901/4907, loss=0.451775, lr=0.000017
Epoch: 6: Step: 901/4907, loss=0.451775, lr=0.000017
Train batch 1000
Avg. loss per last 100 batches: 0.068726
Train batch 1000
Avg. loss per last 100 batches: 0.068726
Epoch: 6: Step: 1001/4907, loss=0.004435, lr=0.000017
Epoch: 6: Step: 1001/4907, loss=0.004435, lr=0.000017
Train batch 1100
Avg. loss per last 100 batches: 0.075515
Train batch 1100
Avg. loss per last 100 batches: 0.075515
Epoch: 6: Step: 1101/4907, loss=0.000037, lr=0.000017
Epoch: 6: Step: 1101/4907, loss=0.000037, lr=0.000017
Train batch 1200
Avg. loss per last 100 batches: 0.060234
Train batch 1200
Avg. loss per last 100 batches: 0.060234
Epoch: 6: Step: 1201/4907, loss=0.000031, lr=0.000017
Epoch: 6: Step: 1201/4907, loss=0.000031, lr=0.000017
Train batch 1300
Avg. loss per last 100 batches: 0.097634
Train batch 1300
Avg. loss per last 100 batches: 0.097634
Epoch: 6: Step: 1301/4907, loss=0.000418, lr=0.000017
Epoch: 6: Step: 1301/4907, loss=0.000418, lr=0.000017
Train batch 1400
Avg. loss per last 100 batches: 0.081273
Train batch 1400
Avg. loss per last 100 batches: 0.081273
Epoch: 6: Step: 1401/4907, loss=0.000054, lr=0.000017
Epoch: 6: Step: 1401/4907, loss=0.000054, lr=0.000017
Train batch 1500
Avg. loss per last 100 batches: 0.094356
Train batch 1500
Avg. loss per last 100 batches: 0.094356
Epoch: 6: Step: 1501/4907, loss=0.368932, lr=0.000017
Epoch: 6: Step: 1501/4907, loss=0.368932, lr=0.000017
Train batch 1600
Avg. loss per last 100 batches: 0.072749
Train batch 1600
Avg. loss per last 100 batches: 0.072749
Epoch: 6: Step: 1601/4907, loss=0.001388, lr=0.000017
Epoch: 6: Step: 1601/4907, loss=0.001388, lr=0.000017
Train batch 1700
Avg. loss per last 100 batches: 0.094738
Train batch 1700
Avg. loss per last 100 batches: 0.094738
Epoch: 6: Step: 1701/4907, loss=0.009944, lr=0.000017
Epoch: 6: Step: 1701/4907, loss=0.009944, lr=0.000017
Train batch 1800
Avg. loss per last 100 batches: 0.092816
Train batch 1800
Avg. loss per last 100 batches: 0.092816
Epoch: 6: Step: 1801/4907, loss=0.097529, lr=0.000017
Epoch: 6: Step: 1801/4907, loss=0.097529, lr=0.000017
Train batch 1900
Avg. loss per last 100 batches: 0.081563
Train batch 1900
Avg. loss per last 100 batches: 0.081563
Epoch: 6: Step: 1901/4907, loss=0.001177, lr=0.000017
Epoch: 6: Step: 1901/4907, loss=0.001177, lr=0.000017
Train batch 2000
Avg. loss per last 100 batches: 0.068301
Train batch 2000
Avg. loss per last 100 batches: 0.068301
Epoch: 6: Step: 2001/4907, loss=0.585829, lr=0.000017
Epoch: 6: Step: 2001/4907, loss=0.585829, lr=0.000017
Train batch 2100
Avg. loss per last 100 batches: 0.112286
Train batch 2100
Avg. loss per last 100 batches: 0.112286
Epoch: 6: Step: 2101/4907, loss=0.000595, lr=0.000017
Epoch: 6: Step: 2101/4907, loss=0.000595, lr=0.000017
Train batch 2200
Avg. loss per last 100 batches: 0.134307
Train batch 2200
Avg. loss per last 100 batches: 0.134307
Epoch: 6: Step: 2201/4907, loss=0.000854, lr=0.000017
Epoch: 6: Step: 2201/4907, loss=0.000854, lr=0.000017
Train batch 2300
Avg. loss per last 100 batches: 0.085777
Train batch 2300
Avg. loss per last 100 batches: 0.085777
Epoch: 6: Step: 2301/4907, loss=0.032577, lr=0.000017
Epoch: 6: Step: 2301/4907, loss=0.032577, lr=0.000017
Train batch 2400
Avg. loss per last 100 batches: 0.071501
Train batch 2400
Avg. loss per last 100 batches: 0.071501
Epoch: 6: Step: 2401/4907, loss=0.002471, lr=0.000017
Epoch: 6: Step: 2401/4907, loss=0.002471, lr=0.000017
Train batch 2500
Avg. loss per last 100 batches: 0.077398
Train batch 2500
Avg. loss per last 100 batches: 0.077398
Epoch: 6: Step: 2501/4907, loss=0.000190, lr=0.000017
Epoch: 6: Step: 2501/4907, loss=0.000190, lr=0.000017
Train batch 2600
Train batch 2600
Avg. loss per last 100 batches: 0.072408
Avg. loss per last 100 batches: 0.072408
Epoch: 6: Step: 2601/4907, loss=0.000737, lr=0.000017
Epoch: 6: Step: 2601/4907, loss=0.000737, lr=0.000017
Train batch 2700
Avg. loss per last 100 batches: 0.090639
Train batch 2700
Avg. loss per last 100 batches: 0.090639
Epoch: 6: Step: 2701/4907, loss=0.000766, lr=0.000017
Epoch: 6: Step: 2701/4907, loss=0.000766, lr=0.000017
Train batch 2800
Avg. loss per last 100 batches: 0.099195
Train batch 2800
Avg. loss per last 100 batches: 0.099195
Epoch: 6: Step: 2801/4907, loss=0.012895, lr=0.000017
Epoch: 6: Step: 2801/4907, loss=0.012895, lr=0.000017
Train batch 2900
Avg. loss per last 100 batches: 0.097264
Train batch 2900
Avg. loss per last 100 batches: 0.097264
Epoch: 6: Step: 2901/4907, loss=0.272441, lr=0.000017
Epoch: 6: Step: 2901/4907, loss=0.272441, lr=0.000017
Train batch 3000
Avg. loss per last 100 batches: 0.100313
Train batch 3000
Avg. loss per last 100 batches: 0.100313
Epoch: 6: Step: 3001/4907, loss=0.133421, lr=0.000017
Epoch: 6: Step: 3001/4907, loss=0.133421, lr=0.000017
Train batch 3100
Avg. loss per last 100 batches: 0.102799
Train batch 3100
Avg. loss per last 100 batches: 0.102799
Epoch: 6: Step: 3101/4907, loss=0.000444, lr=0.000017
Epoch: 6: Step: 3101/4907, loss=0.000444, lr=0.000017
Train batch 3200
Avg. loss per last 100 batches: 0.070003
Train batch 3200
Avg. loss per last 100 batches: 0.070003
Epoch: 6: Step: 3201/4907, loss=0.000113, lr=0.000017
Epoch: 6: Step: 3201/4907, loss=0.000113, lr=0.000017
Train batch 3300
Avg. loss per last 100 batches: 0.079333
Train batch 3300
Avg. loss per last 100 batches: 0.079333
Epoch: 6: Step: 3301/4907, loss=0.002829, lr=0.000017
Epoch: 6: Step: 3301/4907, loss=0.002829, lr=0.000017
Train batch 3400
Avg. loss per last 100 batches: 0.077684
Train batch 3400
Avg. loss per last 100 batches: 0.077684
Epoch: 6: Step: 3401/4907, loss=0.396310, lr=0.000017
Epoch: 6: Step: 3401/4907, loss=0.396310, lr=0.000017
Train batch 3500
Avg. loss per last 100 batches: 0.086425
Train batch 3500
Avg. loss per last 100 batches: 0.086425
Epoch: 6: Step: 3501/4907, loss=0.010679, lr=0.000017
Epoch: 6: Step: 3501/4907, loss=0.010679, lr=0.000017
Train batch 3600
Avg. loss per last 100 batches: 0.092924
Train batch 3600
Avg. loss per last 100 batches: 0.092924
Epoch: 6: Step: 3601/4907, loss=0.190662, lr=0.000017
Epoch: 6: Step: 3601/4907, loss=0.190662, lr=0.000017
Train batch 3700
Avg. loss per last 100 batches: 0.069668
Train batch 3700
Avg. loss per last 100 batches: 0.069668
Epoch: 6: Step: 3701/4907, loss=0.000005, lr=0.000017
Epoch: 6: Step: 3701/4907, loss=0.000005, lr=0.000017
Train batch 3800
Avg. loss per last 100 batches: 0.058090
Train batch 3800
Avg. loss per last 100 batches: 0.058090
Epoch: 6: Step: 3801/4907, loss=0.000964, lr=0.000017
Epoch: 6: Step: 3801/4907, loss=0.000964, lr=0.000017
Train batch 3900
Avg. loss per last 100 batches: 0.087477
Train batch 3900
Avg. loss per last 100 batches: 0.087477
Epoch: 6: Step: 3901/4907, loss=0.002059, lr=0.000017
Epoch: 6: Step: 3901/4907, loss=0.002059, lr=0.000017
Train batch 4000
Avg. loss per last 100 batches: 0.098058
Train batch 4000
Avg. loss per last 100 batches: 0.098058
Epoch: 6: Step: 4001/4907, loss=0.000003, lr=0.000017
Epoch: 6: Step: 4001/4907, loss=0.000003, lr=0.000017
Train batch 4100
Avg. loss per last 100 batches: 0.076271
Train batch 4100
Avg. loss per last 100 batches: 0.076271
Epoch: 6: Step: 4101/4907, loss=0.010690, lr=0.000017
Epoch: 6: Step: 4101/4907, loss=0.010690, lr=0.000017
Train batch 4200
Avg. loss per last 100 batches: 0.094515
Train batch 4200
Avg. loss per last 100 batches: 0.094515
Epoch: 6: Step: 4201/4907, loss=0.003195, lr=0.000017
Epoch: 6: Step: 4201/4907, loss=0.003195, lr=0.000017
Train batch 4300
Avg. loss per last 100 batches: 0.091310
Train batch 4300
Avg. loss per last 100 batches: 0.091310
Epoch: 6: Step: 4301/4907, loss=0.007376, lr=0.000017
Epoch: 6: Step: 4301/4907, loss=0.007376, lr=0.000017
Train batch 4400
Avg. loss per last 100 batches: 0.074341
Train batch 4400
Avg. loss per last 100 batches: 0.074341
Epoch: 6: Step: 4401/4907, loss=0.000826, lr=0.000017
Epoch: 6: Step: 4401/4907, loss=0.000826, lr=0.000017
Train batch 4500
Avg. loss per last 100 batches: 0.082182
Train batch 4500
Avg. loss per last 100 batches: 0.082182
Epoch: 6: Step: 4501/4907, loss=0.000173, lr=0.000017
Epoch: 6: Step: 4501/4907, loss=0.000173, lr=0.000017
Train batch 4600
Avg. loss per last 100 batches: 0.066895
Train batch 4600
Avg. loss per last 100 batches: 0.066895
Epoch: 6: Step: 4601/4907, loss=0.000008, lr=0.000017
Epoch: 6: Step: 4601/4907, loss=0.000008, lr=0.000017
Train batch 4700
Avg. loss per last 100 batches: 0.101400
Train batch 4700
Avg. loss per last 100 batches: 0.101400
Epoch: 6: Step: 4701/4907, loss=0.000179, lr=0.000017
Epoch: 6: Step: 4701/4907, loss=0.000179, lr=0.000017
Train batch 4800
Avg. loss per last 100 batches: 0.092067
Train batch 4800
Avg. loss per last 100 batches: 0.092067
Epoch: 6: Step: 4801/4907, loss=0.102185, lr=0.000017
Epoch: 6: Step: 4801/4907, loss=0.102185, lr=0.000017
Train batch 4900
Avg. loss per last 100 batches: 0.129384
Train batch 4900
Avg. loss per last 100 batches: 0.129384
Epoch: 6: Step: 4901/4907, loss=0.002799, lr=0.000017
Epoch: 6: Step: 4901/4907, loss=0.002799, lr=0.000017
Validation: Epoch: 6 Step: 4907/4907
NLL validation ...
Validation: Epoch: 6 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=29.344657 sec., loss=0.016341 
Eval step: 99 , used_time=28.310895 sec., loss=0.016341 
Eval step: 199 , used_time=58.375244 sec., loss=0.184667 
Eval step: 199 , used_time=57.341444 sec., loss=0.184667 
Eval step: 299 , used_time=86.379302 sec., loss=0.769093 
Eval step: 299 , used_time=87.413107 sec., loss=0.769093 
Eval step: 399 , used_time=116.474859 sec., loss=0.430086 
Eval step: 399 , used_time=115.441031 sec., loss=0.430086 
Eval step: 499 , used_time=144.482489 sec., loss=0.480656 
Eval step: 499 , used_time=145.516335 sec., loss=0.480656 
NLL Validation: loss = 0.489618. correct prediction ratio  5847/6516 ~  0.897330
NLL Validation: loss = 0.489618. correct prediction ratio  5847/6516 ~  0.897330
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.6.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.6.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.899105 sec., loss=0.016341 
Eval step: 99 , used_time=33.901947 sec., loss=0.016341 
Eval step: 199 , used_time=62.931384 sec., loss=0.184667 
Eval step: 199 , used_time=56.928526 sec., loss=0.184667 
Eval step: 299 , used_time=85.980535 sec., loss=0.769093 
Eval step: 299 , used_time=91.983396 sec., loss=0.769093 
Eval step: 399 , used_time=121.032466 sec., loss=0.430086 
Eval step: 399 , used_time=115.029665 sec., loss=0.430086 
Eval step: 499 , used_time=144.116852 sec., loss=0.480656 
Eval step: 499 , used_time=150.119714 sec., loss=0.480656 
NLL Validation: loss = 0.489618. correct prediction ratio  5847/6516 ~  0.897330
NLL Validation: loss = 0.489618. correct prediction ratio  5847/6516 ~  0.897330
Av Loss per epoch=0.087754
epoch total correct predictions=57317
***** Epoch 7 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.6.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.6.4907
Av Loss per epoch=0.087754
epoch total correct predictions=57317
***** Epoch 7 *****
Epoch: 7: Step: 1/4907, loss=0.000480, lr=0.000017
Epoch: 7: Step: 1/4907, loss=0.000480, lr=0.000017
Train batch 100
Avg. loss per last 100 batches: 0.065679
Train batch 100
Avg. loss per last 100 batches: 0.065679
Epoch: 7: Step: 101/4907, loss=0.000625, lr=0.000017
Epoch: 7: Step: 101/4907, loss=0.000625, lr=0.000017
Train batch 200
Avg. loss per last 100 batches: 0.114472
Train batch 200
Avg. loss per last 100 batches: 0.114472
Epoch: 7: Step: 201/4907, loss=0.000214, lr=0.000017
Epoch: 7: Step: 201/4907, loss=0.000214, lr=0.000017
Train batch 300
Avg. loss per last 100 batches: 0.065210
Train batch 300
Avg. loss per last 100 batches: 0.065210
Epoch: 7: Step: 301/4907, loss=0.000094, lr=0.000017
Epoch: 7: Step: 301/4907, loss=0.000094, lr=0.000017
Train batch 400
Avg. loss per last 100 batches: 0.080226
Train batch 400
Avg. loss per last 100 batches: 0.080226
Epoch: 7: Step: 401/4907, loss=0.113557, lr=0.000017
Epoch: 7: Step: 401/4907, loss=0.113557, lr=0.000017
Train batch 500
Avg. loss per last 100 batches: 0.063933
Train batch 500
Avg. loss per last 100 batches: 0.063933
Epoch: 7: Step: 501/4907, loss=0.000842, lr=0.000017
Epoch: 7: Step: 501/4907, loss=0.000842, lr=0.000017
Train batch 600
Avg. loss per last 100 batches: 0.096065
Train batch 600
Avg. loss per last 100 batches: 0.096065
Epoch: 7: Step: 601/4907, loss=0.022144, lr=0.000017
Epoch: 7: Step: 601/4907, loss=0.022144, lr=0.000017
Train batch 700
Avg. loss per last 100 batches: 0.086925
Train batch 700
Avg. loss per last 100 batches: 0.086925
Epoch: 7: Step: 701/4907, loss=0.000666, lr=0.000017
Epoch: 7: Step: 701/4907, loss=0.000666, lr=0.000017
Train batch 800
Avg. loss per last 100 batches: 0.065456
Train batch 800
Avg. loss per last 100 batches: 0.065456
Epoch: 7: Step: 801/4907, loss=0.002340, lr=0.000017
Epoch: 7: Step: 801/4907, loss=0.002340, lr=0.000017
Train batch 900
Avg. loss per last 100 batches: 0.076763
Train batch 900
Avg. loss per last 100 batches: 0.076763
Epoch: 7: Step: 901/4907, loss=0.000115, lr=0.000017
Epoch: 7: Step: 901/4907, loss=0.000115, lr=0.000017
Train batch 1000
Avg. loss per last 100 batches: 0.078586
Train batch 1000
Avg. loss per last 100 batches: 0.078586
Epoch: 7: Step: 1001/4907, loss=0.000090, lr=0.000017
Epoch: 7: Step: 1001/4907, loss=0.000090, lr=0.000017
Train batch 1100
Avg. loss per last 100 batches: 0.080666
Train batch 1100
Avg. loss per last 100 batches: 0.080666
Epoch: 7: Step: 1101/4907, loss=0.025755, lr=0.000016
Epoch: 7: Step: 1101/4907, loss=0.025755, lr=0.000016
Train batch 1200
Avg. loss per last 100 batches: 0.071405
Train batch 1200
Avg. loss per last 100 batches: 0.071405
Epoch: 7: Step: 1201/4907, loss=0.007574, lr=0.000016
Epoch: 7: Step: 1201/4907, loss=0.007574, lr=0.000016
Train batch 1300
Avg. loss per last 100 batches: 0.106337
Train batch 1300
Avg. loss per last 100 batches: 0.106337
Epoch: 7: Step: 1301/4907, loss=0.523521, lr=0.000016
Epoch: 7: Step: 1301/4907, loss=0.523521, lr=0.000016
Train batch 1400
Avg. loss per last 100 batches: 0.097387
Train batch 1400
Avg. loss per last 100 batches: 0.097387
Epoch: 7: Step: 1401/4907, loss=0.073359, lr=0.000016
Epoch: 7: Step: 1401/4907, loss=0.073359, lr=0.000016
Train batch 1500
Avg. loss per last 100 batches: 0.075762
Train batch 1500
Avg. loss per last 100 batches: 0.075762
Epoch: 7: Step: 1501/4907, loss=0.011146, lr=0.000016
Epoch: 7: Step: 1501/4907, loss=0.011146, lr=0.000016
Train batch 1600
Avg. loss per last 100 batches: 0.080228
Train batch 1600
Avg. loss per last 100 batches: 0.080228
Epoch: 7: Step: 1601/4907, loss=0.000459, lr=0.000016
Epoch: 7: Step: 1601/4907, loss=0.000459, lr=0.000016
Train batch 1700
Train batch 1700
Avg. loss per last 100 batches: 0.067484
Avg. loss per last 100 batches: 0.067484
Epoch: 7: Step: 1701/4907, loss=0.000870, lr=0.000016
Epoch: 7: Step: 1701/4907, loss=0.000870, lr=0.000016
Train batch 1800
Train batch 1800
Avg. loss per last 100 batches: 0.088977
Avg. loss per last 100 batches: 0.088977
Epoch: 7: Step: 1801/4907, loss=0.000340, lr=0.000016
Epoch: 7: Step: 1801/4907, loss=0.000340, lr=0.000016
Train batch 1900
Avg. loss per last 100 batches: 0.098549
Train batch 1900
Avg. loss per last 100 batches: 0.098549
Epoch: 7: Step: 1901/4907, loss=0.000484, lr=0.000016
Epoch: 7: Step: 1901/4907, loss=0.000484, lr=0.000016
Train batch 2000
Avg. loss per last 100 batches: 0.111791
Train batch 2000
Avg. loss per last 100 batches: 0.111791
Epoch: 7: Step: 2001/4907, loss=0.048905, lr=0.000016
Epoch: 7: Step: 2001/4907, loss=0.048905, lr=0.000016
Train batch 2100
Avg. loss per last 100 batches: 0.081453
Train batch 2100
Avg. loss per last 100 batches: 0.081453
Epoch: 7: Step: 2101/4907, loss=0.001518, lr=0.000016
Epoch: 7: Step: 2101/4907, loss=0.001518, lr=0.000016
Train batch 2200
Avg. loss per last 100 batches: 0.095524
Train batch 2200
Avg. loss per last 100 batches: 0.095524
Epoch: 7: Step: 2201/4907, loss=0.000670, lr=0.000016
Epoch: 7: Step: 2201/4907, loss=0.000670, lr=0.000016
Train batch 2300
Avg. loss per last 100 batches: 0.085113
Train batch 2300
Avg. loss per last 100 batches: 0.085113
Epoch: 7: Step: 2301/4907, loss=0.002080, lr=0.000016
Epoch: 7: Step: 2301/4907, loss=0.002080, lr=0.000016
Train batch 2400
Avg. loss per last 100 batches: 0.093796
Train batch 2400
Avg. loss per last 100 batches: 0.093796
Epoch: 7: Step: 2401/4907, loss=0.043784, lr=0.000016
Epoch: 7: Step: 2401/4907, loss=0.043784, lr=0.000016
Train batch 2500
Avg. loss per last 100 batches: 0.096450
Train batch 2500
Avg. loss per last 100 batches: 0.096450
Epoch: 7: Step: 2501/4907, loss=0.003208, lr=0.000016
Epoch: 7: Step: 2501/4907, loss=0.003208, lr=0.000016
Train batch 2600
Avg. loss per last 100 batches: 0.076486
Train batch 2600
Avg. loss per last 100 batches: 0.076486
Epoch: 7: Step: 2601/4907, loss=0.007242, lr=0.000016
Epoch: 7: Step: 2601/4907, loss=0.007242, lr=0.000016
Train batch 2700
Avg. loss per last 100 batches: 0.078602
Train batch 2700
Avg. loss per last 100 batches: 0.078602
Epoch: 7: Step: 2701/4907, loss=0.000387, lr=0.000016
Epoch: 7: Step: 2701/4907, loss=0.000387, lr=0.000016
Train batch 2800
Avg. loss per last 100 batches: 0.110889
Train batch 2800
Avg. loss per last 100 batches: 0.110889
Epoch: 7: Step: 2801/4907, loss=0.009572, lr=0.000016
Epoch: 7: Step: 2801/4907, loss=0.009572, lr=0.000016
Train batch 2900
Avg. loss per last 100 batches: 0.108971
Train batch 2900
Avg. loss per last 100 batches: 0.108971
Epoch: 7: Step: 2901/4907, loss=0.000282, lr=0.000016
Epoch: 7: Step: 2901/4907, loss=0.000282, lr=0.000016
Train batch 3000
Avg. loss per last 100 batches: 0.064337
Train batch 3000
Avg. loss per last 100 batches: 0.064337
Epoch: 7: Step: 3001/4907, loss=0.000014, lr=0.000016
Epoch: 7: Step: 3001/4907, loss=0.000014, lr=0.000016
Train batch 3100
Avg. loss per last 100 batches: 0.082370
Train batch 3100
Avg. loss per last 100 batches: 0.082370
Epoch: 7: Step: 3101/4907, loss=0.028245, lr=0.000016
Epoch: 7: Step: 3101/4907, loss=0.028245, lr=0.000016
Train batch 3200
Avg. loss per last 100 batches: 0.082281
Train batch 3200
Avg. loss per last 100 batches: 0.082281
Epoch: 7: Step: 3201/4907, loss=0.015354, lr=0.000016
Epoch: 7: Step: 3201/4907, loss=0.015354, lr=0.000016
Train batch 3300
Avg. loss per last 100 batches: 0.106675
Train batch 3300
Avg. loss per last 100 batches: 0.106675
Epoch: 7: Step: 3301/4907, loss=0.003004, lr=0.000016
Epoch: 7: Step: 3301/4907, loss=0.003004, lr=0.000016
Train batch 3400
Avg. loss per last 100 batches: 0.075201
Train batch 3400
Avg. loss per last 100 batches: 0.075201
Epoch: 7: Step: 3401/4907, loss=0.094503, lr=0.000016
Epoch: 7: Step: 3401/4907, loss=0.094503, lr=0.000016
Train batch 3500
Avg. loss per last 100 batches: 0.083243
Train batch 3500
Avg. loss per last 100 batches: 0.083243
Epoch: 7: Step: 3501/4907, loss=0.000035, lr=0.000016
Epoch: 7: Step: 3501/4907, loss=0.000035, lr=0.000016
Train batch 3600
Avg. loss per last 100 batches: 0.063061
Train batch 3600
Avg. loss per last 100 batches: 0.063061
Epoch: 7: Step: 3601/4907, loss=0.000185, lr=0.000016
Epoch: 7: Step: 3601/4907, loss=0.000185, lr=0.000016
Train batch 3700
Avg. loss per last 100 batches: 0.094305
Train batch 3700
Avg. loss per last 100 batches: 0.094305
Epoch: 7: Step: 3701/4907, loss=0.000308, lr=0.000016
Epoch: 7: Step: 3701/4907, loss=0.000308, lr=0.000016
Train batch 3800
Avg. loss per last 100 batches: 0.089041
Train batch 3800
Avg. loss per last 100 batches: 0.089041
Epoch: 7: Step: 3801/4907, loss=0.663883, lr=0.000016
Epoch: 7: Step: 3801/4907, loss=0.663883, lr=0.000016
Train batch 3900
Avg. loss per last 100 batches: 0.098057
Train batch 3900
Avg. loss per last 100 batches: 0.098057
Epoch: 7: Step: 3901/4907, loss=0.000014, lr=0.000016
Epoch: 7: Step: 3901/4907, loss=0.000014, lr=0.000016
Train batch 4000
Avg. loss per last 100 batches: 0.081666
Train batch 4000
Avg. loss per last 100 batches: 0.081666
Epoch: 7: Step: 4001/4907, loss=0.037900, lr=0.000016
Epoch: 7: Step: 4001/4907, loss=0.037900, lr=0.000016
Train batch 4100
Avg. loss per last 100 batches: 0.107468
Train batch 4100
Avg. loss per last 100 batches: 0.107468
Epoch: 7: Step: 4101/4907, loss=0.000017, lr=0.000016
Epoch: 7: Step: 4101/4907, loss=0.000017, lr=0.000016
Train batch 4200
Avg. loss per last 100 batches: 0.061198
Train batch 4200
Avg. loss per last 100 batches: 0.061198
Epoch: 7: Step: 4201/4907, loss=0.000002, lr=0.000016
Epoch: 7: Step: 4201/4907, loss=0.000002, lr=0.000016
Train batch 4300
Avg. loss per last 100 batches: 0.076402
Train batch 4300
Avg. loss per last 100 batches: 0.076402
Epoch: 7: Step: 4301/4907, loss=0.033236, lr=0.000016
Epoch: 7: Step: 4301/4907, loss=0.033236, lr=0.000016
Train batch 4400
Avg. loss per last 100 batches: 0.072593
Train batch 4400
Avg. loss per last 100 batches: 0.072593
Epoch: 7: Step: 4401/4907, loss=0.106432, lr=0.000016
Epoch: 7: Step: 4401/4907, loss=0.106432, lr=0.000016
Train batch 4500
Avg. loss per last 100 batches: 0.061877
Train batch 4500
Avg. loss per last 100 batches: 0.061877
Epoch: 7: Step: 4501/4907, loss=0.012983, lr=0.000016
Epoch: 7: Step: 4501/4907, loss=0.012983, lr=0.000016
Train batch 4600
Avg. loss per last 100 batches: 0.074350
Train batch 4600
Avg. loss per last 100 batches: 0.074350
Epoch: 7: Step: 4601/4907, loss=0.056296, lr=0.000016
Epoch: 7: Step: 4601/4907, loss=0.056296, lr=0.000016
Train batch 4700
Avg. loss per last 100 batches: 0.078976
Train batch 4700
Avg. loss per last 100 batches: 0.078976
Epoch: 7: Step: 4701/4907, loss=0.230718, lr=0.000016
Epoch: 7: Step: 4701/4907, loss=0.230718, lr=0.000016
Train batch 4800
Avg. loss per last 100 batches: 0.074102
Train batch 4800
Avg. loss per last 100 batches: 0.074102
Epoch: 7: Step: 4801/4907, loss=0.000803, lr=0.000016
Epoch: 7: Step: 4801/4907, loss=0.000803, lr=0.000016
Train batch 4900
Avg. loss per last 100 batches: 0.086608
Train batch 4900
Avg. loss per last 100 batches: 0.086608
Epoch: 7: Step: 4901/4907, loss=0.093994, lr=0.000016
Epoch: 7: Step: 4901/4907, loss=0.093994, lr=0.000016
Validation: Epoch: 7 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 7 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.579347 sec., loss=0.504641 
Eval step: 99 , used_time=28.546266 sec., loss=0.504641 
Eval step: 199 , used_time=57.656549 sec., loss=0.280401 
Eval step: 199 , used_time=57.623450 sec., loss=0.280401 
Eval step: 299 , used_time=86.705496 sec., loss=0.915160 
Eval step: 299 , used_time=86.672453 sec., loss=0.915160 
Eval step: 399 , used_time=115.778207 sec., loss=1.006705 
Eval step: 399 , used_time=115.745107 sec., loss=1.006705 
Eval step: 499 , used_time=144.824808 sec., loss=0.148476 
Eval step: 499 , used_time=144.791716 sec., loss=0.148476 
NLL Validation: loss = 0.555214. correct prediction ratio  5765/6516 ~  0.884745
NLL Validation: loss = 0.555214. correct prediction ratio  5765/6516 ~  0.884745
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.7.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.7.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.675543 sec., loss=0.504641 
Eval step: 99 , used_time=38.213223 sec., loss=0.504641 
Eval step: 199 , used_time=67.261878 sec., loss=0.280401 
Eval step: 199 , used_time=56.724257 sec., loss=0.280401 
Eval step: 299 , used_time=96.308991 sec., loss=0.915160 
Eval step: 299 , used_time=85.771489 sec., loss=0.915160 
Eval step: 399 , used_time=114.843019 sec., loss=1.006705 
Eval step: 399 , used_time=125.380852 sec., loss=1.006705 
Eval step: 499 , used_time=154.545474 sec., loss=0.148476 
Eval step: 499 , used_time=144.007942 sec., loss=0.148476 
NLL Validation: loss = 0.555214. correct prediction ratio  5765/6516 ~  0.884745
NLL Validation: loss = 0.555214. correct prediction ratio  5765/6516 ~  0.884745
Av Loss per epoch=0.083858
epoch total correct predictions=57437
***** Epoch 8 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.7.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.7.4907
Av Loss per epoch=0.083858
epoch total correct predictions=57437
***** Epoch 8 *****
Epoch: 8: Step: 1/4907, loss=0.070257, lr=0.000016
Epoch: 8: Step: 1/4907, loss=0.070257, lr=0.000016
Train batch 100
Avg. loss per last 100 batches: 0.079939
Train batch 100
Avg. loss per last 100 batches: 0.079939
Epoch: 8: Step: 101/4907, loss=0.000678, lr=0.000016
Epoch: 8: Step: 101/4907, loss=0.000678, lr=0.000016
Train batch 200
Avg. loss per last 100 batches: 0.091158
Train batch 200
Avg. loss per last 100 batches: 0.091158
Epoch: 8: Step: 201/4907, loss=0.000032, lr=0.000016
Epoch: 8: Step: 201/4907, loss=0.000032, lr=0.000016
Train batch 300
Avg. loss per last 100 batches: 0.091427
Train batch 300
Avg. loss per last 100 batches: 0.091427
Epoch: 8: Step: 301/4907, loss=0.180556, lr=0.000016
Epoch: 8: Step: 301/4907, loss=0.180556, lr=0.000016
Train batch 400
Avg. loss per last 100 batches: 0.051529
Train batch 400
Avg. loss per last 100 batches: 0.051529
Epoch: 8: Step: 401/4907, loss=0.000031, lr=0.000016
Epoch: 8: Step: 401/4907, loss=0.000031, lr=0.000016
Train batch 500
Avg. loss per last 100 batches: 0.045360
Train batch 500
Avg. loss per last 100 batches: 0.045360
Epoch: 8: Step: 501/4907, loss=0.000952, lr=0.000016
Epoch: 8: Step: 501/4907, loss=0.000952, lr=0.000016
Train batch 600
Avg. loss per last 100 batches: 0.082107
Train batch 600
Avg. loss per last 100 batches: 0.082107
Epoch: 8: Step: 601/4907, loss=0.005888, lr=0.000016
Epoch: 8: Step: 601/4907, loss=0.005888, lr=0.000016
Train batch 700
Avg. loss per last 100 batches: 0.092970
Train batch 700
Avg. loss per last 100 batches: 0.092970
Epoch: 8: Step: 701/4907, loss=0.014154, lr=0.000016
Epoch: 8: Step: 701/4907, loss=0.014154, lr=0.000016
Train batch 800
Avg. loss per last 100 batches: 0.063562
Train batch 800
Avg. loss per last 100 batches: 0.063562
Epoch: 8: Step: 801/4907, loss=0.172330, lr=0.000016
Epoch: 8: Step: 801/4907, loss=0.172330, lr=0.000016
Train batch 900
Avg. loss per last 100 batches: 0.095049
Train batch 900
Avg. loss per last 100 batches: 0.095049
Epoch: 8: Step: 901/4907, loss=0.082524, lr=0.000016
Epoch: 8: Step: 901/4907, loss=0.082524, lr=0.000016
Train batch 1000
Avg. loss per last 100 batches: 0.053242
Train batch 1000
Avg. loss per last 100 batches: 0.053242
Epoch: 8: Step: 1001/4907, loss=0.012699, lr=0.000016
Epoch: 8: Step: 1001/4907, loss=0.012699, lr=0.000016
Train batch 1100
Avg. loss per last 100 batches: 0.052201
Train batch 1100
Avg. loss per last 100 batches: 0.052201
Epoch: 8: Step: 1101/4907, loss=0.000523, lr=0.000016
Epoch: 8: Step: 1101/4907, loss=0.000523, lr=0.000016
Train batch 1200
Avg. loss per last 100 batches: 0.072569
Train batch 1200
Avg. loss per last 100 batches: 0.072569
Epoch: 8: Step: 1201/4907, loss=0.071539, lr=0.000016
Epoch: 8: Step: 1201/4907, loss=0.071539, lr=0.000016
Train batch 1300
Avg. loss per last 100 batches: 0.053351
Train batch 1300
Avg. loss per last 100 batches: 0.053351
Epoch: 8: Step: 1301/4907, loss=0.007190, lr=0.000016
Epoch: 8: Step: 1301/4907, loss=0.007190, lr=0.000016
Train batch 1400
Avg. loss per last 100 batches: 0.065817
Train batch 1400
Avg. loss per last 100 batches: 0.065817
Epoch: 8: Step: 1401/4907, loss=0.000029, lr=0.000016
Epoch: 8: Step: 1401/4907, loss=0.000029, lr=0.000016
Train batch 1500
Avg. loss per last 100 batches: 0.110809
Train batch 1500
Avg. loss per last 100 batches: 0.110809
Epoch: 8: Step: 1501/4907, loss=0.000100, lr=0.000016
Epoch: 8: Step: 1501/4907, loss=0.000100, lr=0.000016
Train batch 1600
Avg. loss per last 100 batches: 0.079748
Train batch 1600
Avg. loss per last 100 batches: 0.079748
Epoch: 8: Step: 1601/4907, loss=0.007258, lr=0.000016
Epoch: 8: Step: 1601/4907, loss=0.007258, lr=0.000016
Train batch 1700
Avg. loss per last 100 batches: 0.097599
Train batch 1700
Avg. loss per last 100 batches: 0.097599
Epoch: 8: Step: 1701/4907, loss=0.011171, lr=0.000016
Epoch: 8: Step: 1701/4907, loss=0.011171, lr=0.000016
Train batch 1800
Avg. loss per last 100 batches: 0.057102
Train batch 1800
Avg. loss per last 100 batches: 0.057102
Epoch: 8: Step: 1801/4907, loss=0.000010, lr=0.000016
Epoch: 8: Step: 1801/4907, loss=0.000010, lr=0.000016
Train batch 1900
Avg. loss per last 100 batches: 0.088025
Train batch 1900
Avg. loss per last 100 batches: 0.088025
Epoch: 8: Step: 1901/4907, loss=0.177887, lr=0.000016
Epoch: 8: Step: 1901/4907, loss=0.177887, lr=0.000016
Train batch 2000
Avg. loss per last 100 batches: 0.075209
Train batch 2000
Avg. loss per last 100 batches: 0.075209
Epoch: 8: Step: 2001/4907, loss=0.000195, lr=0.000016
Epoch: 8: Step: 2001/4907, loss=0.000195, lr=0.000016
Train batch 2100
Avg. loss per last 100 batches: 0.056331
Train batch 2100
Avg. loss per last 100 batches: 0.056331
Epoch: 8: Step: 2101/4907, loss=0.000522, lr=0.000016
Epoch: 8: Step: 2101/4907, loss=0.000522, lr=0.000016
Train batch 2200
Avg. loss per last 100 batches: 0.082153
Train batch 2200
Avg. loss per last 100 batches: 0.082153
Epoch: 8: Step: 2201/4907, loss=0.014669, lr=0.000016
Epoch: 8: Step: 2201/4907, loss=0.014669, lr=0.000016
Train batch 2300
Avg. loss per last 100 batches: 0.090084
Train batch 2300
Avg. loss per last 100 batches: 0.090084
Epoch: 8: Step: 2301/4907, loss=0.000086, lr=0.000016
Epoch: 8: Step: 2301/4907, loss=0.000086, lr=0.000016
Train batch 2400
Avg. loss per last 100 batches: 0.069294
Train batch 2400
Avg. loss per last 100 batches: 0.069294
Epoch: 8: Step: 2401/4907, loss=0.000360, lr=0.000016
Epoch: 8: Step: 2401/4907, loss=0.000360, lr=0.000016
Train batch 2500
Avg. loss per last 100 batches: 0.092288
Train batch 2500
Avg. loss per last 100 batches: 0.092288
Epoch: 8: Step: 2501/4907, loss=0.000070, lr=0.000016
Epoch: 8: Step: 2501/4907, loss=0.000070, lr=0.000016
Train batch 2600
Avg. loss per last 100 batches: 0.066027
Train batch 2600
Avg. loss per last 100 batches: 0.066027
Epoch: 8: Step: 2601/4907, loss=0.006723, lr=0.000016
Epoch: 8: Step: 2601/4907, loss=0.006723, lr=0.000016
Train batch 2700
Avg. loss per last 100 batches: 0.097288
Train batch 2700
Avg. loss per last 100 batches: 0.097288
Epoch: 8: Step: 2701/4907, loss=0.271489, lr=0.000016
Epoch: 8: Step: 2701/4907, loss=0.271489, lr=0.000016
Train batch 2800
Avg. loss per last 100 batches: 0.081208
Train batch 2800
Avg. loss per last 100 batches: 0.081208
Epoch: 8: Step: 2801/4907, loss=0.000062, lr=0.000016
Epoch: 8: Step: 2801/4907, loss=0.000062, lr=0.000016
Train batch 2900
Avg. loss per last 100 batches: 0.062278
Train batch 2900
Avg. loss per last 100 batches: 0.062278
Epoch: 8: Step: 2901/4907, loss=0.000288, lr=0.000016
Epoch: 8: Step: 2901/4907, loss=0.000288, lr=0.000016
Train batch 3000
Avg. loss per last 100 batches: 0.094584
Train batch 3000
Avg. loss per last 100 batches: 0.094584
Epoch: 8: Step: 3001/4907, loss=0.324464, lr=0.000016
Epoch: 8: Step: 3001/4907, loss=0.324464, lr=0.000016
Train batch 3100
Avg. loss per last 100 batches: 0.064599
Train batch 3100
Avg. loss per last 100 batches: 0.064599
Epoch: 8: Step: 3101/4907, loss=0.002729, lr=0.000016
Epoch: 8: Step: 3101/4907, loss=0.002729, lr=0.000016
Train batch 3200
Avg. loss per last 100 batches: 0.076925
Train batch 3200
Avg. loss per last 100 batches: 0.076925
Epoch: 8: Step: 3201/4907, loss=0.156578, lr=0.000016
Epoch: 8: Step: 3201/4907, loss=0.156578, lr=0.000016
Train batch 3300
Avg. loss per last 100 batches: 0.096871
Train batch 3300
Avg. loss per last 100 batches: 0.096871
Epoch: 8: Step: 3301/4907, loss=0.031046, lr=0.000016
Epoch: 8: Step: 3301/4907, loss=0.031046, lr=0.000016
Train batch 3400
Avg. loss per last 100 batches: 0.078630
Train batch 3400
Avg. loss per last 100 batches: 0.078630
Epoch: 8: Step: 3401/4907, loss=0.027878, lr=0.000016
Epoch: 8: Step: 3401/4907, loss=0.027878, lr=0.000016
Train batch 3500
Avg. loss per last 100 batches: 0.088615
Train batch 3500
Avg. loss per last 100 batches: 0.088615
Epoch: 8: Step: 3501/4907, loss=0.000286, lr=0.000016
Epoch: 8: Step: 3501/4907, loss=0.000286, lr=0.000016
Train batch 3600
Avg. loss per last 100 batches: 0.068922
Train batch 3600
Avg. loss per last 100 batches: 0.068922
Epoch: 8: Step: 3601/4907, loss=0.029098, lr=0.000016
Epoch: 8: Step: 3601/4907, loss=0.029098, lr=0.000016
Train batch 3700
Avg. loss per last 100 batches: 0.053638
Train batch 3700
Avg. loss per last 100 batches: 0.053638
Epoch: 8: Step: 3701/4907, loss=0.000996, lr=0.000016
Epoch: 8: Step: 3701/4907, loss=0.000996, lr=0.000016
Train batch 3800
Avg. loss per last 100 batches: 0.079211
Train batch 3800
Avg. loss per last 100 batches: 0.079211
Epoch: 8: Step: 3801/4907, loss=0.005629, lr=0.000016
Epoch: 8: Step: 3801/4907, loss=0.005629, lr=0.000016
Train batch 3900
Avg. loss per last 100 batches: 0.083883
Train batch 3900
Avg. loss per last 100 batches: 0.083883
Epoch: 8: Step: 3901/4907, loss=0.003567, lr=0.000016
Epoch: 8: Step: 3901/4907, loss=0.003567, lr=0.000016
Train batch 4000
Avg. loss per last 100 batches: 0.088798
Train batch 4000
Avg. loss per last 100 batches: 0.088798
Epoch: 8: Step: 4001/4907, loss=0.000329, lr=0.000016
Epoch: 8: Step: 4001/4907, loss=0.000329, lr=0.000016
Train batch 4100
Avg. loss per last 100 batches: 0.080580
Train batch 4100
Avg. loss per last 100 batches: 0.080580
Epoch: 8: Step: 4101/4907, loss=0.000002, lr=0.000016
Epoch: 8: Step: 4101/4907, loss=0.000002, lr=0.000016
Train batch 4200
Avg. loss per last 100 batches: 0.084776
Train batch 4200
Avg. loss per last 100 batches: 0.084776
Epoch: 8: Step: 4201/4907, loss=0.000351, lr=0.000016
Epoch: 8: Step: 4201/4907, loss=0.000351, lr=0.000016
Train batch 4300
Avg. loss per last 100 batches: 0.049539
Train batch 4300
Avg. loss per last 100 batches: 0.049539
Epoch: 8: Step: 4301/4907, loss=0.014014, lr=0.000016
Epoch: 8: Step: 4301/4907, loss=0.014014, lr=0.000016
Train batch 4400
Avg. loss per last 100 batches: 0.145195
Train batch 4400
Avg. loss per last 100 batches: 0.145195
Epoch: 8: Step: 4401/4907, loss=0.002478, lr=0.000016
Epoch: 8: Step: 4401/4907, loss=0.002478, lr=0.000016
Train batch 4500
Avg. loss per last 100 batches: 0.064997
Train batch 4500
Avg. loss per last 100 batches: 0.064997
Epoch: 8: Step: 4501/4907, loss=0.000702, lr=0.000016
Epoch: 8: Step: 4501/4907, loss=0.000702, lr=0.000016
Train batch 4600
Avg. loss per last 100 batches: 0.082679
Train batch 4600
Avg. loss per last 100 batches: 0.082679
Epoch: 8: Step: 4601/4907, loss=0.224568, lr=0.000016
Epoch: 8: Step: 4601/4907, loss=0.224568, lr=0.000016
Train batch 4700
Avg. loss per last 100 batches: 0.079916
Train batch 4700
Avg. loss per last 100 batches: 0.079916
Epoch: 8: Step: 4701/4907, loss=0.051505, lr=0.000016
Epoch: 8: Step: 4701/4907, loss=0.051505, lr=0.000016
Train batch 4800
Avg. loss per last 100 batches: 0.094487
Train batch 4800
Avg. loss per last 100 batches: 0.094487
Epoch: 8: Step: 4801/4907, loss=0.000089, lr=0.000016
Epoch: 8: Step: 4801/4907, loss=0.000089, lr=0.000016
Train batch 4900
Avg. loss per last 100 batches: 0.053240
Train batch 4900
Avg. loss per last 100 batches: 0.053240
Epoch: 8: Step: 4901/4907, loss=0.073132, lr=0.000016
Epoch: 8: Step: 4901/4907, loss=0.073132, lr=0.000016
Validation: Epoch: 8 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 8 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.518122 sec., loss=0.165052 
Eval step: 99 , used_time=28.560748 sec., loss=0.165052 
Eval step: 199 , used_time=57.614597 sec., loss=0.815722 
Eval step: 199 , used_time=57.571932 sec., loss=0.815722 
Eval step: 299 , used_time=86.608543 sec., loss=0.439045 
Eval step: 299 , used_time=86.651236 sec., loss=0.439045 
Eval step: 399 , used_time=115.678103 sec., loss=0.456287 
Eval step: 399 , used_time=115.635420 sec., loss=0.456287 
Eval step: 499 , used_time=144.698786 sec., loss=0.261196 
Eval step: 499 , used_time=144.656055 sec., loss=0.261196 
NLL Validation: loss = 0.536356. correct prediction ratio  5832/6516 ~  0.895028
NLL Validation: loss = 0.536356. correct prediction ratio  5832/6516 ~  0.895028
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.8.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.8.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=38.717816 sec., loss=0.165052 
Eval step: 99 , used_time=27.831742 sec., loss=0.165052 
Eval step: 199 , used_time=67.766044 sec., loss=0.815722 
Eval step: 199 , used_time=56.879966 sec., loss=0.815722 
Eval step: 299 , used_time=96.788867 sec., loss=0.439045 
Eval step: 299 , used_time=85.902771 sec., loss=0.439045 
Eval step: 399 , used_time=114.978999 sec., loss=0.456287 
Eval step: 399 , used_time=125.865098 sec., loss=0.456287 
Eval step: 499 , used_time=154.946917 sec., loss=0.261196 
Eval step: 499 , used_time=144.061025 sec., loss=0.261196 
NLL Validation: loss = 0.536356. correct prediction ratio  5832/6516 ~  0.895028
NLL Validation: loss = 0.536356. correct prediction ratio  5832/6516 ~  0.895028
Av Loss per epoch=0.077725
epoch total correct predictions=57505
***** Epoch 9 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.8.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.8.4907
Av Loss per epoch=0.077725
epoch total correct predictions=57505
***** Epoch 9 *****
Epoch: 9: Step: 1/4907, loss=0.000180, lr=0.000016
Epoch: 9: Step: 1/4907, loss=0.000180, lr=0.000016
Train batch 100
Avg. loss per last 100 batches: 0.092055
Train batch 100
Avg. loss per last 100 batches: 0.092055
Epoch: 9: Step: 101/4907, loss=0.000053, lr=0.000016
Epoch: 9: Step: 101/4907, loss=0.000053, lr=0.000016
Train batch 200
Avg. loss per last 100 batches: 0.095327
Train batch 200
Avg. loss per last 100 batches: 0.095327
Epoch: 9: Step: 201/4907, loss=0.000219, lr=0.000016
Epoch: 9: Step: 201/4907, loss=0.000219, lr=0.000016
Train batch 300
Avg. loss per last 100 batches: 0.068487
Train batch 300
Avg. loss per last 100 batches: 0.068487
Epoch: 9: Step: 301/4907, loss=0.003441, lr=0.000016
Epoch: 9: Step: 301/4907, loss=0.003441, lr=0.000016
Train batch 400
Avg. loss per last 100 batches: 0.070286
Train batch 400
Avg. loss per last 100 batches: 0.070286
Epoch: 9: Step: 401/4907, loss=0.005840, lr=0.000016
Epoch: 9: Step: 401/4907, loss=0.005840, lr=0.000016
Train batch 500
Avg. loss per last 100 batches: 0.053985
Train batch 500
Avg. loss per last 100 batches: 0.053985
Epoch: 9: Step: 501/4907, loss=0.014112, lr=0.000016
Epoch: 9: Step: 501/4907, loss=0.014112, lr=0.000016
Train batch 600
Avg. loss per last 100 batches: 0.079526
Train batch 600
Avg. loss per last 100 batches: 0.079526
Epoch: 9: Step: 601/4907, loss=0.000268, lr=0.000016
Epoch: 9: Step: 601/4907, loss=0.000268, lr=0.000016
Train batch 700
Avg. loss per last 100 batches: 0.053199
Train batch 700
Avg. loss per last 100 batches: 0.053199
Epoch: 9: Step: 701/4907, loss=0.297094, lr=0.000016
Epoch: 9: Step: 701/4907, loss=0.297094, lr=0.000016
Train batch 800
Avg. loss per last 100 batches: 0.082671
Train batch 800
Avg. loss per last 100 batches: 0.082671
Epoch: 9: Step: 801/4907, loss=0.000058, lr=0.000016
Epoch: 9: Step: 801/4907, loss=0.000058, lr=0.000016
Train batch 900
Avg. loss per last 100 batches: 0.053164
Train batch 900
Avg. loss per last 100 batches: 0.053164
Epoch: 9: Step: 901/4907, loss=0.175520, lr=0.000016
Epoch: 9: Step: 901/4907, loss=0.175520, lr=0.000016
Train batch 1000
Avg. loss per last 100 batches: 0.085332
Train batch 1000
Avg. loss per last 100 batches: 0.085332
Epoch: 9: Step: 1001/4907, loss=0.000508, lr=0.000015
Epoch: 9: Step: 1001/4907, loss=0.000508, lr=0.000015
Train batch 1100
Avg. loss per last 100 batches: 0.071163
Train batch 1100
Avg. loss per last 100 batches: 0.071163
Epoch: 9: Step: 1101/4907, loss=0.000062, lr=0.000015
Epoch: 9: Step: 1101/4907, loss=0.000062, lr=0.000015
Train batch 1200
Avg. loss per last 100 batches: 0.043143
Train batch 1200
Avg. loss per last 100 batches: 0.043143
Epoch: 9: Step: 1201/4907, loss=0.006720, lr=0.000015
Epoch: 9: Step: 1201/4907, loss=0.006720, lr=0.000015
Train batch 1300
Avg. loss per last 100 batches: 0.060981
Train batch 1300
Avg. loss per last 100 batches: 0.060981
Epoch: 9: Step: 1301/4907, loss=0.143115, lr=0.000015
Epoch: 9: Step: 1301/4907, loss=0.143115, lr=0.000015
Train batch 1400
Avg. loss per last 100 batches: 0.099989
Train batch 1400
Avg. loss per last 100 batches: 0.099989
Epoch: 9: Step: 1401/4907, loss=0.005054, lr=0.000015
Epoch: 9: Step: 1401/4907, loss=0.005054, lr=0.000015
Train batch 1500
Avg. loss per last 100 batches: 0.038679
Train batch 1500
Avg. loss per last 100 batches: 0.038679
Epoch: 9: Step: 1501/4907, loss=0.013143, lr=0.000015
Epoch: 9: Step: 1501/4907, loss=0.013143, lr=0.000015
Train batch 1600
Avg. loss per last 100 batches: 0.055712
Train batch 1600
Avg. loss per last 100 batches: 0.055712
Epoch: 9: Step: 1601/4907, loss=0.000021, lr=0.000015
Epoch: 9: Step: 1601/4907, loss=0.000021, lr=0.000015
Train batch 1700
Avg. loss per last 100 batches: 0.083167
Train batch 1700
Avg. loss per last 100 batches: 0.083167
Epoch: 9: Step: 1701/4907, loss=0.000332, lr=0.000015
Epoch: 9: Step: 1701/4907, loss=0.000332, lr=0.000015
Train batch 1800
Avg. loss per last 100 batches: 0.062022
Train batch 1800
Avg. loss per last 100 batches: 0.062022
Epoch: 9: Step: 1801/4907, loss=0.001443, lr=0.000015
Epoch: 9: Step: 1801/4907, loss=0.001443, lr=0.000015
Train batch 1900
Avg. loss per last 100 batches: 0.039011
Train batch 1900
Avg. loss per last 100 batches: 0.039011
Epoch: 9: Step: 1901/4907, loss=0.001032, lr=0.000015
Epoch: 9: Step: 1901/4907, loss=0.001032, lr=0.000015
Train batch 2000
Avg. loss per last 100 batches: 0.096983
Train batch 2000
Avg. loss per last 100 batches: 0.096983
Epoch: 9: Step: 2001/4907, loss=0.000126, lr=0.000015
Epoch: 9: Step: 2001/4907, loss=0.000126, lr=0.000015
Train batch 2100
Avg. loss per last 100 batches: 0.118146
Train batch 2100
Avg. loss per last 100 batches: 0.118146
Epoch: 9: Step: 2101/4907, loss=0.000041, lr=0.000015
Epoch: 9: Step: 2101/4907, loss=0.000041, lr=0.000015
Train batch 2200
Avg. loss per last 100 batches: 0.079496
Train batch 2200
Avg. loss per last 100 batches: 0.079496
Epoch: 9: Step: 2201/4907, loss=0.000073, lr=0.000015
Epoch: 9: Step: 2201/4907, loss=0.000073, lr=0.000015
Train batch 2300
Avg. loss per last 100 batches: 0.093427
Train batch 2300
Avg. loss per last 100 batches: 0.093427
Epoch: 9: Step: 2301/4907, loss=0.000566, lr=0.000015
Epoch: 9: Step: 2301/4907, loss=0.000566, lr=0.000015
Train batch 2400
Avg. loss per last 100 batches: 0.063792
Train batch 2400
Avg. loss per last 100 batches: 0.063792
Epoch: 9: Step: 2401/4907, loss=0.415041, lr=0.000015
Epoch: 9: Step: 2401/4907, loss=0.415041, lr=0.000015
Train batch 2500
Avg. loss per last 100 batches: 0.105930
Train batch 2500
Avg. loss per last 100 batches: 0.105930
Epoch: 9: Step: 2501/4907, loss=0.000313, lr=0.000015
Epoch: 9: Step: 2501/4907, loss=0.000313, lr=0.000015
Train batch 2600
Avg. loss per last 100 batches: 0.098088
Train batch 2600
Avg. loss per last 100 batches: 0.098088
Epoch: 9: Step: 2601/4907, loss=0.196144, lr=0.000015
Epoch: 9: Step: 2601/4907, loss=0.196144, lr=0.000015
Train batch 2700
Avg. loss per last 100 batches: 0.053122
Train batch 2700
Avg. loss per last 100 batches: 0.053122
Epoch: 9: Step: 2701/4907, loss=0.000810, lr=0.000015
Epoch: 9: Step: 2701/4907, loss=0.000810, lr=0.000015
Train batch 2800
Avg. loss per last 100 batches: 0.055527
Train batch 2800
Avg. loss per last 100 batches: 0.055527
Epoch: 9: Step: 2801/4907, loss=0.000016, lr=0.000015
Epoch: 9: Step: 2801/4907, loss=0.000016, lr=0.000015
Train batch 2900
Avg. loss per last 100 batches: 0.060963
Train batch 2900
Avg. loss per last 100 batches: 0.060963
Epoch: 9: Step: 2901/4907, loss=0.000001, lr=0.000015
Epoch: 9: Step: 2901/4907, loss=0.000001, lr=0.000015
Train batch 3000
Avg. loss per last 100 batches: 0.064744
Train batch 3000
Avg. loss per last 100 batches: 0.064744
Epoch: 9: Step: 3001/4907, loss=0.010129, lr=0.000015
Epoch: 9: Step: 3001/4907, loss=0.010129, lr=0.000015
Train batch 3100
Avg. loss per last 100 batches: 0.057724
Train batch 3100
Avg. loss per last 100 batches: 0.057724
Epoch: 9: Step: 3101/4907, loss=0.000060, lr=0.000015
Epoch: 9: Step: 3101/4907, loss=0.000060, lr=0.000015
Train batch 3200
Avg. loss per last 100 batches: 0.093083
Train batch 3200
Avg. loss per last 100 batches: 0.093083
Epoch: 9: Step: 3201/4907, loss=0.000221, lr=0.000015
Epoch: 9: Step: 3201/4907, loss=0.000221, lr=0.000015
Train batch 3300
Avg. loss per last 100 batches: 0.073371
Train batch 3300
Avg. loss per last 100 batches: 0.073371
Epoch: 9: Step: 3301/4907, loss=0.000134, lr=0.000015
Epoch: 9: Step: 3301/4907, loss=0.000134, lr=0.000015
Train batch 3400
Avg. loss per last 100 batches: 0.107307
Train batch 3400
Avg. loss per last 100 batches: 0.107307
Epoch: 9: Step: 3401/4907, loss=0.002184, lr=0.000015
Epoch: 9: Step: 3401/4907, loss=0.002184, lr=0.000015
Train batch 3500
Avg. loss per last 100 batches: 0.079961
Train batch 3500
Avg. loss per last 100 batches: 0.079961
Epoch: 9: Step: 3501/4907, loss=0.010111, lr=0.000015
Epoch: 9: Step: 3501/4907, loss=0.010111, lr=0.000015
Train batch 3600
Avg. loss per last 100 batches: 0.063268
Train batch 3600
Avg. loss per last 100 batches: 0.063268
Epoch: 9: Step: 3601/4907, loss=0.155614, lr=0.000015
Epoch: 9: Step: 3601/4907, loss=0.155614, lr=0.000015
Train batch 3700
Avg. loss per last 100 batches: 0.082149
Train batch 3700
Avg. loss per last 100 batches: 0.082149
Epoch: 9: Step: 3701/4907, loss=0.000155, lr=0.000015
Epoch: 9: Step: 3701/4907, loss=0.000155, lr=0.000015
Train batch 3800
Avg. loss per last 100 batches: 0.077359
Train batch 3800
Avg. loss per last 100 batches: 0.077359
Epoch: 9: Step: 3801/4907, loss=0.019452, lr=0.000015
Epoch: 9: Step: 3801/4907, loss=0.019452, lr=0.000015
Train batch 3900
Avg. loss per last 100 batches: 0.069099
Train batch 3900
Avg. loss per last 100 batches: 0.069099
Epoch: 9: Step: 3901/4907, loss=0.055285, lr=0.000015
Epoch: 9: Step: 3901/4907, loss=0.055285, lr=0.000015
Train batch 4000
Avg. loss per last 100 batches: 0.076267
Train batch 4000
Avg. loss per last 100 batches: 0.076267
Epoch: 9: Step: 4001/4907, loss=0.001843, lr=0.000015
Epoch: 9: Step: 4001/4907, loss=0.001843, lr=0.000015
Train batch 4100
Avg. loss per last 100 batches: 0.092053
Train batch 4100
Avg. loss per last 100 batches: 0.092053
Epoch: 9: Step: 4101/4907, loss=0.296710, lr=0.000015
Epoch: 9: Step: 4101/4907, loss=0.296710, lr=0.000015
Train batch 4200
Avg. loss per last 100 batches: 0.062896
Train batch 4200
Avg. loss per last 100 batches: 0.062896
Epoch: 9: Step: 4201/4907, loss=0.001777, lr=0.000015
Epoch: 9: Step: 4201/4907, loss=0.001777, lr=0.000015
Train batch 4300
Avg. loss per last 100 batches: 0.067501
Train batch 4300
Avg. loss per last 100 batches: 0.067501
Epoch: 9: Step: 4301/4907, loss=0.073721, lr=0.000015
Epoch: 9: Step: 4301/4907, loss=0.073721, lr=0.000015
Train batch 4400
Avg. loss per last 100 batches: 0.082231
Train batch 4400
Avg. loss per last 100 batches: 0.082231
Epoch: 9: Step: 4401/4907, loss=0.000537, lr=0.000015
Epoch: 9: Step: 4401/4907, loss=0.000537, lr=0.000015
Train batch 4500
Avg. loss per last 100 batches: 0.059495
Train batch 4500
Avg. loss per last 100 batches: 0.059495
Epoch: 9: Step: 4501/4907, loss=0.407325, lr=0.000015
Epoch: 9: Step: 4501/4907, loss=0.407325, lr=0.000015
Train batch 4600
Avg. loss per last 100 batches: 0.086401
Train batch 4600
Avg. loss per last 100 batches: 0.086401
Epoch: 9: Step: 4601/4907, loss=0.000136, lr=0.000015
Epoch: 9: Step: 4601/4907, loss=0.000136, lr=0.000015
Train batch 4700
Avg. loss per last 100 batches: 0.072732
Train batch 4700
Avg. loss per last 100 batches: 0.072732
Epoch: 9: Step: 4701/4907, loss=0.003944, lr=0.000015
Epoch: 9: Step: 4701/4907, loss=0.003944, lr=0.000015
Train batch 4800
Avg. loss per last 100 batches: 0.055341
Train batch 4800
Avg. loss per last 100 batches: 0.055341
Epoch: 9: Step: 4801/4907, loss=0.000027, lr=0.000015
Epoch: 9: Step: 4801/4907, loss=0.000027, lr=0.000015
Train batch 4900
Avg. loss per last 100 batches: 0.056554
Train batch 4900
Avg. loss per last 100 batches: 0.056554
Epoch: 9: Step: 4901/4907, loss=0.000005, lr=0.000015
Epoch: 9: Step: 4901/4907, loss=0.000005, lr=0.000015
Validation: Epoch: 9 Step: 4907/4907
NLL validation ...
Validation: Epoch: 9 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.232532 sec., loss=0.174056 
Eval step: 99 , used_time=28.263746 sec., loss=0.174056 
Eval step: 199 , used_time=57.301748 sec., loss=0.475807 
Eval step: 199 , used_time=57.332971 sec., loss=0.475807 
Eval step: 299 , used_time=86.369617 sec., loss=0.530662 
Eval step: 299 , used_time=86.400835 sec., loss=0.530662 
Eval step: 399 , used_time=115.494409 sec., loss=0.383181 
Eval step: 399 , used_time=115.525627 sec., loss=0.383181 
Eval step: 499 , used_time=144.655092 sec., loss=0.128535 
Eval step: 499 , used_time=144.623964 sec., loss=0.128535 
NLL Validation: loss = 0.552753. correct prediction ratio  5830/6516 ~  0.894721
NLL Validation: loss = 0.552753. correct prediction ratio  5830/6516 ~  0.894721
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.9.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.9.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=38.537642 sec., loss=0.174056 
Eval step: 99 , used_time=27.821020 sec., loss=0.174056 
Eval step: 199 , used_time=67.601649 sec., loss=0.475807 
Eval step: 199 , used_time=56.885020 sec., loss=0.475807 
Eval step: 299 , used_time=85.965075 sec., loss=0.530662 
Eval step: 299 , used_time=96.681709 sec., loss=0.530662 
Eval step: 399 , used_time=125.757493 sec., loss=0.383181 
Eval step: 399 , used_time=115.040880 sec., loss=0.383181 
Eval step: 499 , used_time=154.833003 sec., loss=0.128535 
Eval step: 499 , used_time=144.116390 sec., loss=0.128535 
NLL Validation: loss = 0.552753. correct prediction ratio  5830/6516 ~  0.894721
NLL Validation: loss = 0.552753. correct prediction ratio  5830/6516 ~  0.894721
Av Loss per epoch=0.073376
epoch total correct predictions=57644
***** Epoch 10 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.9.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.9.4907
Av Loss per epoch=0.073376
epoch total correct predictions=57644
***** Epoch 10 *****
Epoch: 10: Step: 1/4907, loss=0.000987, lr=0.000015
Epoch: 10: Step: 1/4907, loss=0.000987, lr=0.000015
Train batch 100
Avg. loss per last 100 batches: 0.047294
Train batch 100
Avg. loss per last 100 batches: 0.047294
Epoch: 10: Step: 101/4907, loss=0.000003, lr=0.000015
Epoch: 10: Step: 101/4907, loss=0.000003, lr=0.000015
Train batch 200
Avg. loss per last 100 batches: 0.071296
Train batch 200
Avg. loss per last 100 batches: 0.071296
Epoch: 10: Step: 201/4907, loss=0.000116, lr=0.000015
Epoch: 10: Step: 201/4907, loss=0.000116, lr=0.000015
Train batch 300
Avg. loss per last 100 batches: 0.027456
Train batch 300
Avg. loss per last 100 batches: 0.027456
Epoch: 10: Step: 301/4907, loss=0.000109, lr=0.000015
Epoch: 10: Step: 301/4907, loss=0.000109, lr=0.000015
Train batch 400
Avg. loss per last 100 batches: 0.074147
Train batch 400
Avg. loss per last 100 batches: 0.074147
Epoch: 10: Step: 401/4907, loss=0.000014, lr=0.000015
Epoch: 10: Step: 401/4907, loss=0.000014, lr=0.000015
Train batch 500
Avg. loss per last 100 batches: 0.075777
Train batch 500
Avg. loss per last 100 batches: 0.075777
Epoch: 10: Step: 501/4907, loss=0.012963, lr=0.000015
Epoch: 10: Step: 501/4907, loss=0.012963, lr=0.000015
Train batch 600
Avg. loss per last 100 batches: 0.080881
Train batch 600
Avg. loss per last 100 batches: 0.080881
Epoch: 10: Step: 601/4907, loss=0.000284, lr=0.000015
Epoch: 10: Step: 601/4907, loss=0.000284, lr=0.000015
Train batch 700
Avg. loss per last 100 batches: 0.092570
Train batch 700
Avg. loss per last 100 batches: 0.092570
Epoch: 10: Step: 701/4907, loss=0.000002, lr=0.000015
Epoch: 10: Step: 701/4907, loss=0.000002, lr=0.000015
Train batch 800
Avg. loss per last 100 batches: 0.076907
Train batch 800
Avg. loss per last 100 batches: 0.076907
Epoch: 10: Step: 801/4907, loss=0.002904, lr=0.000015
Epoch: 10: Step: 801/4907, loss=0.002904, lr=0.000015
Train batch 900
Avg. loss per last 100 batches: 0.044300
Train batch 900
Avg. loss per last 100 batches: 0.044300
Epoch: 10: Step: 901/4907, loss=0.112001, lr=0.000015
Epoch: 10: Step: 901/4907, loss=0.112001, lr=0.000015
Train batch 1000
Avg. loss per last 100 batches: 0.075018
Train batch 1000
Avg. loss per last 100 batches: 0.075018
Epoch: 10: Step: 1001/4907, loss=0.019157, lr=0.000015
Epoch: 10: Step: 1001/4907, loss=0.019157, lr=0.000015
Train batch 1100
Avg. loss per last 100 batches: 0.076574
Train batch 1100
Avg. loss per last 100 batches: 0.076574
Epoch: 10: Step: 1101/4907, loss=0.000010, lr=0.000015
Epoch: 10: Step: 1101/4907, loss=0.000010, lr=0.000015
Train batch 1200
Avg. loss per last 100 batches: 0.047168
Train batch 1200
Avg. loss per last 100 batches: 0.047168
Epoch: 10: Step: 1201/4907, loss=0.000007, lr=0.000015
Epoch: 10: Step: 1201/4907, loss=0.000007, lr=0.000015
Train batch 1300
Avg. loss per last 100 batches: 0.076987
Train batch 1300
Avg. loss per last 100 batches: 0.076987
Epoch: 10: Step: 1301/4907, loss=0.000347, lr=0.000015
Epoch: 10: Step: 1301/4907, loss=0.000347, lr=0.000015
Train batch 1400
Avg. loss per last 100 batches: 0.076074
Train batch 1400
Avg. loss per last 100 batches: 0.076074
Epoch: 10: Step: 1401/4907, loss=0.000601, lr=0.000015
Epoch: 10: Step: 1401/4907, loss=0.000601, lr=0.000015
Train batch 1500
Avg. loss per last 100 batches: 0.070065
Train batch 1500
Avg. loss per last 100 batches: 0.070065
Epoch: 10: Step: 1501/4907, loss=0.000036, lr=0.000015
Epoch: 10: Step: 1501/4907, loss=0.000036, lr=0.000015
Train batch 1600
Avg. loss per last 100 batches: 0.070114
Train batch 1600
Avg. loss per last 100 batches: 0.070114
Epoch: 10: Step: 1601/4907, loss=0.031496, lr=0.000015
Epoch: 10: Step: 1601/4907, loss=0.031496, lr=0.000015
Train batch 1700
Avg. loss per last 100 batches: 0.075231
Train batch 1700
Avg. loss per last 100 batches: 0.075231
Epoch: 10: Step: 1701/4907, loss=0.089258, lr=0.000015
Epoch: 10: Step: 1701/4907, loss=0.089258, lr=0.000015
Train batch 1800
Avg. loss per last 100 batches: 0.094266
Train batch 1800
Avg. loss per last 100 batches: 0.094266
Epoch: 10: Step: 1801/4907, loss=0.000013, lr=0.000015
Epoch: 10: Step: 1801/4907, loss=0.000013, lr=0.000015
Train batch 1900
Avg. loss per last 100 batches: 0.037046
Train batch 1900
Avg. loss per last 100 batches: 0.037046
Epoch: 10: Step: 1901/4907, loss=0.000099, lr=0.000015
Epoch: 10: Step: 1901/4907, loss=0.000099, lr=0.000015
Train batch 2000
Avg. loss per last 100 batches: 0.079096
Train batch 2000
Avg. loss per last 100 batches: 0.079096
Epoch: 10: Step: 2001/4907, loss=0.037162, lr=0.000015
Epoch: 10: Step: 2001/4907, loss=0.037162, lr=0.000015
Train batch 2100
Avg. loss per last 100 batches: 0.041409
Train batch 2100
Avg. loss per last 100 batches: 0.041409
Epoch: 10: Step: 2101/4907, loss=0.143008, lr=0.000015
Epoch: 10: Step: 2101/4907, loss=0.143008, lr=0.000015
Train batch 2200
Avg. loss per last 100 batches: 0.066911
Train batch 2200
Avg. loss per last 100 batches: 0.066911
Epoch: 10: Step: 2201/4907, loss=0.141689, lr=0.000015
Epoch: 10: Step: 2201/4907, loss=0.141689, lr=0.000015
Train batch 2300
Avg. loss per last 100 batches: 0.088802
Train batch 2300
Avg. loss per last 100 batches: 0.088802
Epoch: 10: Step: 2301/4907, loss=0.343575, lr=0.000015
Epoch: 10: Step: 2301/4907, loss=0.343575, lr=0.000015
Train batch 2400
Avg. loss per last 100 batches: 0.056024
Train batch 2400
Avg. loss per last 100 batches: 0.056024
Epoch: 10: Step: 2401/4907, loss=0.000904, lr=0.000015
Epoch: 10: Step: 2401/4907, loss=0.000904, lr=0.000015
Train batch 2500
Avg. loss per last 100 batches: 0.082424
Train batch 2500
Avg. loss per last 100 batches: 0.082424
Epoch: 10: Step: 2501/4907, loss=0.005260, lr=0.000015
Epoch: 10: Step: 2501/4907, loss=0.005260, lr=0.000015
Train batch 2600
Avg. loss per last 100 batches: 0.087347
Train batch 2600
Avg. loss per last 100 batches: 0.087347
Epoch: 10: Step: 2601/4907, loss=0.005381, lr=0.000015
Epoch: 10: Step: 2601/4907, loss=0.005381, lr=0.000015
Train batch 2700
Avg. loss per last 100 batches: 0.052543
Train batch 2700
Avg. loss per last 100 batches: 0.052543
Epoch: 10: Step: 2701/4907, loss=0.000306, lr=0.000015
Epoch: 10: Step: 2701/4907, loss=0.000306, lr=0.000015
Train batch 2800
Avg. loss per last 100 batches: 0.062820
Train batch 2800
Avg. loss per last 100 batches: 0.062820
Epoch: 10: Step: 2801/4907, loss=0.254281, lr=0.000015
Epoch: 10: Step: 2801/4907, loss=0.254281, lr=0.000015
Train batch 2900
Avg. loss per last 100 batches: 0.054382
Train batch 2900
Avg. loss per last 100 batches: 0.054382
Epoch: 10: Step: 2901/4907, loss=0.190167, lr=0.000015
Epoch: 10: Step: 2901/4907, loss=0.190167, lr=0.000015
Train batch 3000
Avg. loss per last 100 batches: 0.060719
Train batch 3000
Avg. loss per last 100 batches: 0.060719
Epoch: 10: Step: 3001/4907, loss=0.198898, lr=0.000015
Epoch: 10: Step: 3001/4907, loss=0.198898, lr=0.000015
Train batch 3100
Avg. loss per last 100 batches: 0.045563
Train batch 3100
Avg. loss per last 100 batches: 0.045563
Epoch: 10: Step: 3101/4907, loss=0.000863, lr=0.000015
Epoch: 10: Step: 3101/4907, loss=0.000863, lr=0.000015
Train batch 3200
Avg. loss per last 100 batches: 0.054740
Train batch 3200
Avg. loss per last 100 batches: 0.054740
Epoch: 10: Step: 3201/4907, loss=0.002378, lr=0.000015
Epoch: 10: Step: 3201/4907, loss=0.002378, lr=0.000015
Train batch 3300
Avg. loss per last 100 batches: 0.077398
Train batch 3300
Avg. loss per last 100 batches: 0.077398
Epoch: 10: Step: 3301/4907, loss=0.610407, lr=0.000015
Epoch: 10: Step: 3301/4907, loss=0.610407, lr=0.000015
Train batch 3400
Avg. loss per last 100 batches: 0.079666
Train batch 3400
Avg. loss per last 100 batches: 0.079666
Epoch: 10: Step: 3401/4907, loss=0.053906, lr=0.000015
Epoch: 10: Step: 3401/4907, loss=0.053906, lr=0.000015
Train batch 3500
Avg. loss per last 100 batches: 0.091298
Train batch 3500
Avg. loss per last 100 batches: 0.091298
Epoch: 10: Step: 3501/4907, loss=0.004024, lr=0.000015
Epoch: 10: Step: 3501/4907, loss=0.004024, lr=0.000015
Train batch 3600
Avg. loss per last 100 batches: 0.069476
Train batch 3600
Avg. loss per last 100 batches: 0.069476
Epoch: 10: Step: 3601/4907, loss=0.000145, lr=0.000015
Epoch: 10: Step: 3601/4907, loss=0.000145, lr=0.000015
Train batch 3700
Avg. loss per last 100 batches: 0.078743
Train batch 3700
Avg. loss per last 100 batches: 0.078743
Epoch: 10: Step: 3701/4907, loss=0.003354, lr=0.000015
Epoch: 10: Step: 3701/4907, loss=0.003354, lr=0.000015
Train batch 3800
Avg. loss per last 100 batches: 0.072705
Train batch 3800
Avg. loss per last 100 batches: 0.072705
Epoch: 10: Step: 3801/4907, loss=0.335031, lr=0.000015
Epoch: 10: Step: 3801/4907, loss=0.335031, lr=0.000015
Train batch 3900
Avg. loss per last 100 batches: 0.067039
Train batch 3900
Avg. loss per last 100 batches: 0.067039
Epoch: 10: Step: 3901/4907, loss=0.007218, lr=0.000015
Epoch: 10: Step: 3901/4907, loss=0.007218, lr=0.000015
Train batch 4000
Avg. loss per last 100 batches: 0.080921
Train batch 4000
Avg. loss per last 100 batches: 0.080921
Epoch: 10: Step: 4001/4907, loss=0.000576, lr=0.000015
Epoch: 10: Step: 4001/4907, loss=0.000576, lr=0.000015
Train batch 4100
Avg. loss per last 100 batches: 0.078751
Train batch 4100
Avg. loss per last 100 batches: 0.078751
Epoch: 10: Step: 4101/4907, loss=0.616859, lr=0.000015
Epoch: 10: Step: 4101/4907, loss=0.616859, lr=0.000015
Train batch 4200
Avg. loss per last 100 batches: 0.086800
Train batch 4200
Avg. loss per last 100 batches: 0.086800
Epoch: 10: Step: 4201/4907, loss=0.069466, lr=0.000015
Epoch: 10: Step: 4201/4907, loss=0.069466, lr=0.000015
Train batch 4300
Avg. loss per last 100 batches: 0.076331
Train batch 4300
Avg. loss per last 100 batches: 0.076331
Epoch: 10: Step: 4301/4907, loss=0.000287, lr=0.000015
Epoch: 10: Step: 4301/4907, loss=0.000287, lr=0.000015
Train batch 4400
Avg. loss per last 100 batches: 0.046416
Train batch 4400
Avg. loss per last 100 batches: 0.046416
Epoch: 10: Step: 4401/4907, loss=0.000002, lr=0.000015
Epoch: 10: Step: 4401/4907, loss=0.000002, lr=0.000015
Train batch 4500
Avg. loss per last 100 batches: 0.088773
Train batch 4500
Avg. loss per last 100 batches: 0.088773
Epoch: 10: Step: 4501/4907, loss=0.000464, lr=0.000015
Epoch: 10: Step: 4501/4907, loss=0.000464, lr=0.000015
Train batch 4600
Avg. loss per last 100 batches: 0.051550
Train batch 4600
Avg. loss per last 100 batches: 0.051550
Epoch: 10: Step: 4601/4907, loss=0.000862, lr=0.000015
Epoch: 10: Step: 4601/4907, loss=0.000862, lr=0.000015
Train batch 4700
Avg. loss per last 100 batches: 0.059316
Train batch 4700
Avg. loss per last 100 batches: 0.059316
Epoch: 10: Step: 4701/4907, loss=0.264175, lr=0.000015
Epoch: 10: Step: 4701/4907, loss=0.264175, lr=0.000015
Train batch 4800
Avg. loss per last 100 batches: 0.077372
Train batch 4800
Avg. loss per last 100 batches: 0.077372
Epoch: 10: Step: 4801/4907, loss=0.001017, lr=0.000015
Epoch: 10: Step: 4801/4907, loss=0.001017, lr=0.000015
Train batch 4900
Avg. loss per last 100 batches: 0.057812
Train batch 4900
Avg. loss per last 100 batches: 0.057812
Epoch: 10: Step: 4901/4907, loss=0.000707, lr=0.000015
Epoch: 10: Step: 4901/4907, loss=0.000707, lr=0.000015
Validation: Epoch: 10 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 10 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
../baselines/DPR/train_dense_encoder.py:282: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)
  gold_idx = (indices[i] == idx).nonzero()
../baselines/DPR/train_dense_encoder.py:282: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)
  gold_idx = (indices[i] == idx).nonzero()
Av.rank validation: average rank 193.34745242480048, total questions=6516
Av.rank validation: average rank 193.34745242480048, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.10.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.10.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.10.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 193.34745242480048, total questions=6516
Av.rank validation: average rank 193.34745242480048, total questions=6516
Av Loss per epoch=0.068525
epoch total correct predictions=57729
***** Epoch 11 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.10.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.10.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.10.4907
Av Loss per epoch=0.068525
epoch total correct predictions=57729
***** Epoch 11 *****
Epoch: 11: Step: 1/4907, loss=0.000007, lr=0.000015
Epoch: 11: Step: 1/4907, loss=0.000007, lr=0.000015
Train batch 100
Avg. loss per last 100 batches: 0.044466
Train batch 100
Avg. loss per last 100 batches: 0.044466
Epoch: 11: Step: 101/4907, loss=0.503454, lr=0.000015
Epoch: 11: Step: 101/4907, loss=0.503454, lr=0.000015
Train batch 200
Avg. loss per last 100 batches: 0.046913
Train batch 200
Avg. loss per last 100 batches: 0.046913
Epoch: 11: Step: 201/4907, loss=0.845312, lr=0.000015
Epoch: 11: Step: 201/4907, loss=0.845312, lr=0.000015
Train batch 300
Avg. loss per last 100 batches: 0.069361
Train batch 300
Avg. loss per last 100 batches: 0.069361
Epoch: 11: Step: 301/4907, loss=0.000000, lr=0.000015
Epoch: 11: Step: 301/4907, loss=0.000000, lr=0.000015
Train batch 400
Avg. loss per last 100 batches: 0.051020
Train batch 400
Avg. loss per last 100 batches: 0.051020
Epoch: 11: Step: 401/4907, loss=0.000030, lr=0.000015
Epoch: 11: Step: 401/4907, loss=0.000030, lr=0.000015
Train batch 500
Avg. loss per last 100 batches: 0.042606
Train batch 500
Avg. loss per last 100 batches: 0.042606
Epoch: 11: Step: 501/4907, loss=0.358192, lr=0.000015
Epoch: 11: Step: 501/4907, loss=0.358192, lr=0.000015
Train batch 600
Avg. loss per last 100 batches: 0.076132
Train batch 600
Avg. loss per last 100 batches: 0.076132
Epoch: 11: Step: 601/4907, loss=0.000001, lr=0.000015
Epoch: 11: Step: 601/4907, loss=0.000001, lr=0.000015
Train batch 700
Avg. loss per last 100 batches: 0.071068
Train batch 700
Avg. loss per last 100 batches: 0.071068
Epoch: 11: Step: 701/4907, loss=0.000321, lr=0.000015
Epoch: 11: Step: 701/4907, loss=0.000321, lr=0.000015
Train batch 800
Avg. loss per last 100 batches: 0.043192
Train batch 800
Avg. loss per last 100 batches: 0.043192
Epoch: 11: Step: 801/4907, loss=0.277084, lr=0.000015
Epoch: 11: Step: 801/4907, loss=0.277084, lr=0.000015
Train batch 900
Avg. loss per last 100 batches: 0.040193
Train batch 900
Avg. loss per last 100 batches: 0.040193
Epoch: 11: Step: 901/4907, loss=0.001065, lr=0.000014
Epoch: 11: Step: 901/4907, loss=0.001065, lr=0.000014
Train batch 1000
Avg. loss per last 100 batches: 0.052884
Train batch 1000
Avg. loss per last 100 batches: 0.052884
Epoch: 11: Step: 1001/4907, loss=0.000041, lr=0.000014
Epoch: 11: Step: 1001/4907, loss=0.000041, lr=0.000014
Train batch 1100
Avg. loss per last 100 batches: 0.043680
Train batch 1100
Avg. loss per last 100 batches: 0.043680
Epoch: 11: Step: 1101/4907, loss=0.000494, lr=0.000014
Epoch: 11: Step: 1101/4907, loss=0.000494, lr=0.000014
Train batch 1200
Avg. loss per last 100 batches: 0.078244
Train batch 1200
Avg. loss per last 100 batches: 0.078244
Epoch: 11: Step: 1201/4907, loss=0.000594, lr=0.000014
Epoch: 11: Step: 1201/4907, loss=0.000594, lr=0.000014
Train batch 1300
Avg. loss per last 100 batches: 0.062604
Train batch 1300
Avg. loss per last 100 batches: 0.062604
Epoch: 11: Step: 1301/4907, loss=0.148166, lr=0.000014
Epoch: 11: Step: 1301/4907, loss=0.148166, lr=0.000014
Train batch 1400
Avg. loss per last 100 batches: 0.052381
Train batch 1400
Avg. loss per last 100 batches: 0.052381
Epoch: 11: Step: 1401/4907, loss=0.017911, lr=0.000014
Epoch: 11: Step: 1401/4907, loss=0.017911, lr=0.000014
Train batch 1500
Avg. loss per last 100 batches: 0.076602
Train batch 1500
Avg. loss per last 100 batches: 0.076602
Epoch: 11: Step: 1501/4907, loss=0.000000, lr=0.000014
Epoch: 11: Step: 1501/4907, loss=0.000000, lr=0.000014
Train batch 1600
Avg. loss per last 100 batches: 0.045208
Train batch 1600
Avg. loss per last 100 batches: 0.045208
Epoch: 11: Step: 1601/4907, loss=0.002329, lr=0.000014
Epoch: 11: Step: 1601/4907, loss=0.002329, lr=0.000014
Train batch 1700
Avg. loss per last 100 batches: 0.053554
Train batch 1700
Avg. loss per last 100 batches: 0.053554
Epoch: 11: Step: 1701/4907, loss=0.000002, lr=0.000014
Epoch: 11: Step: 1701/4907, loss=0.000002, lr=0.000014
Train batch 1800
Avg. loss per last 100 batches: 0.076491
Train batch 1800
Avg. loss per last 100 batches: 0.076491
Epoch: 11: Step: 1801/4907, loss=0.002316, lr=0.000014
Epoch: 11: Step: 1801/4907, loss=0.002316, lr=0.000014
Train batch 1900
Avg. loss per last 100 batches: 0.073875
Train batch 1900
Avg. loss per last 100 batches: 0.073875
Epoch: 11: Step: 1901/4907, loss=0.000960, lr=0.000014
Epoch: 11: Step: 1901/4907, loss=0.000960, lr=0.000014
Train batch 2000
Avg. loss per last 100 batches: 0.073785
Train batch 2000
Avg. loss per last 100 batches: 0.073785
Epoch: 11: Step: 2001/4907, loss=0.004562, lr=0.000014
Epoch: 11: Step: 2001/4907, loss=0.004562, lr=0.000014
Train batch 2100
Avg. loss per last 100 batches: 0.063725
Train batch 2100
Avg. loss per last 100 batches: 0.063725
Epoch: 11: Step: 2101/4907, loss=0.002623, lr=0.000014
Epoch: 11: Step: 2101/4907, loss=0.002623, lr=0.000014
Train batch 2200
Avg. loss per last 100 batches: 0.066446
Train batch 2200
Avg. loss per last 100 batches: 0.066446
Epoch: 11: Step: 2201/4907, loss=0.000036, lr=0.000014
Epoch: 11: Step: 2201/4907, loss=0.000036, lr=0.000014
Train batch 2300
Avg. loss per last 100 batches: 0.074770
Train batch 2300
Avg. loss per last 100 batches: 0.074770
Epoch: 11: Step: 2301/4907, loss=0.000034, lr=0.000014
Epoch: 11: Step: 2301/4907, loss=0.000034, lr=0.000014
Train batch 2400
Avg. loss per last 100 batches: 0.045657
Train batch 2400
Avg. loss per last 100 batches: 0.045657
Epoch: 11: Step: 2401/4907, loss=0.000861, lr=0.000014
Epoch: 11: Step: 2401/4907, loss=0.000861, lr=0.000014
Train batch 2500
Avg. loss per last 100 batches: 0.037811
Train batch 2500
Avg. loss per last 100 batches: 0.037811
Epoch: 11: Step: 2501/4907, loss=0.031501, lr=0.000014
Epoch: 11: Step: 2501/4907, loss=0.031501, lr=0.000014
Train batch 2600
Avg. loss per last 100 batches: 0.064771
Train batch 2600
Avg. loss per last 100 batches: 0.064771
Epoch: 11: Step: 2601/4907, loss=0.071353, lr=0.000014
Epoch: 11: Step: 2601/4907, loss=0.071353, lr=0.000014
Train batch 2700
Avg. loss per last 100 batches: 0.058731
Train batch 2700
Avg. loss per last 100 batches: 0.058731
Epoch: 11: Step: 2701/4907, loss=0.033143, lr=0.000014
Epoch: 11: Step: 2701/4907, loss=0.033143, lr=0.000014
Train batch 2800
Avg. loss per last 100 batches: 0.060452
Train batch 2800
Avg. loss per last 100 batches: 0.060452
Epoch: 11: Step: 2801/4907, loss=0.019707, lr=0.000014
Epoch: 11: Step: 2801/4907, loss=0.019707, lr=0.000014
Train batch 2900
Avg. loss per last 100 batches: 0.062219
Train batch 2900
Avg. loss per last 100 batches: 0.062219
Epoch: 11: Step: 2901/4907, loss=0.005429, lr=0.000014
Epoch: 11: Step: 2901/4907, loss=0.005429, lr=0.000014
Train batch 3000
Avg. loss per last 100 batches: 0.058062
Train batch 3000
Avg. loss per last 100 batches: 0.058062
Epoch: 11: Step: 3001/4907, loss=0.000016, lr=0.000014
Epoch: 11: Step: 3001/4907, loss=0.000016, lr=0.000014
Train batch 3100
Avg. loss per last 100 batches: 0.060968
Train batch 3100
Avg. loss per last 100 batches: 0.060968
Epoch: 11: Step: 3101/4907, loss=0.038155, lr=0.000014
Epoch: 11: Step: 3101/4907, loss=0.038155, lr=0.000014
Train batch 3200
Avg. loss per last 100 batches: 0.048979
Train batch 3200
Avg. loss per last 100 batches: 0.048979
Epoch: 11: Step: 3201/4907, loss=0.055738, lr=0.000014
Epoch: 11: Step: 3201/4907, loss=0.055738, lr=0.000014
Train batch 3300
Avg. loss per last 100 batches: 0.078374
Train batch 3300
Avg. loss per last 100 batches: 0.078374
Epoch: 11: Step: 3301/4907, loss=0.000077, lr=0.000014
Epoch: 11: Step: 3301/4907, loss=0.000077, lr=0.000014
Train batch 3400
Avg. loss per last 100 batches: 0.053694
Train batch 3400
Avg. loss per last 100 batches: 0.053694
Epoch: 11: Step: 3401/4907, loss=0.004131, lr=0.000014
Epoch: 11: Step: 3401/4907, loss=0.004131, lr=0.000014
Train batch 3500
Avg. loss per last 100 batches: 0.065531
Train batch 3500
Avg. loss per last 100 batches: 0.065531
Epoch: 11: Step: 3501/4907, loss=0.000168, lr=0.000014
Epoch: 11: Step: 3501/4907, loss=0.000168, lr=0.000014
Train batch 3600
Avg. loss per last 100 batches: 0.102978
Train batch 3600
Avg. loss per last 100 batches: 0.102978
Epoch: 11: Step: 3601/4907, loss=0.000513, lr=0.000014
Epoch: 11: Step: 3601/4907, loss=0.000513, lr=0.000014
Train batch 3700
Avg. loss per last 100 batches: 0.079212
Train batch 3700
Avg. loss per last 100 batches: 0.079212
Epoch: 11: Step: 3701/4907, loss=0.111071, lr=0.000014
Epoch: 11: Step: 3701/4907, loss=0.111071, lr=0.000014
Train batch 3800
Avg. loss per last 100 batches: 0.064209
Train batch 3800
Avg. loss per last 100 batches: 0.064209
Epoch: 11: Step: 3801/4907, loss=0.000385, lr=0.000014
Epoch: 11: Step: 3801/4907, loss=0.000385, lr=0.000014
Train batch 3900
Avg. loss per last 100 batches: 0.056603
Train batch 3900
Avg. loss per last 100 batches: 0.056603
Epoch: 11: Step: 3901/4907, loss=0.002479, lr=0.000014
Epoch: 11: Step: 3901/4907, loss=0.002479, lr=0.000014
Train batch 4000
Avg. loss per last 100 batches: 0.044380
Train batch 4000
Avg. loss per last 100 batches: 0.044380
Epoch: 11: Step: 4001/4907, loss=0.000003, lr=0.000014
Epoch: 11: Step: 4001/4907, loss=0.000003, lr=0.000014
Train batch 4100
Avg. loss per last 100 batches: 0.049807
Train batch 4100
Avg. loss per last 100 batches: 0.049807
Epoch: 11: Step: 4101/4907, loss=0.000089, lr=0.000014
Epoch: 11: Step: 4101/4907, loss=0.000089, lr=0.000014
Train batch 4200
Avg. loss per last 100 batches: 0.032936
Train batch 4200
Avg. loss per last 100 batches: 0.032936
Epoch: 11: Step: 4201/4907, loss=0.141587, lr=0.000014
Epoch: 11: Step: 4201/4907, loss=0.141587, lr=0.000014
Train batch 4300
Avg. loss per last 100 batches: 0.085931
Train batch 4300
Avg. loss per last 100 batches: 0.085931
Epoch: 11: Step: 4301/4907, loss=0.116308, lr=0.000014
Epoch: 11: Step: 4301/4907, loss=0.116308, lr=0.000014
Train batch 4400
Avg. loss per last 100 batches: 0.061514
Train batch 4400
Avg. loss per last 100 batches: 0.061514
Epoch: 11: Step: 4401/4907, loss=0.013392, lr=0.000014
Epoch: 11: Step: 4401/4907, loss=0.013392, lr=0.000014
Train batch 4500
Avg. loss per last 100 batches: 0.068784
Train batch 4500
Avg. loss per last 100 batches: 0.068784
Epoch: 11: Step: 4501/4907, loss=0.000007, lr=0.000014
Epoch: 11: Step: 4501/4907, loss=0.000007, lr=0.000014
Train batch 4600
Avg. loss per last 100 batches: 0.065190
Train batch 4600
Avg. loss per last 100 batches: 0.065190
Epoch: 11: Step: 4601/4907, loss=0.334085, lr=0.000014
Epoch: 11: Step: 4601/4907, loss=0.334085, lr=0.000014
Train batch 4700
Avg. loss per last 100 batches: 0.066530
Train batch 4700
Avg. loss per last 100 batches: 0.066530
Epoch: 11: Step: 4701/4907, loss=0.000012, lr=0.000014
Epoch: 11: Step: 4701/4907, loss=0.000012, lr=0.000014
Train batch 4800
Avg. loss per last 100 batches: 0.061669
Train batch 4800
Avg. loss per last 100 batches: 0.061669
Epoch: 11: Step: 4801/4907, loss=0.000024, lr=0.000014
Epoch: 11: Step: 4801/4907, loss=0.000024, lr=0.000014
Train batch 4900
Avg. loss per last 100 batches: 0.057194
Train batch 4900
Avg. loss per last 100 batches: 0.057194
Epoch: 11: Step: 4901/4907, loss=0.009478, lr=0.000014
Epoch: 11: Step: 4901/4907, loss=0.009478, lr=0.000014
Validation: Epoch: 11 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 11 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 199.71132596685084, total questions=6516
Av.rank validation: average rank 199.71132596685084, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.11.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.11.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 199.71132596685084, total questions=6516
Av.rank validation: average rank 199.71132596685084, total questions=6516
Av Loss per epoch=0.060582
epoch total correct predictions=57850
***** Epoch 12 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.11.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.11.4907
Av Loss per epoch=0.060582
epoch total correct predictions=57850
***** Epoch 12 *****
Epoch: 12: Step: 1/4907, loss=0.000859, lr=0.000014
Epoch: 12: Step: 1/4907, loss=0.000859, lr=0.000014
Train batch 100
Avg. loss per last 100 batches: 0.030073
Train batch 100
Avg. loss per last 100 batches: 0.030073
Epoch: 12: Step: 101/4907, loss=0.000005, lr=0.000014
Epoch: 12: Step: 101/4907, loss=0.000005, lr=0.000014
Train batch 200
Avg. loss per last 100 batches: 0.069324
Train batch 200
Avg. loss per last 100 batches: 0.069324
Epoch: 12: Step: 201/4907, loss=0.000000, lr=0.000014
Epoch: 12: Step: 201/4907, loss=0.000000, lr=0.000014
Train batch 300
Avg. loss per last 100 batches: 0.060460
Train batch 300
Avg. loss per last 100 batches: 0.060460
Epoch: 12: Step: 301/4907, loss=0.002744, lr=0.000014
Epoch: 12: Step: 301/4907, loss=0.002744, lr=0.000014
Train batch 400
Avg. loss per last 100 batches: 0.057361
Train batch 400
Avg. loss per last 100 batches: 0.057361
Epoch: 12: Step: 401/4907, loss=0.080886, lr=0.000014
Epoch: 12: Step: 401/4907, loss=0.080886, lr=0.000014
Train batch 500
Avg. loss per last 100 batches: 0.052697
Train batch 500
Avg. loss per last 100 batches: 0.052697
Epoch: 12: Step: 501/4907, loss=0.265885, lr=0.000014
Epoch: 12: Step: 501/4907, loss=0.265885, lr=0.000014
Train batch 600
Avg. loss per last 100 batches: 0.077685
Train batch 600
Avg. loss per last 100 batches: 0.077685
Epoch: 12: Step: 601/4907, loss=0.004843, lr=0.000014
Epoch: 12: Step: 601/4907, loss=0.004843, lr=0.000014
Train batch 700
Avg. loss per last 100 batches: 0.032966
Train batch 700
Avg. loss per last 100 batches: 0.032966
Epoch: 12: Step: 701/4907, loss=0.000098, lr=0.000014
Epoch: 12: Step: 701/4907, loss=0.000098, lr=0.000014
Train batch 800
Avg. loss per last 100 batches: 0.051106
Train batch 800
Avg. loss per last 100 batches: 0.051106
Epoch: 12: Step: 801/4907, loss=0.000708, lr=0.000014
Epoch: 12: Step: 801/4907, loss=0.000708, lr=0.000014
Train batch 900
Avg. loss per last 100 batches: 0.062672
Train batch 900
Avg. loss per last 100 batches: 0.062672
Epoch: 12: Step: 901/4907, loss=0.000267, lr=0.000014
Epoch: 12: Step: 901/4907, loss=0.000267, lr=0.000014
Train batch 1000
Avg. loss per last 100 batches: 0.050067
Train batch 1000
Avg. loss per last 100 batches: 0.050067
Epoch: 12: Step: 1001/4907, loss=0.000000, lr=0.000014
Epoch: 12: Step: 1001/4907, loss=0.000000, lr=0.000014
Train batch 1100
Avg. loss per last 100 batches: 0.049415
Train batch 1100
Avg. loss per last 100 batches: 0.049415
Epoch: 12: Step: 1101/4907, loss=0.019301, lr=0.000014
Epoch: 12: Step: 1101/4907, loss=0.019301, lr=0.000014
Train batch 1200
Avg. loss per last 100 batches: 0.045503
Train batch 1200
Avg. loss per last 100 batches: 0.045503
Epoch: 12: Step: 1201/4907, loss=0.036965, lr=0.000014
Epoch: 12: Step: 1201/4907, loss=0.036965, lr=0.000014
Train batch 1300
Avg. loss per last 100 batches: 0.071991
Train batch 1300
Avg. loss per last 100 batches: 0.071991
Epoch: 12: Step: 1301/4907, loss=0.000083, lr=0.000014
Epoch: 12: Step: 1301/4907, loss=0.000083, lr=0.000014
Train batch 1400
Avg. loss per last 100 batches: 0.073930
Train batch 1400
Avg. loss per last 100 batches: 0.073930
Epoch: 12: Step: 1401/4907, loss=0.521888, lr=0.000014
Epoch: 12: Step: 1401/4907, loss=0.521888, lr=0.000014
Train batch 1500
Avg. loss per last 100 batches: 0.069398
Train batch 1500
Avg. loss per last 100 batches: 0.069398
Epoch: 12: Step: 1501/4907, loss=0.000410, lr=0.000014
Epoch: 12: Step: 1501/4907, loss=0.000410, lr=0.000014
Train batch 1600
Avg. loss per last 100 batches: 0.065550
Train batch 1600
Avg. loss per last 100 batches: 0.065550
Epoch: 12: Step: 1601/4907, loss=0.000002, lr=0.000014
Epoch: 12: Step: 1601/4907, loss=0.000002, lr=0.000014
Train batch 1700
Avg. loss per last 100 batches: 0.047882
Train batch 1700
Avg. loss per last 100 batches: 0.047882
Epoch: 12: Step: 1701/4907, loss=0.000025, lr=0.000014
Epoch: 12: Step: 1701/4907, loss=0.000025, lr=0.000014
Train batch 1800
Avg. loss per last 100 batches: 0.089684
Train batch 1800
Avg. loss per last 100 batches: 0.089684
Epoch: 12: Step: 1801/4907, loss=0.000002, lr=0.000014
Epoch: 12: Step: 1801/4907, loss=0.000002, lr=0.000014
Train batch 1900
Avg. loss per last 100 batches: 0.042367
Train batch 1900
Avg. loss per last 100 batches: 0.042367
Epoch: 12: Step: 1901/4907, loss=0.000141, lr=0.000014
Epoch: 12: Step: 1901/4907, loss=0.000141, lr=0.000014
Train batch 2000
Avg. loss per last 100 batches: 0.071275
Train batch 2000
Avg. loss per last 100 batches: 0.071275
Epoch: 12: Step: 2001/4907, loss=0.000073, lr=0.000014
Epoch: 12: Step: 2001/4907, loss=0.000073, lr=0.000014
Train batch 2100
Avg. loss per last 100 batches: 0.046989
Train batch 2100
Avg. loss per last 100 batches: 0.046989
Epoch: 12: Step: 2101/4907, loss=0.000023, lr=0.000014
Epoch: 12: Step: 2101/4907, loss=0.000023, lr=0.000014
Train batch 2200
Avg. loss per last 100 batches: 0.024631
Train batch 2200
Avg. loss per last 100 batches: 0.024631
Epoch: 12: Step: 2201/4907, loss=0.000021, lr=0.000014
Epoch: 12: Step: 2201/4907, loss=0.000021, lr=0.000014
Train batch 2300
Avg. loss per last 100 batches: 0.077975
Train batch 2300
Avg. loss per last 100 batches: 0.077975
Epoch: 12: Step: 2301/4907, loss=0.000000, lr=0.000014
Epoch: 12: Step: 2301/4907, loss=0.000000, lr=0.000014
Train batch 2400
Avg. loss per last 100 batches: 0.056872
Train batch 2400
Avg. loss per last 100 batches: 0.056872
Epoch: 12: Step: 2401/4907, loss=0.000939, lr=0.000014
Epoch: 12: Step: 2401/4907, loss=0.000939, lr=0.000014
Train batch 2500
Avg. loss per last 100 batches: 0.035800
Train batch 2500
Avg. loss per last 100 batches: 0.035800
Epoch: 12: Step: 2501/4907, loss=0.000116, lr=0.000014
Epoch: 12: Step: 2501/4907, loss=0.000116, lr=0.000014
Train batch 2600
Avg. loss per last 100 batches: 0.041026
Train batch 2600
Avg. loss per last 100 batches: 0.041026
Epoch: 12: Step: 2601/4907, loss=0.000003, lr=0.000014
Epoch: 12: Step: 2601/4907, loss=0.000003, lr=0.000014
Train batch 2700
Avg. loss per last 100 batches: 0.059638
Train batch 2700
Avg. loss per last 100 batches: 0.059638
Epoch: 12: Step: 2701/4907, loss=0.002911, lr=0.000014
Epoch: 12: Step: 2701/4907, loss=0.002911, lr=0.000014
Train batch 2800
Avg. loss per last 100 batches: 0.061307
Train batch 2800
Avg. loss per last 100 batches: 0.061307
Epoch: 12: Step: 2801/4907, loss=0.087639, lr=0.000014
Epoch: 12: Step: 2801/4907, loss=0.087639, lr=0.000014
Train batch 2900
Avg. loss per last 100 batches: 0.061501
Train batch 2900
Avg. loss per last 100 batches: 0.061501
Epoch: 12: Step: 2901/4907, loss=0.052640, lr=0.000014
Epoch: 12: Step: 2901/4907, loss=0.052640, lr=0.000014
Train batch 3000
Avg. loss per last 100 batches: 0.056937
Train batch 3000
Avg. loss per last 100 batches: 0.056937
Epoch: 12: Step: 3001/4907, loss=0.075360, lr=0.000014
Epoch: 12: Step: 3001/4907, loss=0.075360, lr=0.000014
Train batch 3100
Avg. loss per last 100 batches: 0.045703
Train batch 3100
Avg. loss per last 100 batches: 0.045703
Epoch: 12: Step: 3101/4907, loss=0.036153, lr=0.000014
Epoch: 12: Step: 3101/4907, loss=0.036153, lr=0.000014
Train batch 3200
Avg. loss per last 100 batches: 0.063043
Train batch 3200
Avg. loss per last 100 batches: 0.063043
Epoch: 12: Step: 3201/4907, loss=0.000109, lr=0.000014
Epoch: 12: Step: 3201/4907, loss=0.000109, lr=0.000014
Train batch 3300
Avg. loss per last 100 batches: 0.079993
Train batch 3300
Avg. loss per last 100 batches: 0.079993
Epoch: 12: Step: 3301/4907, loss=0.153865, lr=0.000014
Epoch: 12: Step: 3301/4907, loss=0.153865, lr=0.000014
Train batch 3400
Avg. loss per last 100 batches: 0.089038
Train batch 3400
Avg. loss per last 100 batches: 0.089038
Epoch: 12: Step: 3401/4907, loss=0.163339, lr=0.000014
Epoch: 12: Step: 3401/4907, loss=0.163339, lr=0.000014
Train batch 3500
Avg. loss per last 100 batches: 0.103295
Train batch 3500
Avg. loss per last 100 batches: 0.103295
Epoch: 12: Step: 3501/4907, loss=0.022000, lr=0.000014
Epoch: 12: Step: 3501/4907, loss=0.022000, lr=0.000014
Train batch 3600
Avg. loss per last 100 batches: 0.044664
Train batch 3600
Avg. loss per last 100 batches: 0.044664
Epoch: 12: Step: 3601/4907, loss=0.564945, lr=0.000014
Epoch: 12: Step: 3601/4907, loss=0.564945, lr=0.000014
Train batch 3700
Avg. loss per last 100 batches: 0.069840
Train batch 3700
Avg. loss per last 100 batches: 0.069840
Epoch: 12: Step: 3701/4907, loss=0.051575, lr=0.000014
Epoch: 12: Step: 3701/4907, loss=0.051575, lr=0.000014
Train batch 3800
Avg. loss per last 100 batches: 0.058523
Train batch 3800
Avg. loss per last 100 batches: 0.058523
Epoch: 12: Step: 3801/4907, loss=0.000500, lr=0.000014
Epoch: 12: Step: 3801/4907, loss=0.000500, lr=0.000014
Train batch 3900
Avg. loss per last 100 batches: 0.053695
Train batch 3900
Avg. loss per last 100 batches: 0.053695
Epoch: 12: Step: 3901/4907, loss=0.028615, lr=0.000014
Epoch: 12: Step: 3901/4907, loss=0.028615, lr=0.000014
Train batch 4000
Avg. loss per last 100 batches: 0.070014
Train batch 4000
Avg. loss per last 100 batches: 0.070014
Epoch: 12: Step: 4001/4907, loss=0.000153, lr=0.000014
Epoch: 12: Step: 4001/4907, loss=0.000153, lr=0.000014
Train batch 4100
Avg. loss per last 100 batches: 0.051709
Train batch 4100
Avg. loss per last 100 batches: 0.051709
Epoch: 12: Step: 4101/4907, loss=0.000002, lr=0.000014
Epoch: 12: Step: 4101/4907, loss=0.000002, lr=0.000014
Train batch 4200
Avg. loss per last 100 batches: 0.050759
Train batch 4200
Avg. loss per last 100 batches: 0.050759
Epoch: 12: Step: 4201/4907, loss=0.000085, lr=0.000014
Epoch: 12: Step: 4201/4907, loss=0.000085, lr=0.000014
Train batch 4300
Avg. loss per last 100 batches: 0.043624
Train batch 4300
Avg. loss per last 100 batches: 0.043624
Epoch: 12: Step: 4301/4907, loss=0.000002, lr=0.000014
Epoch: 12: Step: 4301/4907, loss=0.000002, lr=0.000014
Train batch 4400
Avg. loss per last 100 batches: 0.043285
Train batch 4400
Avg. loss per last 100 batches: 0.043285
Epoch: 12: Step: 4401/4907, loss=0.584432, lr=0.000014
Epoch: 12: Step: 4401/4907, loss=0.584432, lr=0.000014
Train batch 4500
Avg. loss per last 100 batches: 0.071847
Train batch 4500
Avg. loss per last 100 batches: 0.071847
Epoch: 12: Step: 4501/4907, loss=0.000019, lr=0.000014
Epoch: 12: Step: 4501/4907, loss=0.000019, lr=0.000014
Train batch 4600
Avg. loss per last 100 batches: 0.060204
Train batch 4600
Avg. loss per last 100 batches: 0.060204
Epoch: 12: Step: 4601/4907, loss=0.160092, lr=0.000014
Epoch: 12: Step: 4601/4907, loss=0.160092, lr=0.000014
Train batch 4700
Avg. loss per last 100 batches: 0.058352
Train batch 4700
Avg. loss per last 100 batches: 0.058352
Epoch: 12: Step: 4701/4907, loss=0.074308, lr=0.000014
Epoch: 12: Step: 4701/4907, loss=0.074308, lr=0.000014
Train batch 4800
Avg. loss per last 100 batches: 0.053635
Train batch 4800
Avg. loss per last 100 batches: 0.053635
Epoch: 12: Step: 4801/4907, loss=0.000080, lr=0.000014
Epoch: 12: Step: 4801/4907, loss=0.000080, lr=0.000014
Train batch 4900
Avg. loss per last 100 batches: 0.062613
Train batch 4900
Avg. loss per last 100 batches: 0.062613
Epoch: 12: Step: 4901/4907, loss=0.016783, lr=0.000014
Epoch: 12: Step: 4901/4907, loss=0.016783, lr=0.000014
Validation: Epoch: 12 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 12 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 216.97022713321056, total questions=6516
Av.rank validation: average rank 216.97022713321056, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.12.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.12.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 216.97022713321056, total questions=6516
Av.rank validation: average rank 216.97022713321056, total questions=6516
Av Loss per epoch=0.058477
epoch total correct predictions=57880
***** Epoch 13 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.12.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.12.4907
Av Loss per epoch=0.058477
epoch total correct predictions=57880
***** Epoch 13 *****
Epoch: 13: Step: 1/4907, loss=0.000031, lr=0.000014
Epoch: 13: Step: 1/4907, loss=0.000031, lr=0.000014
Train batch 100
Avg. loss per last 100 batches: 0.047447
Train batch 100
Avg. loss per last 100 batches: 0.047447
Epoch: 13: Step: 101/4907, loss=0.018081, lr=0.000014
Epoch: 13: Step: 101/4907, loss=0.018081, lr=0.000014
Train batch 200
Avg. loss per last 100 batches: 0.062231
Train batch 200
Avg. loss per last 100 batches: 0.062231
Epoch: 13: Step: 201/4907, loss=0.281501, lr=0.000014
Epoch: 13: Step: 201/4907, loss=0.281501, lr=0.000014
Train batch 300
Avg. loss per last 100 batches: 0.043268
Train batch 300
Avg. loss per last 100 batches: 0.043268
Epoch: 13: Step: 301/4907, loss=0.000009, lr=0.000014
Epoch: 13: Step: 301/4907, loss=0.000009, lr=0.000014
Train batch 400
Avg. loss per last 100 batches: 0.061138
Train batch 400
Avg. loss per last 100 batches: 0.061138
Epoch: 13: Step: 401/4907, loss=0.000568, lr=0.000014
Epoch: 13: Step: 401/4907, loss=0.000568, lr=0.000014
Train batch 500
Avg. loss per last 100 batches: 0.041920
Train batch 500
Avg. loss per last 100 batches: 0.041920
Epoch: 13: Step: 501/4907, loss=0.000095, lr=0.000014
Epoch: 13: Step: 501/4907, loss=0.000095, lr=0.000014
Train batch 600
Avg. loss per last 100 batches: 0.051671
Train batch 600
Avg. loss per last 100 batches: 0.051671
Epoch: 13: Step: 601/4907, loss=0.005995, lr=0.000014
Epoch: 13: Step: 601/4907, loss=0.005995, lr=0.000014
Train batch 700
Avg. loss per last 100 batches: 0.020281
Train batch 700
Avg. loss per last 100 batches: 0.020281
Epoch: 13: Step: 701/4907, loss=0.000271, lr=0.000014
Epoch: 13: Step: 701/4907, loss=0.000271, lr=0.000014
Train batch 800
Avg. loss per last 100 batches: 0.069216
Train batch 800
Avg. loss per last 100 batches: 0.069216
Epoch: 13: Step: 801/4907, loss=0.002502, lr=0.000014
Epoch: 13: Step: 801/4907, loss=0.002502, lr=0.000014
Train batch 900
Avg. loss per last 100 batches: 0.042779
Train batch 900
Avg. loss per last 100 batches: 0.042779
Epoch: 13: Step: 901/4907, loss=0.000574, lr=0.000013
Epoch: 13: Step: 901/4907, loss=0.000574, lr=0.000013
Train batch 1000
Avg. loss per last 100 batches: 0.069572
Train batch 1000
Avg. loss per last 100 batches: 0.069572
Epoch: 13: Step: 1001/4907, loss=0.007274, lr=0.000013
Epoch: 13: Step: 1001/4907, loss=0.007274, lr=0.000013
Train batch 1100
Avg. loss per last 100 batches: 0.057850
Train batch 1100
Avg. loss per last 100 batches: 0.057850
Epoch: 13: Step: 1101/4907, loss=0.000002, lr=0.000013
Epoch: 13: Step: 1101/4907, loss=0.000002, lr=0.000013
Train batch 1200
Avg. loss per last 100 batches: 0.038172
Train batch 1200
Avg. loss per last 100 batches: 0.038172
Epoch: 13: Step: 1201/4907, loss=0.000004, lr=0.000013
Epoch: 13: Step: 1201/4907, loss=0.000004, lr=0.000013
Train batch 1300
Avg. loss per last 100 batches: 0.040518
Train batch 1300
Avg. loss per last 100 batches: 0.040518
Epoch: 13: Step: 1301/4907, loss=0.000014, lr=0.000013
Epoch: 13: Step: 1301/4907, loss=0.000014, lr=0.000013
Train batch 1400
Avg. loss per last 100 batches: 0.051847
Train batch 1400
Avg. loss per last 100 batches: 0.051847
Epoch: 13: Step: 1401/4907, loss=0.296537, lr=0.000013
Epoch: 13: Step: 1401/4907, loss=0.296537, lr=0.000013
Train batch 1500
Avg. loss per last 100 batches: 0.047719
Train batch 1500
Avg. loss per last 100 batches: 0.047719
Epoch: 13: Step: 1501/4907, loss=0.000079, lr=0.000013
Epoch: 13: Step: 1501/4907, loss=0.000079, lr=0.000013
Train batch 1600
Avg. loss per last 100 batches: 0.035464
Train batch 1600
Avg. loss per last 100 batches: 0.035464
Epoch: 13: Step: 1601/4907, loss=0.000840, lr=0.000013
Epoch: 13: Step: 1601/4907, loss=0.000840, lr=0.000013
Train batch 1700
Avg. loss per last 100 batches: 0.055355
Train batch 1700
Avg. loss per last 100 batches: 0.055355
Epoch: 13: Step: 1701/4907, loss=0.010222, lr=0.000013
Epoch: 13: Step: 1701/4907, loss=0.010222, lr=0.000013
Train batch 1800
Avg. loss per last 100 batches: 0.030735
Train batch 1800
Avg. loss per last 100 batches: 0.030735
Epoch: 13: Step: 1801/4907, loss=0.000137, lr=0.000013
Epoch: 13: Step: 1801/4907, loss=0.000137, lr=0.000013
Train batch 1900
Avg. loss per last 100 batches: 0.075833
Train batch 1900
Avg. loss per last 100 batches: 0.075833
Epoch: 13: Step: 1901/4907, loss=0.000190, lr=0.000013
Epoch: 13: Step: 1901/4907, loss=0.000190, lr=0.000013
Train batch 2000
Avg. loss per last 100 batches: 0.049873
Train batch 2000
Avg. loss per last 100 batches: 0.049873
Epoch: 13: Step: 2001/4907, loss=0.000229, lr=0.000013
Epoch: 13: Step: 2001/4907, loss=0.000229, lr=0.000013
Train batch 2100
Avg. loss per last 100 batches: 0.042467
Train batch 2100
Avg. loss per last 100 batches: 0.042467
Epoch: 13: Step: 2101/4907, loss=0.004397, lr=0.000013
Epoch: 13: Step: 2101/4907, loss=0.004397, lr=0.000013
Train batch 2200
Avg. loss per last 100 batches: 0.038204
Train batch 2200
Avg. loss per last 100 batches: 0.038204
Epoch: 13: Step: 2201/4907, loss=0.000045, lr=0.000013
Epoch: 13: Step: 2201/4907, loss=0.000045, lr=0.000013
Train batch 2300
Avg. loss per last 100 batches: 0.036913
Train batch 2300
Avg. loss per last 100 batches: 0.036913
Epoch: 13: Step: 2301/4907, loss=0.000333, lr=0.000013
Epoch: 13: Step: 2301/4907, loss=0.000333, lr=0.000013
Train batch 2400
Avg. loss per last 100 batches: 0.055779
Train batch 2400
Avg. loss per last 100 batches: 0.055779
Epoch: 13: Step: 2401/4907, loss=0.000014, lr=0.000013
Epoch: 13: Step: 2401/4907, loss=0.000014, lr=0.000013
Train batch 2500
Avg. loss per last 100 batches: 0.073163
Train batch 2500
Avg. loss per last 100 batches: 0.073163
Epoch: 13: Step: 2501/4907, loss=0.021309, lr=0.000013
Epoch: 13: Step: 2501/4907, loss=0.021309, lr=0.000013
Train batch 2600
Avg. loss per last 100 batches: 0.049371
Train batch 2600
Avg. loss per last 100 batches: 0.049371
Epoch: 13: Step: 2601/4907, loss=0.000073, lr=0.000013
Epoch: 13: Step: 2601/4907, loss=0.000073, lr=0.000013
Train batch 2700
Avg. loss per last 100 batches: 0.056636
Train batch 2700
Avg. loss per last 100 batches: 0.056636
Epoch: 13: Step: 2701/4907, loss=0.000013, lr=0.000013
Epoch: 13: Step: 2701/4907, loss=0.000013, lr=0.000013
Train batch 2800
Avg. loss per last 100 batches: 0.082267
Train batch 2800
Avg. loss per last 100 batches: 0.082267
Epoch: 13: Step: 2801/4907, loss=0.000081, lr=0.000013
Epoch: 13: Step: 2801/4907, loss=0.000081, lr=0.000013
Train batch 2900
Avg. loss per last 100 batches: 0.061213
Train batch 2900
Avg. loss per last 100 batches: 0.061213
Epoch: 13: Step: 2901/4907, loss=0.257709, lr=0.000013
Epoch: 13: Step: 2901/4907, loss=0.257709, lr=0.000013
Train batch 3000
Avg. loss per last 100 batches: 0.043972
Train batch 3000
Avg. loss per last 100 batches: 0.043972
Epoch: 13: Step: 3001/4907, loss=0.002238, lr=0.000013
Epoch: 13: Step: 3001/4907, loss=0.002238, lr=0.000013
Train batch 3100
Avg. loss per last 100 batches: 0.060348
Train batch 3100
Avg. loss per last 100 batches: 0.060348
Epoch: 13: Step: 3101/4907, loss=0.000062, lr=0.000013
Epoch: 13: Step: 3101/4907, loss=0.000062, lr=0.000013
Train batch 3200
Avg. loss per last 100 batches: 0.052380
Train batch 3200
Avg. loss per last 100 batches: 0.052380
Epoch: 13: Step: 3201/4907, loss=0.000377, lr=0.000013
Epoch: 13: Step: 3201/4907, loss=0.000377, lr=0.000013
Train batch 3300
Avg. loss per last 100 batches: 0.038054
Train batch 3300
Avg. loss per last 100 batches: 0.038054
Epoch: 13: Step: 3301/4907, loss=0.000023, lr=0.000013
Epoch: 13: Step: 3301/4907, loss=0.000023, lr=0.000013
Train batch 3400
Avg. loss per last 100 batches: 0.043615
Train batch 3400
Avg. loss per last 100 batches: 0.043615
Epoch: 13: Step: 3401/4907, loss=0.001232, lr=0.000013
Epoch: 13: Step: 3401/4907, loss=0.001232, lr=0.000013
Train batch 3500
Avg. loss per last 100 batches: 0.056543
Train batch 3500
Avg. loss per last 100 batches: 0.056543
Epoch: 13: Step: 3501/4907, loss=0.000423, lr=0.000013
Epoch: 13: Step: 3501/4907, loss=0.000423, lr=0.000013
Train batch 3600
Avg. loss per last 100 batches: 0.037716
Train batch 3600
Avg. loss per last 100 batches: 0.037716
Epoch: 13: Step: 3601/4907, loss=0.000099, lr=0.000013
Epoch: 13: Step: 3601/4907, loss=0.000099, lr=0.000013
Train batch 3700
Avg. loss per last 100 batches: 0.047557
Train batch 3700
Avg. loss per last 100 batches: 0.047557
Epoch: 13: Step: 3701/4907, loss=0.000034, lr=0.000013
Epoch: 13: Step: 3701/4907, loss=0.000034, lr=0.000013
Train batch 3800
Avg. loss per last 100 batches: 0.072199
Train batch 3800
Avg. loss per last 100 batches: 0.072199
Epoch: 13: Step: 3801/4907, loss=0.000044, lr=0.000013
Epoch: 13: Step: 3801/4907, loss=0.000044, lr=0.000013
Train batch 3900
Avg. loss per last 100 batches: 0.072328
Train batch 3900
Avg. loss per last 100 batches: 0.072328
Epoch: 13: Step: 3901/4907, loss=0.000060, lr=0.000013
Epoch: 13: Step: 3901/4907, loss=0.000060, lr=0.000013
Train batch 4000
Avg. loss per last 100 batches: 0.079551
Train batch 4000
Avg. loss per last 100 batches: 0.079551
Epoch: 13: Step: 4001/4907, loss=0.113471, lr=0.000013
Epoch: 13: Step: 4001/4907, loss=0.113471, lr=0.000013
Train batch 4100
Avg. loss per last 100 batches: 0.045846
Train batch 4100
Avg. loss per last 100 batches: 0.045846
Epoch: 13: Step: 4101/4907, loss=0.001682, lr=0.000013
Epoch: 13: Step: 4101/4907, loss=0.001682, lr=0.000013
Train batch 4200
Avg. loss per last 100 batches: 0.075189
Train batch 4200
Avg. loss per last 100 batches: 0.075189
Epoch: 13: Step: 4201/4907, loss=0.000734, lr=0.000013
Epoch: 13: Step: 4201/4907, loss=0.000734, lr=0.000013
Train batch 4300
Avg. loss per last 100 batches: 0.044639
Train batch 4300
Avg. loss per last 100 batches: 0.044639
Epoch: 13: Step: 4301/4907, loss=0.001988, lr=0.000013
Epoch: 13: Step: 4301/4907, loss=0.001988, lr=0.000013
Train batch 4400
Avg. loss per last 100 batches: 0.067495
Train batch 4400
Avg. loss per last 100 batches: 0.067495
Epoch: 13: Step: 4401/4907, loss=0.000009, lr=0.000013
Epoch: 13: Step: 4401/4907, loss=0.000009, lr=0.000013
Train batch 4500
Avg. loss per last 100 batches: 0.079912
Train batch 4500
Avg. loss per last 100 batches: 0.079912
Epoch: 13: Step: 4501/4907, loss=0.796460, lr=0.000013
Epoch: 13: Step: 4501/4907, loss=0.796460, lr=0.000013
Train batch 4600
Avg. loss per last 100 batches: 0.049977
Train batch 4600
Avg. loss per last 100 batches: 0.049977
Epoch: 13: Step: 4601/4907, loss=0.000006, lr=0.000013
Epoch: 13: Step: 4601/4907, loss=0.000006, lr=0.000013
Train batch 4700
Avg. loss per last 100 batches: 0.037608
Train batch 4700
Avg. loss per last 100 batches: 0.037608
Epoch: 13: Step: 4701/4907, loss=0.001008, lr=0.000013
Epoch: 13: Step: 4701/4907, loss=0.001008, lr=0.000013
Train batch 4800
Avg. loss per last 100 batches: 0.061857
Train batch 4800
Avg. loss per last 100 batches: 0.061857
Epoch: 13: Step: 4801/4907, loss=0.001438, lr=0.000013
Epoch: 13: Step: 4801/4907, loss=0.001438, lr=0.000013
Train batch 4900
Avg. loss per last 100 batches: 0.038840
Train batch 4900
Avg. loss per last 100 batches: 0.038840
Epoch: 13: Step: 4901/4907, loss=0.000116, lr=0.000013
Epoch: 13: Step: 4901/4907, loss=0.000116, lr=0.000013
Validation: Epoch: 13 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 13 Step: 4907/4907
Reading file ../data/NQ/biencoder-nq-dev.json
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 234.37154696132598, total questions=6516
Av.rank validation: average rank 234.37154696132598, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.13.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.13.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: average rank 234.37154696132598, total questions=6516
Av.rank validation: average rank 234.37154696132598, total questions=6516
Av Loss per epoch=0.052879
epoch total correct predictions=57996
***** Epoch 14 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.13.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.13.4907
Av Loss per epoch=0.052879
epoch total correct predictions=57996
***** Epoch 14 *****
Epoch: 14: Step: 1/4907, loss=0.000674, lr=0.000013
Epoch: 14: Step: 1/4907, loss=0.000674, lr=0.000013
Train batch 100
Avg. loss per last 100 batches: 0.062176
Train batch 100
Avg. loss per last 100 batches: 0.062176
Epoch: 14: Step: 101/4907, loss=0.000533, lr=0.000013
Epoch: 14: Step: 101/4907, loss=0.000533, lr=0.000013
Train batch 200
Avg. loss per last 100 batches: 0.042506
Train batch 200
Avg. loss per last 100 batches: 0.042506
Epoch: 14: Step: 201/4907, loss=0.198512, lr=0.000013
Epoch: 14: Step: 201/4907, loss=0.198512, lr=0.000013
Train batch 300
Avg. loss per last 100 batches: 0.035192
Train batch 300
Avg. loss per last 100 batches: 0.035192
Epoch: 14: Step: 301/4907, loss=0.024540, lr=0.000013
Epoch: 14: Step: 301/4907, loss=0.024540, lr=0.000013
Train batch 400
Avg. loss per last 100 batches: 0.041315
Train batch 400
Avg. loss per last 100 batches: 0.041315
Epoch: 14: Step: 401/4907, loss=0.000001, lr=0.000013
Epoch: 14: Step: 401/4907, loss=0.000001, lr=0.000013
Train batch 500
Avg. loss per last 100 batches: 0.037204
Train batch 500
Avg. loss per last 100 batches: 0.037204
Epoch: 14: Step: 501/4907, loss=0.004818, lr=0.000013
Epoch: 14: Step: 501/4907, loss=0.004818, lr=0.000013
Train batch 600
Avg. loss per last 100 batches: 0.064358
Train batch 600
Avg. loss per last 100 batches: 0.064358
Epoch: 14: Step: 601/4907, loss=0.000284, lr=0.000013
Epoch: 14: Step: 601/4907, loss=0.000284, lr=0.000013
Train batch 700
Avg. loss per last 100 batches: 0.040500
Train batch 700
Avg. loss per last 100 batches: 0.040500
Epoch: 14: Step: 701/4907, loss=0.000335, lr=0.000013
Epoch: 14: Step: 701/4907, loss=0.000335, lr=0.000013
Train batch 800
Avg. loss per last 100 batches: 0.039389
Train batch 800
Avg. loss per last 100 batches: 0.039389
Epoch: 14: Step: 801/4907, loss=0.000562, lr=0.000013
Epoch: 14: Step: 801/4907, loss=0.000562, lr=0.000013
Train batch 900
Avg. loss per last 100 batches: 0.066153
Train batch 900
Avg. loss per last 100 batches: 0.066153
Epoch: 14: Step: 901/4907, loss=0.000399, lr=0.000013
Epoch: 14: Step: 901/4907, loss=0.000399, lr=0.000013
Train batch 1000
Avg. loss per last 100 batches: 0.060294
Train batch 1000
Avg. loss per last 100 batches: 0.060294
Epoch: 14: Step: 1001/4907, loss=0.000013, lr=0.000013
Epoch: 14: Step: 1001/4907, loss=0.000013, lr=0.000013
Train batch 1100
Avg. loss per last 100 batches: 0.050550
Train batch 1100
Avg. loss per last 100 batches: 0.050550
Epoch: 14: Step: 1101/4907, loss=0.001142, lr=0.000013
Epoch: 14: Step: 1101/4907, loss=0.001142, lr=0.000013
Train batch 1200
Avg. loss per last 100 batches: 0.049500
Train batch 1200
Avg. loss per last 100 batches: 0.049500
Epoch: 14: Step: 1201/4907, loss=0.000035, lr=0.000013
Epoch: 14: Step: 1201/4907, loss=0.000035, lr=0.000013
Train batch 1300
Avg. loss per last 100 batches: 0.075892
Train batch 1300
Avg. loss per last 100 batches: 0.075892
Epoch: 14: Step: 1301/4907, loss=0.005944, lr=0.000013
Epoch: 14: Step: 1301/4907, loss=0.005944, lr=0.000013
Train batch 1400
Avg. loss per last 100 batches: 0.070360
Train batch 1400
Avg. loss per last 100 batches: 0.070360
Epoch: 14: Step: 1401/4907, loss=0.000133, lr=0.000013
Epoch: 14: Step: 1401/4907, loss=0.000133, lr=0.000013
Train batch 1500
Avg. loss per last 100 batches: 0.058507
Train batch 1500
Avg. loss per last 100 batches: 0.058507
Epoch: 14: Step: 1501/4907, loss=0.000549, lr=0.000013
Epoch: 14: Step: 1501/4907, loss=0.000549, lr=0.000013
Train batch 1600
Avg. loss per last 100 batches: 0.049869
Train batch 1600
Avg. loss per last 100 batches: 0.049869
Epoch: 14: Step: 1601/4907, loss=0.003403, lr=0.000013
Epoch: 14: Step: 1601/4907, loss=0.003403, lr=0.000013
Train batch 1700
Avg. loss per last 100 batches: 0.034861
Train batch 1700
Avg. loss per last 100 batches: 0.034861
Epoch: 14: Step: 1701/4907, loss=0.000027, lr=0.000013
Epoch: 14: Step: 1701/4907, loss=0.000027, lr=0.000013
Train batch 1800
Avg. loss per last 100 batches: 0.065202
Train batch 1800
Avg. loss per last 100 batches: 0.065202
Epoch: 14: Step: 1801/4907, loss=0.000151, lr=0.000013
Epoch: 14: Step: 1801/4907, loss=0.000151, lr=0.000013
Train batch 1900
Avg. loss per last 100 batches: 0.046145
Train batch 1900
Avg. loss per last 100 batches: 0.046145
Epoch: 14: Step: 1901/4907, loss=0.000004, lr=0.000013
Epoch: 14: Step: 1901/4907, loss=0.000004, lr=0.000013
Train batch 2000
Avg. loss per last 100 batches: 0.054527
Train batch 2000
Avg. loss per last 100 batches: 0.054527
Epoch: 14: Step: 2001/4907, loss=0.000045, lr=0.000013
Epoch: 14: Step: 2001/4907, loss=0.000045, lr=0.000013
Train batch 2100
Avg. loss per last 100 batches: 0.023258
Train batch 2100
Avg. loss per last 100 batches: 0.023258
Epoch: 14: Step: 2101/4907, loss=0.000000, lr=0.000013
Epoch: 14: Step: 2101/4907, loss=0.000000, lr=0.000013
Train batch 2200
Avg. loss per last 100 batches: 0.061366
Train batch 2200
Avg. loss per last 100 batches: 0.061366
Epoch: 14: Step: 2201/4907, loss=0.399750, lr=0.000013
Epoch: 14: Step: 2201/4907, loss=0.399750, lr=0.000013
Train batch 2300
Avg. loss per last 100 batches: 0.049184
Train batch 2300
Avg. loss per last 100 batches: 0.049184
Epoch: 14: Step: 2301/4907, loss=0.126098, lr=0.000013
Epoch: 14: Step: 2301/4907, loss=0.126098, lr=0.000013
Train batch 2400
Avg. loss per last 100 batches: 0.101070
Train batch 2400
Avg. loss per last 100 batches: 0.101070
Epoch: 14: Step: 2401/4907, loss=0.034958, lr=0.000013
Epoch: 14: Step: 2401/4907, loss=0.034958, lr=0.000013
Train batch 2500
Avg. loss per last 100 batches: 0.094785
Train batch 2500
Avg. loss per last 100 batches: 0.094785
Epoch: 14: Step: 2501/4907, loss=0.234605, lr=0.000013
Epoch: 14: Step: 2501/4907, loss=0.234605, lr=0.000013
Train batch 2600
Avg. loss per last 100 batches: 0.023361
Train batch 2600
Avg. loss per last 100 batches: 0.023361
Epoch: 14: Step: 2601/4907, loss=0.000001, lr=0.000013
Epoch: 14: Step: 2601/4907, loss=0.000001, lr=0.000013
Train batch 2700
Avg. loss per last 100 batches: 0.070830
Train batch 2700
Avg. loss per last 100 batches: 0.070830
Epoch: 14: Step: 2701/4907, loss=0.001670, lr=0.000013
Epoch: 14: Step: 2701/4907, loss=0.001670, lr=0.000013
Train batch 2800
Avg. loss per last 100 batches: 0.066532
Train batch 2800
Avg. loss per last 100 batches: 0.066532
Epoch: 14: Step: 2801/4907, loss=0.000002, lr=0.000013
Epoch: 14: Step: 2801/4907, loss=0.000002, lr=0.000013
Train batch 2900
Avg. loss per last 100 batches: 0.037826
Train batch 2900
Avg. loss per last 100 batches: 0.037826
Epoch: 14: Step: 2901/4907, loss=0.000082, lr=0.000013
Epoch: 14: Step: 2901/4907, loss=0.000082, lr=0.000013
Train batch 3000
Avg. loss per last 100 batches: 0.037110
Train batch 3000
Avg. loss per last 100 batches: 0.037110
Epoch: 14: Step: 3001/4907, loss=0.000012, lr=0.000013
Epoch: 14: Step: 3001/4907, loss=0.000012, lr=0.000013
Train batch 3100
Avg. loss per last 100 batches: 0.065703
Train batch 3100
Avg. loss per last 100 batches: 0.065703
Epoch: 14: Step: 3101/4907, loss=0.006154, lr=0.000013
Epoch: 14: Step: 3101/4907, loss=0.006154, lr=0.000013
Train batch 3200
Avg. loss per last 100 batches: 0.034929
Train batch 3200
Avg. loss per last 100 batches: 0.034929
Epoch: 14: Step: 3201/4907, loss=0.005357, lr=0.000013
Epoch: 14: Step: 3201/4907, loss=0.005357, lr=0.000013
Train batch 3300
Avg. loss per last 100 batches: 0.047775
Train batch 3300
Avg. loss per last 100 batches: 0.047775
Epoch: 14: Step: 3301/4907, loss=0.103714, lr=0.000013
Epoch: 14: Step: 3301/4907, loss=0.103714, lr=0.000013
Train batch 3400
Avg. loss per last 100 batches: 0.080828
Train batch 3400
Avg. loss per last 100 batches: 0.080828
Epoch: 14: Step: 3401/4907, loss=0.000033, lr=0.000013
Epoch: 14: Step: 3401/4907, loss=0.000033, lr=0.000013
Train batch 3500
Avg. loss per last 100 batches: 0.064911
Train batch 3500
Avg. loss per last 100 batches: 0.064911
Epoch: 14: Step: 3501/4907, loss=0.035081, lr=0.000013
Epoch: 14: Step: 3501/4907, loss=0.035081, lr=0.000013
Train batch 3600
Avg. loss per last 100 batches: 0.034659
Train batch 3600
Avg. loss per last 100 batches: 0.034659
Epoch: 14: Step: 3601/4907, loss=0.000001, lr=0.000013
Epoch: 14: Step: 3601/4907, loss=0.000001, lr=0.000013
Train batch 3700
Avg. loss per last 100 batches: 0.038146
Train batch 3700
Avg. loss per last 100 batches: 0.038146
Epoch: 14: Step: 3701/4907, loss=0.000013, lr=0.000013
Epoch: 14: Step: 3701/4907, loss=0.000013, lr=0.000013
Train batch 3800
Avg. loss per last 100 batches: 0.043231
Train batch 3800
Avg. loss per last 100 batches: 0.043231
Epoch: 14: Step: 3801/4907, loss=0.001905, lr=0.000013
Epoch: 14: Step: 3801/4907, loss=0.001905, lr=0.000013
Train batch 3900
Avg. loss per last 100 batches: 0.081166
Train batch 3900
Avg. loss per last 100 batches: 0.081166
Epoch: 14: Step: 3901/4907, loss=0.000177, lr=0.000013
Epoch: 14: Step: 3901/4907, loss=0.000177, lr=0.000013
Train batch 4000
Avg. loss per last 100 batches: 0.034631
Train batch 4000
Avg. loss per last 100 batches: 0.034631
Epoch: 14: Step: 4001/4907, loss=0.047266, lr=0.000013
Epoch: 14: Step: 4001/4907, loss=0.047266, lr=0.000013
Train batch 4100
Avg. loss per last 100 batches: 0.064347
Train batch 4100
Avg. loss per last 100 batches: 0.064347
Epoch: 14: Step: 4101/4907, loss=0.001609, lr=0.000013
Epoch: 14: Step: 4101/4907, loss=0.001609, lr=0.000013
Train batch 4200
Avg. loss per last 100 batches: 0.032960
Train batch 4200
Avg. loss per last 100 batches: 0.032960
Epoch: 14: Step: 4201/4907, loss=0.038453, lr=0.000013
Epoch: 14: Step: 4201/4907, loss=0.038453, lr=0.000013
Train batch 4300
Avg. loss per last 100 batches: 0.061598
Train batch 4300
Avg. loss per last 100 batches: 0.061598
Epoch: 14: Step: 4301/4907, loss=0.000032, lr=0.000013
Epoch: 14: Step: 4301/4907, loss=0.000032, lr=0.000013
Train batch 4400
Avg. loss per last 100 batches: 0.037995
Train batch 4400
Avg. loss per last 100 batches: 0.037995
Epoch: 14: Step: 4401/4907, loss=0.028549, lr=0.000013
Epoch: 14: Step: 4401/4907, loss=0.028549, lr=0.000013
Train batch 4500
Avg. loss per last 100 batches: 0.032351
Train batch 4500
Avg. loss per last 100 batches: 0.032351
Epoch: 14: Step: 4501/4907, loss=0.000004, lr=0.000013
Epoch: 14: Step: 4501/4907, loss=0.000004, lr=0.000013
Train batch 4600
Avg. loss per last 100 batches: 0.039980
Train batch 4600
Avg. loss per last 100 batches: 0.039980
Epoch: 14: Step: 4601/4907, loss=0.000025, lr=0.000013
Epoch: 14: Step: 4601/4907, loss=0.000025, lr=0.000013
Train batch 4700
Avg. loss per last 100 batches: 0.058514
Train batch 4700
Avg. loss per last 100 batches: 0.058514
Epoch: 14: Step: 4701/4907, loss=0.013950, lr=0.000013
Epoch: 14: Step: 4701/4907, loss=0.013950, lr=0.000013
Train batch 4800
Avg. loss per last 100 batches: 0.093436
Train batch 4800
Avg. loss per last 100 batches: 0.093436
Epoch: 14: Step: 4801/4907, loss=0.001141, lr=0.000013
Epoch: 14: Step: 4801/4907, loss=0.001141, lr=0.000013
Train batch 4900
Avg. loss per last 100 batches: 0.060888
Train batch 4900
Avg. loss per last 100 batches: 0.060888
Epoch: 14: Step: 4901/4907, loss=0.002516, lr=0.000013
Epoch: 14: Step: 4901/4907, loss=0.002516, lr=0.000013
Validation: Epoch: 14 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 14 Step: 4907/4907
Reading file ../data/NQ/biencoder-nq-dev.json
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 185.75260896255372, total questions=6516
Av.rank validation: average rank 185.75260896255372, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.14.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.14.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.14.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 185.75260896255372, total questions=6516
Av.rank validation: average rank 185.75260896255372, total questions=6516
Av Loss per epoch=0.053518
epoch total correct predictions=57969
***** Epoch 15 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.14.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.14.4907
Av Loss per epoch=0.053518
epoch total correct predictions=57969
***** Epoch 15 *****
Epoch: 15: Step: 1/4907, loss=0.015643, lr=0.000013
Epoch: 15: Step: 1/4907, loss=0.015643, lr=0.000013
Train batch 100
Avg. loss per last 100 batches: 0.054537
Train batch 100
Avg. loss per last 100 batches: 0.054537
Epoch: 15: Step: 101/4907, loss=0.009355, lr=0.000013
Epoch: 15: Step: 101/4907, loss=0.009355, lr=0.000013
Train batch 200
Avg. loss per last 100 batches: 0.039664
Train batch 200
Avg. loss per last 100 batches: 0.039664
Epoch: 15: Step: 201/4907, loss=0.005572, lr=0.000013
Epoch: 15: Step: 201/4907, loss=0.005572, lr=0.000013
Train batch 300
Avg. loss per last 100 batches: 0.053331
Train batch 300
Avg. loss per last 100 batches: 0.053331
Epoch: 15: Step: 301/4907, loss=0.004446, lr=0.000013
Epoch: 15: Step: 301/4907, loss=0.004446, lr=0.000013
Train batch 400
Avg. loss per last 100 batches: 0.044911
Train batch 400
Avg. loss per last 100 batches: 0.044911
Epoch: 15: Step: 401/4907, loss=0.001841, lr=0.000013
Epoch: 15: Step: 401/4907, loss=0.001841, lr=0.000013
Train batch 500
Avg. loss per last 100 batches: 0.029854
Train batch 500
Avg. loss per last 100 batches: 0.029854
Epoch: 15: Step: 501/4907, loss=0.009716, lr=0.000013
Epoch: 15: Step: 501/4907, loss=0.009716, lr=0.000013
Train batch 600
Avg. loss per last 100 batches: 0.054627
Train batch 600
Avg. loss per last 100 batches: 0.054627
Epoch: 15: Step: 601/4907, loss=0.001082, lr=0.000013
Epoch: 15: Step: 601/4907, loss=0.001082, lr=0.000013
Train batch 700
Avg. loss per last 100 batches: 0.056318
Train batch 700
Avg. loss per last 100 batches: 0.056318
Epoch: 15: Step: 701/4907, loss=0.000063, lr=0.000013
Epoch: 15: Step: 701/4907, loss=0.000063, lr=0.000013
Train batch 800
Avg. loss per last 100 batches: 0.045886
Train batch 800
Avg. loss per last 100 batches: 0.045886
Epoch: 15: Step: 801/4907, loss=0.003954, lr=0.000012
Epoch: 15: Step: 801/4907, loss=0.003954, lr=0.000012
Train batch 900
Avg. loss per last 100 batches: 0.027535
Train batch 900
Avg. loss per last 100 batches: 0.027535
Epoch: 15: Step: 901/4907, loss=0.064072, lr=0.000012
Epoch: 15: Step: 901/4907, loss=0.064072, lr=0.000012
Train batch 1000
Avg. loss per last 100 batches: 0.045018
Train batch 1000
Avg. loss per last 100 batches: 0.045018
Epoch: 15: Step: 1001/4907, loss=0.020658, lr=0.000012
Epoch: 15: Step: 1001/4907, loss=0.020658, lr=0.000012
Train batch 1100
Train batch 1100
Avg. loss per last 100 batches: 0.072125
Avg. loss per last 100 batches: 0.072125
Epoch: 15: Step: 1101/4907, loss=0.000031, lr=0.000012
Epoch: 15: Step: 1101/4907, loss=0.000031, lr=0.000012
Train batch 1200
Avg. loss per last 100 batches: 0.040571
Train batch 1200
Avg. loss per last 100 batches: 0.040571
Epoch: 15: Step: 1201/4907, loss=0.648412, lr=0.000012
Epoch: 15: Step: 1201/4907, loss=0.648412, lr=0.000012
Train batch 1300
Avg. loss per last 100 batches: 0.041626
Train batch 1300
Avg. loss per last 100 batches: 0.041626
Epoch: 15: Step: 1301/4907, loss=0.000025, lr=0.000012
Epoch: 15: Step: 1301/4907, loss=0.000025, lr=0.000012
Train batch 1400
Avg. loss per last 100 batches: 0.040689
Train batch 1400
Avg. loss per last 100 batches: 0.040689
Epoch: 15: Step: 1401/4907, loss=0.005365, lr=0.000012
Epoch: 15: Step: 1401/4907, loss=0.005365, lr=0.000012
Train batch 1500
Avg. loss per last 100 batches: 0.061753
Train batch 1500
Avg. loss per last 100 batches: 0.061753
Epoch: 15: Step: 1501/4907, loss=0.001488, lr=0.000012
Epoch: 15: Step: 1501/4907, loss=0.001488, lr=0.000012
Train batch 1600
Avg. loss per last 100 batches: 0.040090
Train batch 1600
Avg. loss per last 100 batches: 0.040090
Epoch: 15: Step: 1601/4907, loss=0.006033, lr=0.000012
Epoch: 15: Step: 1601/4907, loss=0.006033, lr=0.000012
Train batch 1700
Avg. loss per last 100 batches: 0.044367
Train batch 1700
Avg. loss per last 100 batches: 0.044367
Epoch: 15: Step: 1701/4907, loss=0.000316, lr=0.000012
Epoch: 15: Step: 1701/4907, loss=0.000316, lr=0.000012
Train batch 1800
Avg. loss per last 100 batches: 0.065970
Train batch 1800
Avg. loss per last 100 batches: 0.065970
Epoch: 15: Step: 1801/4907, loss=0.407908, lr=0.000012
Epoch: 15: Step: 1801/4907, loss=0.407908, lr=0.000012
Train batch 1900
Avg. loss per last 100 batches: 0.048511
Train batch 1900
Avg. loss per last 100 batches: 0.048511
Epoch: 15: Step: 1901/4907, loss=0.000199, lr=0.000012
Epoch: 15: Step: 1901/4907, loss=0.000199, lr=0.000012
Train batch 2000
Avg. loss per last 100 batches: 0.033499
Train batch 2000
Avg. loss per last 100 batches: 0.033499
Epoch: 15: Step: 2001/4907, loss=0.000416, lr=0.000012
Epoch: 15: Step: 2001/4907, loss=0.000416, lr=0.000012
Train batch 2100
Avg. loss per last 100 batches: 0.071862
Train batch 2100
Avg. loss per last 100 batches: 0.071862
Epoch: 15: Step: 2101/4907, loss=0.000010, lr=0.000012
Epoch: 15: Step: 2101/4907, loss=0.000010, lr=0.000012
Train batch 2200
Avg. loss per last 100 batches: 0.048230
Train batch 2200
Avg. loss per last 100 batches: 0.048230
Epoch: 15: Step: 2201/4907, loss=0.000041, lr=0.000012
Epoch: 15: Step: 2201/4907, loss=0.000041, lr=0.000012
Train batch 2300
Avg. loss per last 100 batches: 0.047935
Train batch 2300
Avg. loss per last 100 batches: 0.047935
Epoch: 15: Step: 2301/4907, loss=0.018576, lr=0.000012
Epoch: 15: Step: 2301/4907, loss=0.018576, lr=0.000012
Train batch 2400
Avg. loss per last 100 batches: 0.026700
Train batch 2400
Avg. loss per last 100 batches: 0.026700
Epoch: 15: Step: 2401/4907, loss=0.000607, lr=0.000012
Epoch: 15: Step: 2401/4907, loss=0.000607, lr=0.000012
Train batch 2500
Avg. loss per last 100 batches: 0.079500
Train batch 2500
Avg. loss per last 100 batches: 0.079500
Epoch: 15: Step: 2501/4907, loss=0.000000, lr=0.000012
Epoch: 15: Step: 2501/4907, loss=0.000000, lr=0.000012
Train batch 2600
Avg. loss per last 100 batches: 0.048452
Train batch 2600
Avg. loss per last 100 batches: 0.048452
Epoch: 15: Step: 2601/4907, loss=0.000012, lr=0.000012
Epoch: 15: Step: 2601/4907, loss=0.000012, lr=0.000012
Train batch 2700
Avg. loss per last 100 batches: 0.018566
Train batch 2700
Avg. loss per last 100 batches: 0.018566
Epoch: 15: Step: 2701/4907, loss=0.000438, lr=0.000012
Epoch: 15: Step: 2701/4907, loss=0.000438, lr=0.000012
Train batch 2800
Avg. loss per last 100 batches: 0.053899
Train batch 2800
Avg. loss per last 100 batches: 0.053899
Epoch: 15: Step: 2801/4907, loss=0.074280, lr=0.000012
Epoch: 15: Step: 2801/4907, loss=0.074280, lr=0.000012
Train batch 2900
Avg. loss per last 100 batches: 0.054275
Train batch 2900
Avg. loss per last 100 batches: 0.054275
Epoch: 15: Step: 2901/4907, loss=0.000055, lr=0.000012
Epoch: 15: Step: 2901/4907, loss=0.000055, lr=0.000012
Train batch 3000
Avg. loss per last 100 batches: 0.041061
Train batch 3000
Avg. loss per last 100 batches: 0.041061
Epoch: 15: Step: 3001/4907, loss=0.000000, lr=0.000012
Epoch: 15: Step: 3001/4907, loss=0.000000, lr=0.000012
Train batch 3100
Avg. loss per last 100 batches: 0.048717
Train batch 3100
Avg. loss per last 100 batches: 0.048717
Epoch: 15: Step: 3101/4907, loss=0.000146, lr=0.000012
Epoch: 15: Step: 3101/4907, loss=0.000146, lr=0.000012
Train batch 3200
Avg. loss per last 100 batches: 0.040048
Train batch 3200
Avg. loss per last 100 batches: 0.040048
Epoch: 15: Step: 3201/4907, loss=0.000412, lr=0.000012
Epoch: 15: Step: 3201/4907, loss=0.000412, lr=0.000012
Train batch 3300
Avg. loss per last 100 batches: 0.055883
Train batch 3300
Avg. loss per last 100 batches: 0.055883
Epoch: 15: Step: 3301/4907, loss=0.072022, lr=0.000012
Epoch: 15: Step: 3301/4907, loss=0.072022, lr=0.000012
Train batch 3400
Avg. loss per last 100 batches: 0.029670
Train batch 3400
Avg. loss per last 100 batches: 0.029670
Epoch: 15: Step: 3401/4907, loss=0.000028, lr=0.000012
Epoch: 15: Step: 3401/4907, loss=0.000028, lr=0.000012
Train batch 3500
Avg. loss per last 100 batches: 0.063433
Train batch 3500
Avg. loss per last 100 batches: 0.063433
Epoch: 15: Step: 3501/4907, loss=0.056124, lr=0.000012
Epoch: 15: Step: 3501/4907, loss=0.056124, lr=0.000012
Train batch 3600
Avg. loss per last 100 batches: 0.064200
Train batch 3600
Avg. loss per last 100 batches: 0.064200
Epoch: 15: Step: 3601/4907, loss=0.000004, lr=0.000012
Epoch: 15: Step: 3601/4907, loss=0.000004, lr=0.000012
Train batch 3700
Avg. loss per last 100 batches: 0.025933
Train batch 3700
Avg. loss per last 100 batches: 0.025933
Epoch: 15: Step: 3701/4907, loss=0.000210, lr=0.000012
Epoch: 15: Step: 3701/4907, loss=0.000210, lr=0.000012
Train batch 3800
Avg. loss per last 100 batches: 0.062481
Train batch 3800
Avg. loss per last 100 batches: 0.062481
Epoch: 15: Step: 3801/4907, loss=0.008571, lr=0.000012
Epoch: 15: Step: 3801/4907, loss=0.008571, lr=0.000012
Train batch 3900
Avg. loss per last 100 batches: 0.029917
Train batch 3900
Avg. loss per last 100 batches: 0.029917
Epoch: 15: Step: 3901/4907, loss=0.167748, lr=0.000012
Epoch: 15: Step: 3901/4907, loss=0.167748, lr=0.000012
Train batch 4000
Avg. loss per last 100 batches: 0.024427
Train batch 4000
Avg. loss per last 100 batches: 0.024427
Epoch: 15: Step: 4001/4907, loss=0.000000, lr=0.000012
Epoch: 15: Step: 4001/4907, loss=0.000000, lr=0.000012
Train batch 4100
Avg. loss per last 100 batches: 0.064753
Train batch 4100
Avg. loss per last 100 batches: 0.064753
Epoch: 15: Step: 4101/4907, loss=0.000076, lr=0.000012
Epoch: 15: Step: 4101/4907, loss=0.000076, lr=0.000012
Train batch 4200
Avg. loss per last 100 batches: 0.029990
Train batch 4200
Avg. loss per last 100 batches: 0.029990
Epoch: 15: Step: 4201/4907, loss=0.001526, lr=0.000012
Epoch: 15: Step: 4201/4907, loss=0.001526, lr=0.000012
Train batch 4300
Avg. loss per last 100 batches: 0.070509
Train batch 4300
Avg. loss per last 100 batches: 0.070509
Epoch: 15: Step: 4301/4907, loss=0.000027, lr=0.000012
Epoch: 15: Step: 4301/4907, loss=0.000027, lr=0.000012
Train batch 4400
Avg. loss per last 100 batches: 0.029661
Train batch 4400
Avg. loss per last 100 batches: 0.029661
Epoch: 15: Step: 4401/4907, loss=0.000440, lr=0.000012
Epoch: 15: Step: 4401/4907, loss=0.000440, lr=0.000012
Train batch 4500
Avg. loss per last 100 batches: 0.044589
Train batch 4500
Avg. loss per last 100 batches: 0.044589
Epoch: 15: Step: 4501/4907, loss=0.000015, lr=0.000012
Epoch: 15: Step: 4501/4907, loss=0.000015, lr=0.000012
Train batch 4600
Avg. loss per last 100 batches: 0.068031
Train batch 4600
Avg. loss per last 100 batches: 0.068031
Epoch: 15: Step: 4601/4907, loss=0.021363, lr=0.000012
Epoch: 15: Step: 4601/4907, loss=0.021363, lr=0.000012
Train batch 4700
Avg. loss per last 100 batches: 0.053823
Train batch 4700
Avg. loss per last 100 batches: 0.053823
Epoch: 15: Step: 4701/4907, loss=0.326167, lr=0.000012
Epoch: 15: Step: 4701/4907, loss=0.326167, lr=0.000012
Train batch 4800
Avg. loss per last 100 batches: 0.041869
Train batch 4800
Avg. loss per last 100 batches: 0.041869
Epoch: 15: Step: 4801/4907, loss=0.215427, lr=0.000012
Epoch: 15: Step: 4801/4907, loss=0.215427, lr=0.000012
Train batch 4900
Avg. loss per last 100 batches: 0.063647
Train batch 4900
Avg. loss per last 100 batches: 0.063647
Epoch: 15: Step: 4901/4907, loss=0.000006, lr=0.000012
Epoch: 15: Step: 4901/4907, loss=0.000006, lr=0.000012
Validation: Epoch: 15 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 15 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 234.66129527317372, total questions=6516
Av.rank validation: average rank 234.66129527317372, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.15.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.15.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 234.66129527317372, total questions=6516
Av.rank validation: average rank 234.66129527317372, total questions=6516
Av Loss per epoch=0.047759
epoch total correct predictions=58068
***** Epoch 16 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.15.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.15.4907
Av Loss per epoch=0.047759
epoch total correct predictions=58068
***** Epoch 16 *****
Epoch: 16: Step: 1/4907, loss=0.402716, lr=0.000012
Epoch: 16: Step: 1/4907, loss=0.402716, lr=0.000012
Train batch 100
Avg. loss per last 100 batches: 0.052543
Train batch 100
Avg. loss per last 100 batches: 0.052543
Epoch: 16: Step: 101/4907, loss=0.690411, lr=0.000012
Epoch: 16: Step: 101/4907, loss=0.690411, lr=0.000012
Train batch 200
Avg. loss per last 100 batches: 0.039475
Train batch 200
Avg. loss per last 100 batches: 0.039475
Epoch: 16: Step: 201/4907, loss=0.000128, lr=0.000012
Epoch: 16: Step: 201/4907, loss=0.000128, lr=0.000012
Train batch 300
Avg. loss per last 100 batches: 0.072413
Train batch 300
Avg. loss per last 100 batches: 0.072413
Epoch: 16: Step: 301/4907, loss=0.015717, lr=0.000012
Epoch: 16: Step: 301/4907, loss=0.015717, lr=0.000012
Train batch 400
Avg. loss per last 100 batches: 0.041548
Train batch 400
Avg. loss per last 100 batches: 0.041548
Epoch: 16: Step: 401/4907, loss=0.008383, lr=0.000012
Epoch: 16: Step: 401/4907, loss=0.008383, lr=0.000012
Train batch 500
Avg. loss per last 100 batches: 0.060992
Train batch 500
Avg. loss per last 100 batches: 0.060992
Epoch: 16: Step: 501/4907, loss=0.000056, lr=0.000012
Epoch: 16: Step: 501/4907, loss=0.000056, lr=0.000012
Train batch 600
Avg. loss per last 100 batches: 0.055466
Train batch 600
Avg. loss per last 100 batches: 0.055466
Epoch: 16: Step: 601/4907, loss=0.000001, lr=0.000012
Epoch: 16: Step: 601/4907, loss=0.000001, lr=0.000012
Train batch 700
Avg. loss per last 100 batches: 0.038031
Train batch 700
Avg. loss per last 100 batches: 0.038031
Epoch: 16: Step: 701/4907, loss=0.000065, lr=0.000012
Epoch: 16: Step: 701/4907, loss=0.000065, lr=0.000012
Train batch 800
Avg. loss per last 100 batches: 0.029734
Train batch 800
Avg. loss per last 100 batches: 0.029734
Epoch: 16: Step: 801/4907, loss=0.000000, lr=0.000012
Epoch: 16: Step: 801/4907, loss=0.000000, lr=0.000012
Train batch 900
Avg. loss per last 100 batches: 0.023866
Train batch 900
Avg. loss per last 100 batches: 0.023866
Epoch: 16: Step: 901/4907, loss=0.000003, lr=0.000012
Epoch: 16: Step: 901/4907, loss=0.000003, lr=0.000012
Train batch 1000
Avg. loss per last 100 batches: 0.038845
Train batch 1000
Avg. loss per last 100 batches: 0.038845
Epoch: 16: Step: 1001/4907, loss=0.000008, lr=0.000012
Epoch: 16: Step: 1001/4907, loss=0.000008, lr=0.000012
Train batch 1100
Avg. loss per last 100 batches: 0.037261
Train batch 1100
Avg. loss per last 100 batches: 0.037261
Epoch: 16: Step: 1101/4907, loss=0.000016, lr=0.000012
Epoch: 16: Step: 1101/4907, loss=0.000016, lr=0.000012
Train batch 1200
Avg. loss per last 100 batches: 0.033454
Train batch 1200
Avg. loss per last 100 batches: 0.033454
Epoch: 16: Step: 1201/4907, loss=0.000001, lr=0.000012
Epoch: 16: Step: 1201/4907, loss=0.000001, lr=0.000012
Train batch 1300
Avg. loss per last 100 batches: 0.045037
Train batch 1300
Avg. loss per last 100 batches: 0.045037
Epoch: 16: Step: 1301/4907, loss=0.005373, lr=0.000012
Epoch: 16: Step: 1301/4907, loss=0.005373, lr=0.000012
Train batch 1400
Avg. loss per last 100 batches: 0.039112
Train batch 1400
Avg. loss per last 100 batches: 0.039112
Epoch: 16: Step: 1401/4907, loss=0.000169, lr=0.000012
Epoch: 16: Step: 1401/4907, loss=0.000169, lr=0.000012
Train batch 1500
Avg. loss per last 100 batches: 0.082859
Train batch 1500
Avg. loss per last 100 batches: 0.082859
Epoch: 16: Step: 1501/4907, loss=0.063610, lr=0.000012
Epoch: 16: Step: 1501/4907, loss=0.063610, lr=0.000012
Train batch 1600
Avg. loss per last 100 batches: 0.043652
Train batch 1600
Avg. loss per last 100 batches: 0.043652
Epoch: 16: Step: 1601/4907, loss=0.000361, lr=0.000012
Epoch: 16: Step: 1601/4907, loss=0.000361, lr=0.000012
Train batch 1700
Avg. loss per last 100 batches: 0.049040
Train batch 1700
Avg. loss per last 100 batches: 0.049040
Epoch: 16: Step: 1701/4907, loss=0.001410, lr=0.000012
Epoch: 16: Step: 1701/4907, loss=0.001410, lr=0.000012
Train batch 1800
Avg. loss per last 100 batches: 0.031068
Train batch 1800
Avg. loss per last 100 batches: 0.031068
Epoch: 16: Step: 1801/4907, loss=0.002387, lr=0.000012
Epoch: 16: Step: 1801/4907, loss=0.002387, lr=0.000012
Train batch 1900
Avg. loss per last 100 batches: 0.063934
Train batch 1900
Avg. loss per last 100 batches: 0.063934
Epoch: 16: Step: 1901/4907, loss=0.000002, lr=0.000012
Epoch: 16: Step: 1901/4907, loss=0.000002, lr=0.000012
Train batch 2000
Avg. loss per last 100 batches: 0.047098
Train batch 2000
Avg. loss per last 100 batches: 0.047098
Epoch: 16: Step: 2001/4907, loss=0.000007, lr=0.000012
Epoch: 16: Step: 2001/4907, loss=0.000007, lr=0.000012
Train batch 2100
Avg. loss per last 100 batches: 0.020137
Train batch 2100
Avg. loss per last 100 batches: 0.020137
Epoch: 16: Step: 2101/4907, loss=0.000000, lr=0.000012
Epoch: 16: Step: 2101/4907, loss=0.000000, lr=0.000012
Train batch 2200
Avg. loss per last 100 batches: 0.043602
Train batch 2200
Avg. loss per last 100 batches: 0.043602
Epoch: 16: Step: 2201/4907, loss=0.000013, lr=0.000012
Epoch: 16: Step: 2201/4907, loss=0.000013, lr=0.000012
Train batch 2300
Train batch 2300
Avg. loss per last 100 batches: 0.024082
Avg. loss per last 100 batches: 0.024082
Epoch: 16: Step: 2301/4907, loss=0.000010, lr=0.000012
Epoch: 16: Step: 2301/4907, loss=0.000010, lr=0.000012
Train batch 2400
Avg. loss per last 100 batches: 0.051917
Train batch 2400
Avg. loss per last 100 batches: 0.051917
Epoch: 16: Step: 2401/4907, loss=0.000002, lr=0.000012
Epoch: 16: Step: 2401/4907, loss=0.000002, lr=0.000012
Train batch 2500
Avg. loss per last 100 batches: 0.043326
Train batch 2500
Avg. loss per last 100 batches: 0.043326
Epoch: 16: Step: 2501/4907, loss=0.002193, lr=0.000012
Epoch: 16: Step: 2501/4907, loss=0.002193, lr=0.000012
Train batch 2600
Avg. loss per last 100 batches: 0.023378
Train batch 2600
Avg. loss per last 100 batches: 0.023378
Epoch: 16: Step: 2601/4907, loss=0.000216, lr=0.000012
Epoch: 16: Step: 2601/4907, loss=0.000216, lr=0.000012
Train batch 2700
Avg. loss per last 100 batches: 0.051452
Train batch 2700
Avg. loss per last 100 batches: 0.051452
Epoch: 16: Step: 2701/4907, loss=0.000002, lr=0.000012
Epoch: 16: Step: 2701/4907, loss=0.000002, lr=0.000012
Train batch 2800
Avg. loss per last 100 batches: 0.033674
Train batch 2800
Avg. loss per last 100 batches: 0.033674
Epoch: 16: Step: 2801/4907, loss=0.002339, lr=0.000012
Epoch: 16: Step: 2801/4907, loss=0.002339, lr=0.000012
Train batch 2900
Avg. loss per last 100 batches: 0.067200
Train batch 2900
Avg. loss per last 100 batches: 0.067200
Epoch: 16: Step: 2901/4907, loss=0.000000, lr=0.000012
Epoch: 16: Step: 2901/4907, loss=0.000000, lr=0.000012
Train batch 3000
Avg. loss per last 100 batches: 0.057078
Train batch 3000
Avg. loss per last 100 batches: 0.057078
Epoch: 16: Step: 3001/4907, loss=0.000000, lr=0.000012
Epoch: 16: Step: 3001/4907, loss=0.000000, lr=0.000012
Train batch 3100
Avg. loss per last 100 batches: 0.067796
Train batch 3100
Avg. loss per last 100 batches: 0.067796
Epoch: 16: Step: 3101/4907, loss=0.000177, lr=0.000012
Epoch: 16: Step: 3101/4907, loss=0.000177, lr=0.000012
Train batch 3200
Avg. loss per last 100 batches: 0.032139
Train batch 3200
Avg. loss per last 100 batches: 0.032139
Epoch: 16: Step: 3201/4907, loss=0.000015, lr=0.000012
Epoch: 16: Step: 3201/4907, loss=0.000015, lr=0.000012
Train batch 3300
Avg. loss per last 100 batches: 0.068177
Train batch 3300
Avg. loss per last 100 batches: 0.068177
Epoch: 16: Step: 3301/4907, loss=0.000239, lr=0.000012
Epoch: 16: Step: 3301/4907, loss=0.000239, lr=0.000012
Train batch 3400
Avg. loss per last 100 batches: 0.047133
Train batch 3400
Avg. loss per last 100 batches: 0.047133
Epoch: 16: Step: 3401/4907, loss=0.004150, lr=0.000012
Epoch: 16: Step: 3401/4907, loss=0.004150, lr=0.000012
Train batch 3500
Avg. loss per last 100 batches: 0.063218
Train batch 3500
Avg. loss per last 100 batches: 0.063218
Epoch: 16: Step: 3501/4907, loss=0.000065, lr=0.000012
Epoch: 16: Step: 3501/4907, loss=0.000065, lr=0.000012
Train batch 3600
Avg. loss per last 100 batches: 0.029876
Train batch 3600
Avg. loss per last 100 batches: 0.029876
Epoch: 16: Step: 3601/4907, loss=0.000045, lr=0.000012
Epoch: 16: Step: 3601/4907, loss=0.000045, lr=0.000012
Train batch 3700
Avg. loss per last 100 batches: 0.025435
Train batch 3700
Avg. loss per last 100 batches: 0.025435
Epoch: 16: Step: 3701/4907, loss=0.001318, lr=0.000012
Epoch: 16: Step: 3701/4907, loss=0.001318, lr=0.000012
Train batch 3800
Avg. loss per last 100 batches: 0.039335
Train batch 3800
Avg. loss per last 100 batches: 0.039335
Epoch: 16: Step: 3801/4907, loss=0.000115, lr=0.000012
Epoch: 16: Step: 3801/4907, loss=0.000115, lr=0.000012
Train batch 3900
Avg. loss per last 100 batches: 0.050532
Train batch 3900
Avg. loss per last 100 batches: 0.050532
Epoch: 16: Step: 3901/4907, loss=0.000000, lr=0.000012
Epoch: 16: Step: 3901/4907, loss=0.000000, lr=0.000012
Train batch 4000
Avg. loss per last 100 batches: 0.090190
Train batch 4000
Avg. loss per last 100 batches: 0.090190
Epoch: 16: Step: 4001/4907, loss=0.000048, lr=0.000012
Epoch: 16: Step: 4001/4907, loss=0.000048, lr=0.000012
Train batch 4100
Avg. loss per last 100 batches: 0.049724
Train batch 4100
Avg. loss per last 100 batches: 0.049724
Epoch: 16: Step: 4101/4907, loss=0.000001, lr=0.000012
Epoch: 16: Step: 4101/4907, loss=0.000001, lr=0.000012
Train batch 4200
Avg. loss per last 100 batches: 0.053336
Train batch 4200
Avg. loss per last 100 batches: 0.053336
Epoch: 16: Step: 4201/4907, loss=0.077684, lr=0.000012
Epoch: 16: Step: 4201/4907, loss=0.077684, lr=0.000012
Train batch 4300
Avg. loss per last 100 batches: 0.042219
Train batch 4300
Avg. loss per last 100 batches: 0.042219
Epoch: 16: Step: 4301/4907, loss=0.000004, lr=0.000012
Epoch: 16: Step: 4301/4907, loss=0.000004, lr=0.000012
Train batch 4400
Avg. loss per last 100 batches: 0.049760
Train batch 4400
Avg. loss per last 100 batches: 0.049760
Epoch: 16: Step: 4401/4907, loss=0.000748, lr=0.000012
Epoch: 16: Step: 4401/4907, loss=0.000748, lr=0.000012
Train batch 4500
Train batch 4500
Avg. loss per last 100 batches: 0.036968
Avg. loss per last 100 batches: 0.036968
Epoch: 16: Step: 4501/4907, loss=0.026676, lr=0.000012
Epoch: 16: Step: 4501/4907, loss=0.026676, lr=0.000012
Train batch 4600
Avg. loss per last 100 batches: 0.086273
Train batch 4600
Avg. loss per last 100 batches: 0.086273
Epoch: 16: Step: 4601/4907, loss=0.009035, lr=0.000012
Epoch: 16: Step: 4601/4907, loss=0.009035, lr=0.000012
Train batch 4700
Avg. loss per last 100 batches: 0.056884
Train batch 4700
Avg. loss per last 100 batches: 0.056884
Epoch: 16: Step: 4701/4907, loss=0.000941, lr=0.000012
Epoch: 16: Step: 4701/4907, loss=0.000941, lr=0.000012
Train batch 4800
Avg. loss per last 100 batches: 0.067186
Train batch 4800
Avg. loss per last 100 batches: 0.067186
Epoch: 16: Step: 4801/4907, loss=0.193578, lr=0.000012
Epoch: 16: Step: 4801/4907, loss=0.193578, lr=0.000012
Train batch 4900
Avg. loss per last 100 batches: 0.050624
Train batch 4900
Avg. loss per last 100 batches: 0.050624
Epoch: 16: Step: 4901/4907, loss=0.005046, lr=0.000012
Epoch: 16: Step: 4901/4907, loss=0.005046, lr=0.000012
Validation: Epoch: 16 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 16 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 174.64947820748927, total questions=6516
Av.rank validation: average rank 174.64947820748927, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.16.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.16.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.16.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 174.64947820748927, total questions=6516
Av.rank validation: average rank 174.64947820748927, total questions=6516
Av Loss per epoch=0.047955
epoch total correct predictions=58072
***** Epoch 17 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.16.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.16.4907
Av Loss per epoch=0.047955
epoch total correct predictions=58072
***** Epoch 17 *****
Epoch: 17: Step: 1/4907, loss=0.000051, lr=0.000012
Epoch: 17: Step: 1/4907, loss=0.000051, lr=0.000012
Train batch 100
Avg. loss per last 100 batches: 0.045777
Train batch 100
Avg. loss per last 100 batches: 0.045777
Epoch: 17: Step: 101/4907, loss=0.000437, lr=0.000012
Epoch: 17: Step: 101/4907, loss=0.000437, lr=0.000012
Train batch 200
Avg. loss per last 100 batches: 0.037498
Train batch 200
Avg. loss per last 100 batches: 0.037498
Epoch: 17: Step: 201/4907, loss=0.000002, lr=0.000012
Epoch: 17: Step: 201/4907, loss=0.000002, lr=0.000012
Train batch 300
Avg. loss per last 100 batches: 0.035825
Train batch 300
Avg. loss per last 100 batches: 0.035825
Epoch: 17: Step: 301/4907, loss=0.000002, lr=0.000012
Epoch: 17: Step: 301/4907, loss=0.000002, lr=0.000012
Train batch 400
Avg. loss per last 100 batches: 0.098322
Train batch 400
Avg. loss per last 100 batches: 0.098322
Epoch: 17: Step: 401/4907, loss=0.000069, lr=0.000012
Epoch: 17: Step: 401/4907, loss=0.000069, lr=0.000012
Train batch 500
Avg. loss per last 100 batches: 0.052263
Train batch 500
Avg. loss per last 100 batches: 0.052263
Epoch: 17: Step: 501/4907, loss=0.000071, lr=0.000012
Epoch: 17: Step: 501/4907, loss=0.000071, lr=0.000012
Train batch 600
Avg. loss per last 100 batches: 0.029787
Train batch 600
Avg. loss per last 100 batches: 0.029787
Epoch: 17: Step: 601/4907, loss=0.000010, lr=0.000012
Epoch: 17: Step: 601/4907, loss=0.000010, lr=0.000012
Train batch 700
Avg. loss per last 100 batches: 0.041133
Train batch 700
Avg. loss per last 100 batches: 0.041133
Epoch: 17: Step: 701/4907, loss=0.000248, lr=0.000012
Epoch: 17: Step: 701/4907, loss=0.000248, lr=0.000012
Train batch 800
Avg. loss per last 100 batches: 0.048203
Train batch 800
Avg. loss per last 100 batches: 0.048203
Epoch: 17: Step: 801/4907, loss=0.000271, lr=0.000011
Epoch: 17: Step: 801/4907, loss=0.000271, lr=0.000011
Train batch 900
Avg. loss per last 100 batches: 0.025551
Train batch 900
Avg. loss per last 100 batches: 0.025551
Epoch: 17: Step: 901/4907, loss=0.001531, lr=0.000011
Epoch: 17: Step: 901/4907, loss=0.001531, lr=0.000011
Train batch 1000
Avg. loss per last 100 batches: 0.042641
Train batch 1000
Avg. loss per last 100 batches: 0.042641
Epoch: 17: Step: 1001/4907, loss=0.000052, lr=0.000011
Epoch: 17: Step: 1001/4907, loss=0.000052, lr=0.000011
Train batch 1100
Avg. loss per last 100 batches: 0.043336
Train batch 1100
Avg. loss per last 100 batches: 0.043336
Epoch: 17: Step: 1101/4907, loss=0.011681, lr=0.000011
Epoch: 17: Step: 1101/4907, loss=0.011681, lr=0.000011
Train batch 1200
Avg. loss per last 100 batches: 0.030000
Train batch 1200
Avg. loss per last 100 batches: 0.030000
Epoch: 17: Step: 1201/4907, loss=0.000158, lr=0.000011
Epoch: 17: Step: 1201/4907, loss=0.000158, lr=0.000011
Train batch 1300
Avg. loss per last 100 batches: 0.031929
Train batch 1300
Avg. loss per last 100 batches: 0.031929
Epoch: 17: Step: 1301/4907, loss=0.014126, lr=0.000011
Epoch: 17: Step: 1301/4907, loss=0.014126, lr=0.000011
Train batch 1400
Avg. loss per last 100 batches: 0.045514
Train batch 1400
Avg. loss per last 100 batches: 0.045514
Epoch: 17: Step: 1401/4907, loss=0.000063, lr=0.000011
Epoch: 17: Step: 1401/4907, loss=0.000063, lr=0.000011
Train batch 1500
Train batch 1500
Avg. loss per last 100 batches: 0.058146
Avg. loss per last 100 batches: 0.058146
Epoch: 17: Step: 1501/4907, loss=0.000137, lr=0.000011
Epoch: 17: Step: 1501/4907, loss=0.000137, lr=0.000011
Train batch 1600
Avg. loss per last 100 batches: 0.043350
Train batch 1600
Avg. loss per last 100 batches: 0.043350
Epoch: 17: Step: 1601/4907, loss=0.002341, lr=0.000011
Epoch: 17: Step: 1601/4907, loss=0.002341, lr=0.000011
Train batch 1700
Avg. loss per last 100 batches: 0.041420
Train batch 1700
Avg. loss per last 100 batches: 0.041420
Epoch: 17: Step: 1701/4907, loss=0.000000, lr=0.000011
Epoch: 17: Step: 1701/4907, loss=0.000000, lr=0.000011
Train batch 1800
Avg. loss per last 100 batches: 0.042063
Train batch 1800
Avg. loss per last 100 batches: 0.042063
Epoch: 17: Step: 1801/4907, loss=0.054090, lr=0.000011
Epoch: 17: Step: 1801/4907, loss=0.054090, lr=0.000011
Train batch 1900
Avg. loss per last 100 batches: 0.064913
Train batch 1900
Avg. loss per last 100 batches: 0.064913
Epoch: 17: Step: 1901/4907, loss=0.337238, lr=0.000011
Epoch: 17: Step: 1901/4907, loss=0.337238, lr=0.000011
Train batch 2000
Avg. loss per last 100 batches: 0.052599
Train batch 2000
Avg. loss per last 100 batches: 0.052599
Epoch: 17: Step: 2001/4907, loss=0.002019, lr=0.000011
Epoch: 17: Step: 2001/4907, loss=0.002019, lr=0.000011
Train batch 2100
Avg. loss per last 100 batches: 0.041517
Train batch 2100
Avg. loss per last 100 batches: 0.041517
Epoch: 17: Step: 2101/4907, loss=0.000000, lr=0.000011
Epoch: 17: Step: 2101/4907, loss=0.000000, lr=0.000011
Train batch 2200
Avg. loss per last 100 batches: 0.052522
Train batch 2200
Avg. loss per last 100 batches: 0.052522
Epoch: 17: Step: 2201/4907, loss=0.000045, lr=0.000011
Epoch: 17: Step: 2201/4907, loss=0.000045, lr=0.000011
Train batch 2300
Avg. loss per last 100 batches: 0.045543
Train batch 2300
Avg. loss per last 100 batches: 0.045543
Epoch: 17: Step: 2301/4907, loss=0.011984, lr=0.000011
Epoch: 17: Step: 2301/4907, loss=0.011984, lr=0.000011
Train batch 2400
Avg. loss per last 100 batches: 0.028858
Train batch 2400
Avg. loss per last 100 batches: 0.028858
Epoch: 17: Step: 2401/4907, loss=0.006680, lr=0.000011
Epoch: 17: Step: 2401/4907, loss=0.006680, lr=0.000011
Train batch 2500
Avg. loss per last 100 batches: 0.032031
Train batch 2500
Avg. loss per last 100 batches: 0.032031
Epoch: 17: Step: 2501/4907, loss=0.000005, lr=0.000011
Epoch: 17: Step: 2501/4907, loss=0.000005, lr=0.000011
Train batch 2600
Avg. loss per last 100 batches: 0.036745
Train batch 2600
Avg. loss per last 100 batches: 0.036745
Epoch: 17: Step: 2601/4907, loss=0.000686, lr=0.000011
Epoch: 17: Step: 2601/4907, loss=0.000686, lr=0.000011
Train batch 2700
Avg. loss per last 100 batches: 0.062335
Train batch 2700
Avg. loss per last 100 batches: 0.062335
Epoch: 17: Step: 2701/4907, loss=0.000006, lr=0.000011
Epoch: 17: Step: 2701/4907, loss=0.000006, lr=0.000011
Train batch 2800
Avg. loss per last 100 batches: 0.042777
Train batch 2800
Avg. loss per last 100 batches: 0.042777
Epoch: 17: Step: 2801/4907, loss=0.000210, lr=0.000011
Epoch: 17: Step: 2801/4907, loss=0.000210, lr=0.000011
Train batch 2900
Avg. loss per last 100 batches: 0.027030
Train batch 2900
Avg. loss per last 100 batches: 0.027030
Epoch: 17: Step: 2901/4907, loss=0.178871, lr=0.000011
Epoch: 17: Step: 2901/4907, loss=0.178871, lr=0.000011
Train batch 3000
Avg. loss per last 100 batches: 0.036630
Train batch 3000
Avg. loss per last 100 batches: 0.036630
Epoch: 17: Step: 3001/4907, loss=0.000064, lr=0.000011
Epoch: 17: Step: 3001/4907, loss=0.000064, lr=0.000011
Train batch 3100
Avg. loss per last 100 batches: 0.032301
Train batch 3100
Avg. loss per last 100 batches: 0.032301
Epoch: 17: Step: 3101/4907, loss=0.000009, lr=0.000011
Epoch: 17: Step: 3101/4907, loss=0.000009, lr=0.000011
Train batch 3200
Avg. loss per last 100 batches: 0.037057
Train batch 3200
Avg. loss per last 100 batches: 0.037057
Epoch: 17: Step: 3201/4907, loss=0.083944, lr=0.000011
Epoch: 17: Step: 3201/4907, loss=0.083944, lr=0.000011
Train batch 3300
Avg. loss per last 100 batches: 0.041455
Train batch 3300
Avg. loss per last 100 batches: 0.041455
Epoch: 17: Step: 3301/4907, loss=0.000001, lr=0.000011
Epoch: 17: Step: 3301/4907, loss=0.000001, lr=0.000011
Train batch 3400
Avg. loss per last 100 batches: 0.034018
Train batch 3400
Avg. loss per last 100 batches: 0.034018
Epoch: 17: Step: 3401/4907, loss=0.003703, lr=0.000011
Epoch: 17: Step: 3401/4907, loss=0.003703, lr=0.000011
Train batch 3500
Avg. loss per last 100 batches: 0.039029
Train batch 3500
Avg. loss per last 100 batches: 0.039029
Epoch: 17: Step: 3501/4907, loss=0.000000, lr=0.000011
Epoch: 17: Step: 3501/4907, loss=0.000000, lr=0.000011
Train batch 3600
Avg. loss per last 100 batches: 0.020828
Train batch 3600
Avg. loss per last 100 batches: 0.020828
Epoch: 17: Step: 3601/4907, loss=0.000001, lr=0.000011
Epoch: 17: Step: 3601/4907, loss=0.000001, lr=0.000011
Train batch 3700
Avg. loss per last 100 batches: 0.075805
Train batch 3700
Avg. loss per last 100 batches: 0.075805
Epoch: 17: Step: 3701/4907, loss=0.148027, lr=0.000011
Epoch: 17: Step: 3701/4907, loss=0.148027, lr=0.000011
Train batch 3800
Avg. loss per last 100 batches: 0.024007
Train batch 3800
Avg. loss per last 100 batches: 0.024007
Epoch: 17: Step: 3801/4907, loss=0.163110, lr=0.000011
Epoch: 17: Step: 3801/4907, loss=0.163110, lr=0.000011
Train batch 3900
Avg. loss per last 100 batches: 0.036165
Train batch 3900
Avg. loss per last 100 batches: 0.036165
Epoch: 17: Step: 3901/4907, loss=0.000467, lr=0.000011
Epoch: 17: Step: 3901/4907, loss=0.000467, lr=0.000011
Train batch 4000
Avg. loss per last 100 batches: 0.070718
Train batch 4000
Avg. loss per last 100 batches: 0.070718
Epoch: 17: Step: 4001/4907, loss=0.000360, lr=0.000011
Epoch: 17: Step: 4001/4907, loss=0.000360, lr=0.000011
Train batch 4100
Avg. loss per last 100 batches: 0.022741
Train batch 4100
Avg. loss per last 100 batches: 0.022741
Epoch: 17: Step: 4101/4907, loss=0.046215, lr=0.000011
Epoch: 17: Step: 4101/4907, loss=0.046215, lr=0.000011
Train batch 4200
Avg. loss per last 100 batches: 0.048039
Train batch 4200
Avg. loss per last 100 batches: 0.048039
Epoch: 17: Step: 4201/4907, loss=0.002476, lr=0.000011
Epoch: 17: Step: 4201/4907, loss=0.002476, lr=0.000011
Train batch 4300
Avg. loss per last 100 batches: 0.042492
Train batch 4300
Avg. loss per last 100 batches: 0.042492
Epoch: 17: Step: 4301/4907, loss=0.001250, lr=0.000011
Epoch: 17: Step: 4301/4907, loss=0.001250, lr=0.000011
Train batch 4400
Avg. loss per last 100 batches: 0.068021
Train batch 4400
Avg. loss per last 100 batches: 0.068021
Epoch: 17: Step: 4401/4907, loss=0.000033, lr=0.000011
Epoch: 17: Step: 4401/4907, loss=0.000033, lr=0.000011
Train batch 4500
Avg. loss per last 100 batches: 0.026624
Train batch 4500
Avg. loss per last 100 batches: 0.026624
Epoch: 17: Step: 4501/4907, loss=0.076808, lr=0.000011
Epoch: 17: Step: 4501/4907, loss=0.076808, lr=0.000011
Train batch 4600
Avg. loss per last 100 batches: 0.049414
Train batch 4600
Avg. loss per last 100 batches: 0.049414
Epoch: 17: Step: 4601/4907, loss=0.000002, lr=0.000011
Epoch: 17: Step: 4601/4907, loss=0.000002, lr=0.000011
Train batch 4700
Avg. loss per last 100 batches: 0.068897
Train batch 4700
Avg. loss per last 100 batches: 0.068897
Epoch: 17: Step: 4701/4907, loss=0.044801, lr=0.000011
Epoch: 17: Step: 4701/4907, loss=0.044801, lr=0.000011
Train batch 4800
Avg. loss per last 100 batches: 0.025147
Train batch 4800
Avg. loss per last 100 batches: 0.025147
Epoch: 17: Step: 4801/4907, loss=0.087414, lr=0.000011
Epoch: 17: Step: 4801/4907, loss=0.087414, lr=0.000011
Train batch 4900
Avg. loss per last 100 batches: 0.030879
Train batch 4900
Avg. loss per last 100 batches: 0.030879
Epoch: 17: Step: 4901/4907, loss=0.018618, lr=0.000011
Epoch: 17: Step: 4901/4907, loss=0.018618, lr=0.000011
Validation: Epoch: 17 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 17 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 255.39318600368324, total questions=6516
Av.rank validation: average rank 255.39318600368324, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.17.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.17.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 255.39318600368324, total questions=6516
Av.rank validation: average rank 255.39318600368324, total questions=6516
Av Loss per epoch=0.043057
epoch total correct predictions=58178
***** Epoch 18 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.17.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.17.4907
Av Loss per epoch=0.043057
epoch total correct predictions=58178
***** Epoch 18 *****
Epoch: 18: Step: 1/4907, loss=0.057029, lr=0.000011
Epoch: 18: Step: 1/4907, loss=0.057029, lr=0.000011
Train batch 100
Avg. loss per last 100 batches: 0.044138
Train batch 100
Avg. loss per last 100 batches: 0.044138
Epoch: 18: Step: 101/4907, loss=0.082884, lr=0.000011
Epoch: 18: Step: 101/4907, loss=0.082884, lr=0.000011
Train batch 200
Avg. loss per last 100 batches: 0.054125
Train batch 200
Avg. loss per last 100 batches: 0.054125
Epoch: 18: Step: 201/4907, loss=0.065374, lr=0.000011
Epoch: 18: Step: 201/4907, loss=0.065374, lr=0.000011
Train batch 300
Avg. loss per last 100 batches: 0.032964
Train batch 300
Avg. loss per last 100 batches: 0.032964
Epoch: 18: Step: 301/4907, loss=0.482648, lr=0.000011
Epoch: 18: Step: 301/4907, loss=0.482648, lr=0.000011
Train batch 400
Avg. loss per last 100 batches: 0.042900
Train batch 400
Avg. loss per last 100 batches: 0.042900
Epoch: 18: Step: 401/4907, loss=0.000398, lr=0.000011
Epoch: 18: Step: 401/4907, loss=0.000398, lr=0.000011
Train batch 500
Avg. loss per last 100 batches: 0.046206
Train batch 500
Avg. loss per last 100 batches: 0.046206
Epoch: 18: Step: 501/4907, loss=0.000160, lr=0.000011
Epoch: 18: Step: 501/4907, loss=0.000160, lr=0.000011
Train batch 600
Avg. loss per last 100 batches: 0.041340
Train batch 600
Avg. loss per last 100 batches: 0.041340
Epoch: 18: Step: 601/4907, loss=0.000786, lr=0.000011
Epoch: 18: Step: 601/4907, loss=0.000786, lr=0.000011
Train batch 700
Avg. loss per last 100 batches: 0.048765
Train batch 700
Avg. loss per last 100 batches: 0.048765
Epoch: 18: Step: 701/4907, loss=0.000000, lr=0.000011
Epoch: 18: Step: 701/4907, loss=0.000000, lr=0.000011
Train batch 800
Avg. loss per last 100 batches: 0.041288
Train batch 800
Avg. loss per last 100 batches: 0.041288
Epoch: 18: Step: 801/4907, loss=0.000078, lr=0.000011
Epoch: 18: Step: 801/4907, loss=0.000078, lr=0.000011
Train batch 900
Avg. loss per last 100 batches: 0.026242
Train batch 900
Avg. loss per last 100 batches: 0.026242
Epoch: 18: Step: 901/4907, loss=0.001674, lr=0.000011
Epoch: 18: Step: 901/4907, loss=0.001674, lr=0.000011
Train batch 1000
Avg. loss per last 100 batches: 0.048190
Train batch 1000
Avg. loss per last 100 batches: 0.048190
Epoch: 18: Step: 1001/4907, loss=0.000035, lr=0.000011
Epoch: 18: Step: 1001/4907, loss=0.000035, lr=0.000011
Train batch 1100
Avg. loss per last 100 batches: 0.037399
Train batch 1100
Avg. loss per last 100 batches: 0.037399
Epoch: 18: Step: 1101/4907, loss=0.000030, lr=0.000011
Epoch: 18: Step: 1101/4907, loss=0.000030, lr=0.000011
Train batch 1200
Train batch 1200
Avg. loss per last 100 batches: 0.060687
Avg. loss per last 100 batches: 0.060687
Epoch: 18: Step: 1201/4907, loss=0.000223, lr=0.000011
Epoch: 18: Step: 1201/4907, loss=0.000223, lr=0.000011
Train batch 1300
Avg. loss per last 100 batches: 0.072460
Train batch 1300
Avg. loss per last 100 batches: 0.072460
Epoch: 18: Step: 1301/4907, loss=0.096355, lr=0.000011
Epoch: 18: Step: 1301/4907, loss=0.096355, lr=0.000011
Train batch 1400
Avg. loss per last 100 batches: 0.040415
Train batch 1400
Avg. loss per last 100 batches: 0.040415
Epoch: 18: Step: 1401/4907, loss=0.001222, lr=0.000011
Epoch: 18: Step: 1401/4907, loss=0.001222, lr=0.000011
Train batch 1500
Avg. loss per last 100 batches: 0.034732
Train batch 1500
Avg. loss per last 100 batches: 0.034732
Epoch: 18: Step: 1501/4907, loss=0.000099, lr=0.000011
Epoch: 18: Step: 1501/4907, loss=0.000099, lr=0.000011
Train batch 1600
Avg. loss per last 100 batches: 0.041427
Train batch 1600
Avg. loss per last 100 batches: 0.041427
Epoch: 18: Step: 1601/4907, loss=0.000024, lr=0.000011
Epoch: 18: Step: 1601/4907, loss=0.000024, lr=0.000011
Train batch 1700
Avg. loss per last 100 batches: 0.037464
Train batch 1700
Avg. loss per last 100 batches: 0.037464
Epoch: 18: Step: 1701/4907, loss=0.000035, lr=0.000011
Epoch: 18: Step: 1701/4907, loss=0.000035, lr=0.000011
Train batch 1800
Avg. loss per last 100 batches: 0.024481
Train batch 1800
Avg. loss per last 100 batches: 0.024481
Epoch: 18: Step: 1801/4907, loss=0.011022, lr=0.000011
Epoch: 18: Step: 1801/4907, loss=0.011022, lr=0.000011
Train batch 1900
Avg. loss per last 100 batches: 0.043037
Train batch 1900
Avg. loss per last 100 batches: 0.043037
Epoch: 18: Step: 1901/4907, loss=0.000342, lr=0.000011
Epoch: 18: Step: 1901/4907, loss=0.000342, lr=0.000011
Train batch 2000
Avg. loss per last 100 batches: 0.058118
Train batch 2000
Avg. loss per last 100 batches: 0.058118
Epoch: 18: Step: 2001/4907, loss=0.073788, lr=0.000011
Epoch: 18: Step: 2001/4907, loss=0.073788, lr=0.000011
Train batch 2100
Avg. loss per last 100 batches: 0.052533
Train batch 2100
Avg. loss per last 100 batches: 0.052533
Epoch: 18: Step: 2101/4907, loss=0.000001, lr=0.000011
Epoch: 18: Step: 2101/4907, loss=0.000001, lr=0.000011
Train batch 2200
Avg. loss per last 100 batches: 0.032280
Train batch 2200
Avg. loss per last 100 batches: 0.032280
Epoch: 18: Step: 2201/4907, loss=0.003892, lr=0.000011
Epoch: 18: Step: 2201/4907, loss=0.003892, lr=0.000011
Train batch 2300
Avg. loss per last 100 batches: 0.029756
Train batch 2300
Avg. loss per last 100 batches: 0.029756
Epoch: 18: Step: 2301/4907, loss=0.003451, lr=0.000011
Epoch: 18: Step: 2301/4907, loss=0.003451, lr=0.000011
Train batch 2400
Avg. loss per last 100 batches: 0.039857
Train batch 2400
Avg. loss per last 100 batches: 0.039857
Epoch: 18: Step: 2401/4907, loss=0.000197, lr=0.000011
Epoch: 18: Step: 2401/4907, loss=0.000197, lr=0.000011
Train batch 2500
Avg. loss per last 100 batches: 0.028343
Train batch 2500
Avg. loss per last 100 batches: 0.028343
Epoch: 18: Step: 2501/4907, loss=0.024249, lr=0.000011
Epoch: 18: Step: 2501/4907, loss=0.024249, lr=0.000011
Train batch 2600
Avg. loss per last 100 batches: 0.044139
Train batch 2600
Avg. loss per last 100 batches: 0.044139
Epoch: 18: Step: 2601/4907, loss=0.000002, lr=0.000011
Epoch: 18: Step: 2601/4907, loss=0.000002, lr=0.000011
Train batch 2700
Avg. loss per last 100 batches: 0.040673
Train batch 2700
Avg. loss per last 100 batches: 0.040673
Epoch: 18: Step: 2701/4907, loss=0.000004, lr=0.000011
Epoch: 18: Step: 2701/4907, loss=0.000004, lr=0.000011
Train batch 2800
Avg. loss per last 100 batches: 0.047551
Train batch 2800
Avg. loss per last 100 batches: 0.047551
Epoch: 18: Step: 2801/4907, loss=0.000004, lr=0.000011
Epoch: 18: Step: 2801/4907, loss=0.000004, lr=0.000011
Train batch 2900
Avg. loss per last 100 batches: 0.029335
Train batch 2900
Avg. loss per last 100 batches: 0.029335
Epoch: 18: Step: 2901/4907, loss=0.077980, lr=0.000011
Epoch: 18: Step: 2901/4907, loss=0.077980, lr=0.000011
Train batch 3000
Avg. loss per last 100 batches: 0.034344
Train batch 3000
Avg. loss per last 100 batches: 0.034344
Epoch: 18: Step: 3001/4907, loss=0.000016, lr=0.000011
Epoch: 18: Step: 3001/4907, loss=0.000016, lr=0.000011
Train batch 3100
Avg. loss per last 100 batches: 0.051848
Train batch 3100
Avg. loss per last 100 batches: 0.051848
Epoch: 18: Step: 3101/4907, loss=0.001782, lr=0.000011
Epoch: 18: Step: 3101/4907, loss=0.001782, lr=0.000011
Train batch 3200
Avg. loss per last 100 batches: 0.044111
Train batch 3200
Avg. loss per last 100 batches: 0.044111
Epoch: 18: Step: 3201/4907, loss=0.003540, lr=0.000011
Epoch: 18: Step: 3201/4907, loss=0.003540, lr=0.000011
Train batch 3300
Avg. loss per last 100 batches: 0.054045
Train batch 3300
Avg. loss per last 100 batches: 0.054045
Epoch: 18: Step: 3301/4907, loss=0.046189, lr=0.000011
Epoch: 18: Step: 3301/4907, loss=0.046189, lr=0.000011
Train batch 3400
Avg. loss per last 100 batches: 0.052501
Train batch 3400
Avg. loss per last 100 batches: 0.052501
Epoch: 18: Step: 3401/4907, loss=0.000091, lr=0.000011
Epoch: 18: Step: 3401/4907, loss=0.000091, lr=0.000011
Train batch 3500
Avg. loss per last 100 batches: 0.033369
Train batch 3500
Avg. loss per last 100 batches: 0.033369
Epoch: 18: Step: 3501/4907, loss=0.294114, lr=0.000011
Epoch: 18: Step: 3501/4907, loss=0.294114, lr=0.000011
Train batch 3600
Avg. loss per last 100 batches: 0.052667
Train batch 3600
Avg. loss per last 100 batches: 0.052667
Epoch: 18: Step: 3601/4907, loss=0.798964, lr=0.000011
Epoch: 18: Step: 3601/4907, loss=0.798964, lr=0.000011
Train batch 3700
Avg. loss per last 100 batches: 0.047458
Train batch 3700
Avg. loss per last 100 batches: 0.047458
Epoch: 18: Step: 3701/4907, loss=0.003805, lr=0.000011
Epoch: 18: Step: 3701/4907, loss=0.003805, lr=0.000011
Train batch 3800
Avg. loss per last 100 batches: 0.042445
Train batch 3800
Avg. loss per last 100 batches: 0.042445
Epoch: 18: Step: 3801/4907, loss=0.000282, lr=0.000011
Epoch: 18: Step: 3801/4907, loss=0.000282, lr=0.000011
Train batch 3900
Avg. loss per last 100 batches: 0.045077
Train batch 3900
Avg. loss per last 100 batches: 0.045077
Epoch: 18: Step: 3901/4907, loss=0.000003, lr=0.000011
Epoch: 18: Step: 3901/4907, loss=0.000003, lr=0.000011
Train batch 4000
Avg. loss per last 100 batches: 0.025911
Train batch 4000
Avg. loss per last 100 batches: 0.025911
Epoch: 18: Step: 4001/4907, loss=0.098314, lr=0.000011
Epoch: 18: Step: 4001/4907, loss=0.098314, lr=0.000011
Train batch 4100
Avg. loss per last 100 batches: 0.026066
Train batch 4100
Avg. loss per last 100 batches: 0.026066
Epoch: 18: Step: 4101/4907, loss=0.001164, lr=0.000011
Epoch: 18: Step: 4101/4907, loss=0.001164, lr=0.000011
Train batch 4200
Train batch 4200
Avg. loss per last 100 batches: 0.028246
Avg. loss per last 100 batches: 0.028246
Epoch: 18: Step: 4201/4907, loss=0.555903, lr=0.000011
Epoch: 18: Step: 4201/4907, loss=0.555903, lr=0.000011
Train batch 4300
Avg. loss per last 100 batches: 0.052092
Train batch 4300
Avg. loss per last 100 batches: 0.052092
Epoch: 18: Step: 4301/4907, loss=0.000018, lr=0.000011
Epoch: 18: Step: 4301/4907, loss=0.000018, lr=0.000011
Train batch 4400
Avg. loss per last 100 batches: 0.031877
Train batch 4400
Avg. loss per last 100 batches: 0.031877
Epoch: 18: Step: 4401/4907, loss=0.000004, lr=0.000011
Epoch: 18: Step: 4401/4907, loss=0.000004, lr=0.000011
Train batch 4500
Avg. loss per last 100 batches: 0.016927
Train batch 4500
Avg. loss per last 100 batches: 0.016927
Epoch: 18: Step: 4501/4907, loss=0.000972, lr=0.000011
Epoch: 18: Step: 4501/4907, loss=0.000972, lr=0.000011
Train batch 4600
Avg. loss per last 100 batches: 0.044521
Train batch 4600
Avg. loss per last 100 batches: 0.044521
Epoch: 18: Step: 4601/4907, loss=0.004136, lr=0.000011
Epoch: 18: Step: 4601/4907, loss=0.004136, lr=0.000011
Train batch 4700
Avg. loss per last 100 batches: 0.049558
Train batch 4700
Avg. loss per last 100 batches: 0.049558
Epoch: 18: Step: 4701/4907, loss=0.000000, lr=0.000011
Epoch: 18: Step: 4701/4907, loss=0.000000, lr=0.000011
Train batch 4800
Avg. loss per last 100 batches: 0.021906
Train batch 4800
Avg. loss per last 100 batches: 0.021906
Epoch: 18: Step: 4801/4907, loss=0.000014, lr=0.000011
Epoch: 18: Step: 4801/4907, loss=0.000014, lr=0.000011
Train batch 4900
Avg. loss per last 100 batches: 0.060039
Train batch 4900
Avg. loss per last 100 batches: 0.060039
Epoch: 18: Step: 4901/4907, loss=0.114673, lr=0.000011
Epoch: 18: Step: 4901/4907, loss=0.114673, lr=0.000011
Validation: Epoch: 18 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 18 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 162.41175567833025, total questions=6516
Av.rank validation: average rank 162.41175567833025, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.18.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.18.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.18.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 162.41175567833025, total questions=6516
Av.rank validation: average rank 162.41175567833025, total questions=6516
Av Loss per epoch=0.041534
epoch total correct predictions=58185
***** Epoch 19 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.18.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.18.4907
Av Loss per epoch=0.041534
epoch total correct predictions=58185
***** Epoch 19 *****
Epoch: 19: Step: 1/4907, loss=0.000027, lr=0.000011
Epoch: 19: Step: 1/4907, loss=0.000027, lr=0.000011
Train batch 100
Avg. loss per last 100 batches: 0.046032
Train batch 100
Avg. loss per last 100 batches: 0.046032
Epoch: 19: Step: 101/4907, loss=0.017771, lr=0.000011
Epoch: 19: Step: 101/4907, loss=0.017771, lr=0.000011
Train batch 200
Avg. loss per last 100 batches: 0.038476
Train batch 200
Avg. loss per last 100 batches: 0.038476
Epoch: 19: Step: 201/4907, loss=0.000005, lr=0.000011
Epoch: 19: Step: 201/4907, loss=0.000005, lr=0.000011
Train batch 300
Avg. loss per last 100 batches: 0.036634
Train batch 300
Avg. loss per last 100 batches: 0.036634
Epoch: 19: Step: 301/4907, loss=0.000410, lr=0.000011
Epoch: 19: Step: 301/4907, loss=0.000410, lr=0.000011
Train batch 400
Avg. loss per last 100 batches: 0.056468
Train batch 400
Avg. loss per last 100 batches: 0.056468
Epoch: 19: Step: 401/4907, loss=0.066661, lr=0.000011
Epoch: 19: Step: 401/4907, loss=0.066661, lr=0.000011
Train batch 500
Avg. loss per last 100 batches: 0.030887
Train batch 500
Avg. loss per last 100 batches: 0.030887
Epoch: 19: Step: 501/4907, loss=0.001518, lr=0.000011
Epoch: 19: Step: 501/4907, loss=0.001518, lr=0.000011
Train batch 600
Avg. loss per last 100 batches: 0.038652
Train batch 600
Avg. loss per last 100 batches: 0.038652
Epoch: 19: Step: 601/4907, loss=0.000000, lr=0.000011
Epoch: 19: Step: 601/4907, loss=0.000000, lr=0.000011
Train batch 700
Avg. loss per last 100 batches: 0.067776
Train batch 700
Avg. loss per last 100 batches: 0.067776
Epoch: 19: Step: 701/4907, loss=0.000031, lr=0.000010
Epoch: 19: Step: 701/4907, loss=0.000031, lr=0.000010
Train batch 800
Avg. loss per last 100 batches: 0.040218
Train batch 800
Avg. loss per last 100 batches: 0.040218
Epoch: 19: Step: 801/4907, loss=0.000024, lr=0.000010
Epoch: 19: Step: 801/4907, loss=0.000024, lr=0.000010
Train batch 900
Avg. loss per last 100 batches: 0.026655
Train batch 900
Avg. loss per last 100 batches: 0.026655
Epoch: 19: Step: 901/4907, loss=0.000000, lr=0.000010
Epoch: 19: Step: 901/4907, loss=0.000000, lr=0.000010
Train batch 1000
Avg. loss per last 100 batches: 0.037682
Train batch 1000
Avg. loss per last 100 batches: 0.037682
Epoch: 19: Step: 1001/4907, loss=0.560050, lr=0.000010
Epoch: 19: Step: 1001/4907, loss=0.560050, lr=0.000010
Train batch 1100
Avg. loss per last 100 batches: 0.066114
Train batch 1100
Avg. loss per last 100 batches: 0.066114
Epoch: 19: Step: 1101/4907, loss=0.006356, lr=0.000010
Epoch: 19: Step: 1101/4907, loss=0.006356, lr=0.000010
Train batch 1200
Avg. loss per last 100 batches: 0.023634
Train batch 1200
Avg. loss per last 100 batches: 0.023634
Epoch: 19: Step: 1201/4907, loss=0.006532, lr=0.000010
Epoch: 19: Step: 1201/4907, loss=0.006532, lr=0.000010
Train batch 1300
Avg. loss per last 100 batches: 0.032891
Train batch 1300
Avg. loss per last 100 batches: 0.032891
Epoch: 19: Step: 1301/4907, loss=0.163029, lr=0.000010
Epoch: 19: Step: 1301/4907, loss=0.163029, lr=0.000010
Train batch 1400
Avg. loss per last 100 batches: 0.036074
Train batch 1400
Avg. loss per last 100 batches: 0.036074
Epoch: 19: Step: 1401/4907, loss=0.000000, lr=0.000010
Epoch: 19: Step: 1401/4907, loss=0.000000, lr=0.000010
Train batch 1500
Avg. loss per last 100 batches: 0.040133
Train batch 1500
Avg. loss per last 100 batches: 0.040133
Epoch: 19: Step: 1501/4907, loss=0.013157, lr=0.000010
Epoch: 19: Step: 1501/4907, loss=0.013157, lr=0.000010
Train batch 1600
Train batch 1600
Avg. loss per last 100 batches: 0.025112
Avg. loss per last 100 batches: 0.025112
Epoch: 19: Step: 1601/4907, loss=0.000060, lr=0.000010
Epoch: 19: Step: 1601/4907, loss=0.000060, lr=0.000010
Train batch 1700
Avg. loss per last 100 batches: 0.016378
Train batch 1700
Avg. loss per last 100 batches: 0.016378
Epoch: 19: Step: 1701/4907, loss=0.000006, lr=0.000010
Epoch: 19: Step: 1701/4907, loss=0.000006, lr=0.000010
Train batch 1800
Avg. loss per last 100 batches: 0.042495
Train batch 1800
Avg. loss per last 100 batches: 0.042495
Epoch: 19: Step: 1801/4907, loss=0.000000, lr=0.000010
Epoch: 19: Step: 1801/4907, loss=0.000000, lr=0.000010
Train batch 1900
Avg. loss per last 100 batches: 0.026955
Train batch 1900
Avg. loss per last 100 batches: 0.026955
Epoch: 19: Step: 1901/4907, loss=0.025672, lr=0.000010
Epoch: 19: Step: 1901/4907, loss=0.025672, lr=0.000010
Train batch 2000
Avg. loss per last 100 batches: 0.031110
Train batch 2000
Avg. loss per last 100 batches: 0.031110
Epoch: 19: Step: 2001/4907, loss=0.000134, lr=0.000010
Epoch: 19: Step: 2001/4907, loss=0.000134, lr=0.000010
Train batch 2100
Avg. loss per last 100 batches: 0.037477
Train batch 2100
Avg. loss per last 100 batches: 0.037477
Epoch: 19: Step: 2101/4907, loss=0.000024, lr=0.000010
Epoch: 19: Step: 2101/4907, loss=0.000024, lr=0.000010
Train batch 2200
Avg. loss per last 100 batches: 0.049103
Train batch 2200
Avg. loss per last 100 batches: 0.049103
Epoch: 19: Step: 2201/4907, loss=0.000001, lr=0.000010
Epoch: 19: Step: 2201/4907, loss=0.000001, lr=0.000010
Train batch 2300
Avg. loss per last 100 batches: 0.029805
Train batch 2300
Avg. loss per last 100 batches: 0.029805
Epoch: 19: Step: 2301/4907, loss=0.000001, lr=0.000010
Epoch: 19: Step: 2301/4907, loss=0.000001, lr=0.000010
Train batch 2400
Avg. loss per last 100 batches: 0.051950
Train batch 2400
Avg. loss per last 100 batches: 0.051950
Epoch: 19: Step: 2401/4907, loss=0.000865, lr=0.000010
Epoch: 19: Step: 2401/4907, loss=0.000865, lr=0.000010
Train batch 2500
Avg. loss per last 100 batches: 0.023786
Train batch 2500
Avg. loss per last 100 batches: 0.023786
Epoch: 19: Step: 2501/4907, loss=0.000000, lr=0.000010
Epoch: 19: Step: 2501/4907, loss=0.000000, lr=0.000010
Train batch 2600
Avg. loss per last 100 batches: 0.056356
Train batch 2600
Avg. loss per last 100 batches: 0.056356
Epoch: 19: Step: 2601/4907, loss=0.000072, lr=0.000010
Epoch: 19: Step: 2601/4907, loss=0.000072, lr=0.000010
Train batch 2700
Avg. loss per last 100 batches: 0.058685
Train batch 2700
Avg. loss per last 100 batches: 0.058685
Epoch: 19: Step: 2701/4907, loss=0.000001, lr=0.000010
Epoch: 19: Step: 2701/4907, loss=0.000001, lr=0.000010
Train batch 2800
Avg. loss per last 100 batches: 0.049188
Train batch 2800
Avg. loss per last 100 batches: 0.049188
Epoch: 19: Step: 2801/4907, loss=0.000009, lr=0.000010
Epoch: 19: Step: 2801/4907, loss=0.000009, lr=0.000010
Train batch 2900
Avg. loss per last 100 batches: 0.033873
Train batch 2900
Avg. loss per last 100 batches: 0.033873
Epoch: 19: Step: 2901/4907, loss=0.000876, lr=0.000010
Epoch: 19: Step: 2901/4907, loss=0.000876, lr=0.000010
Train batch 3000
Avg. loss per last 100 batches: 0.033785
Train batch 3000
Avg. loss per last 100 batches: 0.033785
Epoch: 19: Step: 3001/4907, loss=0.000006, lr=0.000010
Epoch: 19: Step: 3001/4907, loss=0.000006, lr=0.000010
Train batch 3100
Avg. loss per last 100 batches: 0.023897
Train batch 3100
Avg. loss per last 100 batches: 0.023897
Epoch: 19: Step: 3101/4907, loss=0.000000, lr=0.000010
Epoch: 19: Step: 3101/4907, loss=0.000000, lr=0.000010
Train batch 3200
Avg. loss per last 100 batches: 0.065379
Train batch 3200
Avg. loss per last 100 batches: 0.065379
Epoch: 19: Step: 3201/4907, loss=0.000895, lr=0.000010
Epoch: 19: Step: 3201/4907, loss=0.000895, lr=0.000010
Train batch 3300
Avg. loss per last 100 batches: 0.052722
Train batch 3300
Avg. loss per last 100 batches: 0.052722
Epoch: 19: Step: 3301/4907, loss=0.061719, lr=0.000010
Epoch: 19: Step: 3301/4907, loss=0.061719, lr=0.000010
Train batch 3400
Avg. loss per last 100 batches: 0.023509
Train batch 3400
Avg. loss per last 100 batches: 0.023509
Epoch: 19: Step: 3401/4907, loss=0.000007, lr=0.000010
Epoch: 19: Step: 3401/4907, loss=0.000007, lr=0.000010
Train batch 3500
Avg. loss per last 100 batches: 0.030258
Train batch 3500
Avg. loss per last 100 batches: 0.030258
Epoch: 19: Step: 3501/4907, loss=0.000015, lr=0.000010
Epoch: 19: Step: 3501/4907, loss=0.000015, lr=0.000010
Train batch 3600
Avg. loss per last 100 batches: 0.033840
Train batch 3600
Avg. loss per last 100 batches: 0.033840
Epoch: 19: Step: 3601/4907, loss=0.000015, lr=0.000010
Epoch: 19: Step: 3601/4907, loss=0.000015, lr=0.000010
Train batch 3700
Avg. loss per last 100 batches: 0.046963
Train batch 3700
Avg. loss per last 100 batches: 0.046963
Epoch: 19: Step: 3701/4907, loss=0.002496, lr=0.000010
Epoch: 19: Step: 3701/4907, loss=0.002496, lr=0.000010
Train batch 3800
Avg. loss per last 100 batches: 0.045516
Train batch 3800
Avg. loss per last 100 batches: 0.045516
Epoch: 19: Step: 3801/4907, loss=0.011925, lr=0.000010
Epoch: 19: Step: 3801/4907, loss=0.011925, lr=0.000010
Train batch 3900
Avg. loss per last 100 batches: 0.019670
Train batch 3900
Avg. loss per last 100 batches: 0.019670
Epoch: 19: Step: 3901/4907, loss=0.000000, lr=0.000010
Epoch: 19: Step: 3901/4907, loss=0.000000, lr=0.000010
Train batch 4000
Avg. loss per last 100 batches: 0.034523
Train batch 4000
Avg. loss per last 100 batches: 0.034523
Epoch: 19: Step: 4001/4907, loss=0.000020, lr=0.000010
Epoch: 19: Step: 4001/4907, loss=0.000020, lr=0.000010
Train batch 4100
Avg. loss per last 100 batches: 0.025699
Train batch 4100
Avg. loss per last 100 batches: 0.025699
Epoch: 19: Step: 4101/4907, loss=0.002168, lr=0.000010
Epoch: 19: Step: 4101/4907, loss=0.002168, lr=0.000010
Train batch 4200
Avg. loss per last 100 batches: 0.046165
Train batch 4200
Avg. loss per last 100 batches: 0.046165
Epoch: 19: Step: 4201/4907, loss=0.000001, lr=0.000010
Epoch: 19: Step: 4201/4907, loss=0.000001, lr=0.000010
Train batch 4300
Avg. loss per last 100 batches: 0.048846
Train batch 4300
Avg. loss per last 100 batches: 0.048846
Epoch: 19: Step: 4301/4907, loss=0.000008, lr=0.000010
Epoch: 19: Step: 4301/4907, loss=0.000008, lr=0.000010
Train batch 4400
Avg. loss per last 100 batches: 0.041603
Train batch 4400
Avg. loss per last 100 batches: 0.041603
Epoch: 19: Step: 4401/4907, loss=0.000011, lr=0.000010
Epoch: 19: Step: 4401/4907, loss=0.000011, lr=0.000010
Train batch 4500
Avg. loss per last 100 batches: 0.037637
Train batch 4500
Avg. loss per last 100 batches: 0.037637
Epoch: 19: Step: 4501/4907, loss=0.013220, lr=0.000010
Epoch: 19: Step: 4501/4907, loss=0.013220, lr=0.000010
Train batch 4600
Avg. loss per last 100 batches: 0.083711
Train batch 4600
Avg. loss per last 100 batches: 0.083711
Epoch: 19: Step: 4601/4907, loss=0.178978, lr=0.000010
Epoch: 19: Step: 4601/4907, loss=0.178978, lr=0.000010
Train batch 4700
Avg. loss per last 100 batches: 0.044215
Train batch 4700
Avg. loss per last 100 batches: 0.044215
Epoch: 19: Step: 4701/4907, loss=0.000059, lr=0.000010
Epoch: 19: Step: 4701/4907, loss=0.000059, lr=0.000010
Train batch 4800
Avg. loss per last 100 batches: 0.042985
Train batch 4800
Avg. loss per last 100 batches: 0.042985
Epoch: 19: Step: 4801/4907, loss=0.000975, lr=0.000010
Epoch: 19: Step: 4801/4907, loss=0.000975, lr=0.000010
Train batch 4900
Avg. loss per last 100 batches: 0.038803
Train batch 4900
Avg. loss per last 100 batches: 0.038803
Epoch: 19: Step: 4901/4907, loss=0.039386, lr=0.000010
Epoch: 19: Step: 4901/4907, loss=0.039386, lr=0.000010
Validation: Epoch: 19 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 19 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 200.14103744628608, total questions=6516
Av.rank validation: average rank 200.14103744628608, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.19.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.19.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 200.14103744628608, total questions=6516
Av.rank validation: average rank 200.14103744628608, total questions=6516
Av Loss per epoch=0.040555
epoch total correct predictions=58227
***** Epoch 20 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.19.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.19.4907
Av Loss per epoch=0.040555
epoch total correct predictions=58227
***** Epoch 20 *****
Epoch: 20: Step: 1/4907, loss=0.000213, lr=0.000010
Epoch: 20: Step: 1/4907, loss=0.000213, lr=0.000010
Train batch 100
Avg. loss per last 100 batches: 0.019726
Train batch 100
Avg. loss per last 100 batches: 0.019726
Epoch: 20: Step: 101/4907, loss=0.000001, lr=0.000010
Epoch: 20: Step: 101/4907, loss=0.000001, lr=0.000010
Train batch 200
Avg. loss per last 100 batches: 0.047188
Train batch 200
Avg. loss per last 100 batches: 0.047188
Epoch: 20: Step: 201/4907, loss=0.000057, lr=0.000010
Epoch: 20: Step: 201/4907, loss=0.000057, lr=0.000010
Train batch 300
Avg. loss per last 100 batches: 0.046494
Train batch 300
Avg. loss per last 100 batches: 0.046494
Epoch: 20: Step: 301/4907, loss=0.039936, lr=0.000010
Epoch: 20: Step: 301/4907, loss=0.039936, lr=0.000010
Train batch 400
Avg. loss per last 100 batches: 0.031657
Train batch 400
Avg. loss per last 100 batches: 0.031657
Epoch: 20: Step: 401/4907, loss=0.001789, lr=0.000010
Epoch: 20: Step: 401/4907, loss=0.001789, lr=0.000010
Train batch 500
Avg. loss per last 100 batches: 0.073934
Train batch 500
Avg. loss per last 100 batches: 0.073934
Epoch: 20: Step: 501/4907, loss=0.000014, lr=0.000010
Epoch: 20: Step: 501/4907, loss=0.000014, lr=0.000010
Train batch 600
Avg. loss per last 100 batches: 0.023623
Train batch 600
Avg. loss per last 100 batches: 0.023623
Epoch: 20: Step: 601/4907, loss=0.000001, lr=0.000010
Epoch: 20: Step: 601/4907, loss=0.000001, lr=0.000010
Train batch 700
Avg. loss per last 100 batches: 0.019244
Train batch 700
Avg. loss per last 100 batches: 0.019244
Epoch: 20: Step: 701/4907, loss=0.000001, lr=0.000010
Epoch: 20: Step: 701/4907, loss=0.000001, lr=0.000010
Train batch 800
Avg. loss per last 100 batches: 0.034109
Train batch 800
Avg. loss per last 100 batches: 0.034109
Epoch: 20: Step: 801/4907, loss=0.000144, lr=0.000010
Epoch: 20: Step: 801/4907, loss=0.000144, lr=0.000010
Train batch 900
Avg. loss per last 100 batches: 0.065615
Train batch 900
Avg. loss per last 100 batches: 0.065615
Epoch: 20: Step: 901/4907, loss=0.000414, lr=0.000010
Epoch: 20: Step: 901/4907, loss=0.000414, lr=0.000010
Train batch 1000
Avg. loss per last 100 batches: 0.029733
Train batch 1000
Avg. loss per last 100 batches: 0.029733
Epoch: 20: Step: 1001/4907, loss=0.006458, lr=0.000010
Epoch: 20: Step: 1001/4907, loss=0.006458, lr=0.000010
Train batch 1100
Avg. loss per last 100 batches: 0.045409
Train batch 1100
Avg. loss per last 100 batches: 0.045409
Epoch: 20: Step: 1101/4907, loss=0.000049, lr=0.000010
Epoch: 20: Step: 1101/4907, loss=0.000049, lr=0.000010
Train batch 1200
Avg. loss per last 100 batches: 0.053904
Train batch 1200
Avg. loss per last 100 batches: 0.053904
Epoch: 20: Step: 1201/4907, loss=0.010243, lr=0.000010
Epoch: 20: Step: 1201/4907, loss=0.010243, lr=0.000010
Train batch 1300
Avg. loss per last 100 batches: 0.050601
Train batch 1300
Avg. loss per last 100 batches: 0.050601
Epoch: 20: Step: 1301/4907, loss=0.000841, lr=0.000010
Epoch: 20: Step: 1301/4907, loss=0.000841, lr=0.000010
Train batch 1400
Avg. loss per last 100 batches: 0.028675
Train batch 1400
Avg. loss per last 100 batches: 0.028675
Epoch: 20: Step: 1401/4907, loss=0.003128, lr=0.000010
Epoch: 20: Step: 1401/4907, loss=0.003128, lr=0.000010
Train batch 1500
Avg. loss per last 100 batches: 0.034085
Train batch 1500
Avg. loss per last 100 batches: 0.034085
Epoch: 20: Step: 1501/4907, loss=0.000107, lr=0.000010
Epoch: 20: Step: 1501/4907, loss=0.000107, lr=0.000010
Train batch 1600
Avg. loss per last 100 batches: 0.024701
Train batch 1600
Avg. loss per last 100 batches: 0.024701
Epoch: 20: Step: 1601/4907, loss=0.000098, lr=0.000010
Epoch: 20: Step: 1601/4907, loss=0.000098, lr=0.000010
Train batch 1700
Avg. loss per last 100 batches: 0.032430
Train batch 1700
Avg. loss per last 100 batches: 0.032430
Epoch: 20: Step: 1701/4907, loss=0.056784, lr=0.000010
Epoch: 20: Step: 1701/4907, loss=0.056784, lr=0.000010
Train batch 1800
Avg. loss per last 100 batches: 0.035063
Train batch 1800
Avg. loss per last 100 batches: 0.035063
Epoch: 20: Step: 1801/4907, loss=0.000678, lr=0.000010
Epoch: 20: Step: 1801/4907, loss=0.000678, lr=0.000010
Train batch 1900
Avg. loss per last 100 batches: 0.025937
Train batch 1900
Avg. loss per last 100 batches: 0.025937
Epoch: 20: Step: 1901/4907, loss=0.000182, lr=0.000010
Epoch: 20: Step: 1901/4907, loss=0.000182, lr=0.000010
Train batch 2000
Avg. loss per last 100 batches: 0.059296
Train batch 2000
Avg. loss per last 100 batches: 0.059296
Epoch: 20: Step: 2001/4907, loss=0.000125, lr=0.000010
Epoch: 20: Step: 2001/4907, loss=0.000125, lr=0.000010
Train batch 2100
Avg. loss per last 100 batches: 0.016811
Train batch 2100
Avg. loss per last 100 batches: 0.016811
Epoch: 20: Step: 2101/4907, loss=0.000000, lr=0.000010
Epoch: 20: Step: 2101/4907, loss=0.000000, lr=0.000010
Train batch 2200
Avg. loss per last 100 batches: 0.028946
Train batch 2200
Avg. loss per last 100 batches: 0.028946
Epoch: 20: Step: 2201/4907, loss=0.000110, lr=0.000010
Epoch: 20: Step: 2201/4907, loss=0.000110, lr=0.000010
Train batch 2300
Avg. loss per last 100 batches: 0.026943
Train batch 2300
Avg. loss per last 100 batches: 0.026943
Epoch: 20: Step: 2301/4907, loss=0.000202, lr=0.000010
Epoch: 20: Step: 2301/4907, loss=0.000202, lr=0.000010
Train batch 2400
Avg. loss per last 100 batches: 0.031822
Train batch 2400
Avg. loss per last 100 batches: 0.031822
Epoch: 20: Step: 2401/4907, loss=0.001668, lr=0.000010
Epoch: 20: Step: 2401/4907, loss=0.001668, lr=0.000010
Train batch 2500
Avg. loss per last 100 batches: 0.043459
Train batch 2500
Avg. loss per last 100 batches: 0.043459
Epoch: 20: Step: 2501/4907, loss=0.000005, lr=0.000010
Epoch: 20: Step: 2501/4907, loss=0.000005, lr=0.000010
Train batch 2600
Avg. loss per last 100 batches: 0.050632
Train batch 2600
Avg. loss per last 100 batches: 0.050632
Epoch: 20: Step: 2601/4907, loss=0.000070, lr=0.000010
Epoch: 20: Step: 2601/4907, loss=0.000070, lr=0.000010
Train batch 2700
Avg. loss per last 100 batches: 0.047129
Train batch 2700
Avg. loss per last 100 batches: 0.047129
Epoch: 20: Step: 2701/4907, loss=0.000006, lr=0.000010
Epoch: 20: Step: 2701/4907, loss=0.000006, lr=0.000010
Train batch 2800
Avg. loss per last 100 batches: 0.031059
Train batch 2800
Avg. loss per last 100 batches: 0.031059
Epoch: 20: Step: 2801/4907, loss=0.000008, lr=0.000010
Epoch: 20: Step: 2801/4907, loss=0.000008, lr=0.000010
Train batch 2900
Avg. loss per last 100 batches: 0.047729
Train batch 2900
Avg. loss per last 100 batches: 0.047729
Epoch: 20: Step: 2901/4907, loss=0.000490, lr=0.000010
Epoch: 20: Step: 2901/4907, loss=0.000490, lr=0.000010
Train batch 3000
Avg. loss per last 100 batches: 0.032287
Train batch 3000
Avg. loss per last 100 batches: 0.032287
Epoch: 20: Step: 3001/4907, loss=0.002225, lr=0.000010
Epoch: 20: Step: 3001/4907, loss=0.002225, lr=0.000010
Train batch 3100
Avg. loss per last 100 batches: 0.056518
Train batch 3100
Avg. loss per last 100 batches: 0.056518
Epoch: 20: Step: 3101/4907, loss=0.000144, lr=0.000010
Epoch: 20: Step: 3101/4907, loss=0.000144, lr=0.000010
Train batch 3200
Avg. loss per last 100 batches: 0.027386
Train batch 3200
Avg. loss per last 100 batches: 0.027386
Epoch: 20: Step: 3201/4907, loss=0.016578, lr=0.000010
Epoch: 20: Step: 3201/4907, loss=0.016578, lr=0.000010
Train batch 3300
Avg. loss per last 100 batches: 0.046323
Train batch 3300
Avg. loss per last 100 batches: 0.046323
Epoch: 20: Step: 3301/4907, loss=0.330678, lr=0.000010
Epoch: 20: Step: 3301/4907, loss=0.330678, lr=0.000010
Train batch 3400
Avg. loss per last 100 batches: 0.032711
Train batch 3400
Avg. loss per last 100 batches: 0.032711
Epoch: 20: Step: 3401/4907, loss=0.000216, lr=0.000010
Epoch: 20: Step: 3401/4907, loss=0.000216, lr=0.000010
Train batch 3500
Avg. loss per last 100 batches: 0.045043
Train batch 3500
Avg. loss per last 100 batches: 0.045043
Epoch: 20: Step: 3501/4907, loss=0.000001, lr=0.000010
Epoch: 20: Step: 3501/4907, loss=0.000001, lr=0.000010
Train batch 3600
Avg. loss per last 100 batches: 0.044788
Train batch 3600
Avg. loss per last 100 batches: 0.044788
Epoch: 20: Step: 3601/4907, loss=0.138200, lr=0.000010
Epoch: 20: Step: 3601/4907, loss=0.138200, lr=0.000010
Train batch 3700
Avg. loss per last 100 batches: 0.030114
Train batch 3700
Avg. loss per last 100 batches: 0.030114
Epoch: 20: Step: 3701/4907, loss=0.000016, lr=0.000010
Epoch: 20: Step: 3701/4907, loss=0.000016, lr=0.000010
Train batch 3800
Avg. loss per last 100 batches: 0.016020
Train batch 3800
Avg. loss per last 100 batches: 0.016020
Epoch: 20: Step: 3801/4907, loss=0.000040, lr=0.000010
Epoch: 20: Step: 3801/4907, loss=0.000040, lr=0.000010
Train batch 3900
Avg. loss per last 100 batches: 0.033702
Train batch 3900
Avg. loss per last 100 batches: 0.033702
Epoch: 20: Step: 3901/4907, loss=0.000868, lr=0.000010
Epoch: 20: Step: 3901/4907, loss=0.000868, lr=0.000010
Train batch 4000
Avg. loss per last 100 batches: 0.025176
Train batch 4000
Avg. loss per last 100 batches: 0.025176
Epoch: 20: Step: 4001/4907, loss=0.002307, lr=0.000010
Epoch: 20: Step: 4001/4907, loss=0.002307, lr=0.000010
Train batch 4100
Avg. loss per last 100 batches: 0.025938
Train batch 4100
Avg. loss per last 100 batches: 0.025938
Epoch: 20: Step: 4101/4907, loss=0.000001, lr=0.000010
Epoch: 20: Step: 4101/4907, loss=0.000001, lr=0.000010
Train batch 4200
Avg. loss per last 100 batches: 0.067009
Train batch 4200
Avg. loss per last 100 batches: 0.067009
Epoch: 20: Step: 4201/4907, loss=0.089792, lr=0.000010
Epoch: 20: Step: 4201/4907, loss=0.089792, lr=0.000010
Train batch 4300
Avg. loss per last 100 batches: 0.030736
Train batch 4300
Avg. loss per last 100 batches: 0.030736
Epoch: 20: Step: 4301/4907, loss=0.000083, lr=0.000010
Epoch: 20: Step: 4301/4907, loss=0.000083, lr=0.000010
Train batch 4400
Avg. loss per last 100 batches: 0.052084
Train batch 4400
Avg. loss per last 100 batches: 0.052084
Epoch: 20: Step: 4401/4907, loss=0.001199, lr=0.000010
Epoch: 20: Step: 4401/4907, loss=0.001199, lr=0.000010
Train batch 4500
Avg. loss per last 100 batches: 0.056546
Train batch 4500
Avg. loss per last 100 batches: 0.056546
Epoch: 20: Step: 4501/4907, loss=0.055967, lr=0.000010
Epoch: 20: Step: 4501/4907, loss=0.055967, lr=0.000010
Train batch 4600
Avg. loss per last 100 batches: 0.034207
Train batch 4600
Avg. loss per last 100 batches: 0.034207
Epoch: 20: Step: 4601/4907, loss=0.010151, lr=0.000010
Epoch: 20: Step: 4601/4907, loss=0.010151, lr=0.000010
Train batch 4700
Avg. loss per last 100 batches: 0.029204
Train batch 4700
Avg. loss per last 100 batches: 0.029204
Epoch: 20: Step: 4701/4907, loss=0.000000, lr=0.000010
Epoch: 20: Step: 4701/4907, loss=0.000000, lr=0.000010
Train batch 4800
Avg. loss per last 100 batches: 0.033217
Train batch 4800
Avg. loss per last 100 batches: 0.033217
Epoch: 20: Step: 4801/4907, loss=0.000036, lr=0.000010
Epoch: 20: Step: 4801/4907, loss=0.000036, lr=0.000010
Train batch 4900
Avg. loss per last 100 batches: 0.041761
Train batch 4900
Avg. loss per last 100 batches: 0.041761
Epoch: 20: Step: 4901/4907, loss=0.004265, lr=0.000010
Epoch: 20: Step: 4901/4907, loss=0.004265, lr=0.000010
Validation: Epoch: 20 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 20 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 221.9720687538367, total questions=6516
Av.rank validation: average rank 221.9720687538367, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.20.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.20.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 221.9720687538367, total questions=6516
Av.rank validation: average rank 221.9720687538367, total questions=6516
Av Loss per epoch=0.038057
epoch total correct predictions=58267
***** Epoch 21 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.20.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.20.4907
Av Loss per epoch=0.038057
epoch total correct predictions=58267
***** Epoch 21 *****
Epoch: 21: Step: 1/4907, loss=0.000000, lr=0.000010
Epoch: 21: Step: 1/4907, loss=0.000000, lr=0.000010
Train batch 100
Avg. loss per last 100 batches: 0.048381
Train batch 100
Avg. loss per last 100 batches: 0.048381
Epoch: 21: Step: 101/4907, loss=0.000906, lr=0.000010
Epoch: 21: Step: 101/4907, loss=0.000906, lr=0.000010
Train batch 200
Avg. loss per last 100 batches: 0.029179
Train batch 200
Avg. loss per last 100 batches: 0.029179
Epoch: 21: Step: 201/4907, loss=0.000001, lr=0.000010
Epoch: 21: Step: 201/4907, loss=0.000001, lr=0.000010
Train batch 300
Avg. loss per last 100 batches: 0.047534
Train batch 300
Avg. loss per last 100 batches: 0.047534
Epoch: 21: Step: 301/4907, loss=0.000152, lr=0.000010
Epoch: 21: Step: 301/4907, loss=0.000152, lr=0.000010
Train batch 400
Avg. loss per last 100 batches: 0.032093
Train batch 400
Avg. loss per last 100 batches: 0.032093
Epoch: 21: Step: 401/4907, loss=0.000001, lr=0.000010
Epoch: 21: Step: 401/4907, loss=0.000001, lr=0.000010
Train batch 500
Avg. loss per last 100 batches: 0.030303
Train batch 500
Avg. loss per last 100 batches: 0.030303
Epoch: 21: Step: 501/4907, loss=0.000001, lr=0.000010
Epoch: 21: Step: 501/4907, loss=0.000001, lr=0.000010
Train batch 600
Avg. loss per last 100 batches: 0.031193
Train batch 600
Avg. loss per last 100 batches: 0.031193
Epoch: 21: Step: 601/4907, loss=0.000321, lr=0.000009
Epoch: 21: Step: 601/4907, loss=0.000321, lr=0.000009
Train batch 700
Avg. loss per last 100 batches: 0.027396
Train batch 700
Avg. loss per last 100 batches: 0.027396
Epoch: 21: Step: 701/4907, loss=0.001385, lr=0.000009
Epoch: 21: Step: 701/4907, loss=0.001385, lr=0.000009
Train batch 800
Avg. loss per last 100 batches: 0.044696
Train batch 800
Avg. loss per last 100 batches: 0.044696
Epoch: 21: Step: 801/4907, loss=0.000002, lr=0.000009
Epoch: 21: Step: 801/4907, loss=0.000002, lr=0.000009
Train batch 900
Avg. loss per last 100 batches: 0.023827
Train batch 900
Avg. loss per last 100 batches: 0.023827
Epoch: 21: Step: 901/4907, loss=0.000270, lr=0.000009
Epoch: 21: Step: 901/4907, loss=0.000270, lr=0.000009
Train batch 1000
Avg. loss per last 100 batches: 0.030896
Train batch 1000
Avg. loss per last 100 batches: 0.030896
Epoch: 21: Step: 1001/4907, loss=0.256386, lr=0.000009
Epoch: 21: Step: 1001/4907, loss=0.256386, lr=0.000009
Train batch 1100
Avg. loss per last 100 batches: 0.026328
Train batch 1100
Avg. loss per last 100 batches: 0.026328
Epoch: 21: Step: 1101/4907, loss=0.000000, lr=0.000009
Epoch: 21: Step: 1101/4907, loss=0.000000, lr=0.000009
Train batch 1200
Avg. loss per last 100 batches: 0.022117
Train batch 1200
Avg. loss per last 100 batches: 0.022117
Epoch: 21: Step: 1201/4907, loss=0.001123, lr=0.000009
Epoch: 21: Step: 1201/4907, loss=0.001123, lr=0.000009
Train batch 1300
Avg. loss per last 100 batches: 0.025571
Train batch 1300
Avg. loss per last 100 batches: 0.025571
Epoch: 21: Step: 1301/4907, loss=0.000017, lr=0.000009
Epoch: 21: Step: 1301/4907, loss=0.000017, lr=0.000009
Train batch 1400
Avg. loss per last 100 batches: 0.039034
Train batch 1400
Avg. loss per last 100 batches: 0.039034
Epoch: 21: Step: 1401/4907, loss=0.000009, lr=0.000009
Epoch: 21: Step: 1401/4907, loss=0.000009, lr=0.000009
Train batch 1500
Avg. loss per last 100 batches: 0.030843
Train batch 1500
Avg. loss per last 100 batches: 0.030843
Epoch: 21: Step: 1501/4907, loss=0.000118, lr=0.000009
Epoch: 21: Step: 1501/4907, loss=0.000118, lr=0.000009
Train batch 1600
Avg. loss per last 100 batches: 0.049853
Train batch 1600
Avg. loss per last 100 batches: 0.049853
Epoch: 21: Step: 1601/4907, loss=0.117186, lr=0.000009
Epoch: 21: Step: 1601/4907, loss=0.117186, lr=0.000009
Train batch 1700
Avg. loss per last 100 batches: 0.057048
Train batch 1700
Avg. loss per last 100 batches: 0.057048
Epoch: 21: Step: 1701/4907, loss=0.000040, lr=0.000009
Epoch: 21: Step: 1701/4907, loss=0.000040, lr=0.000009
Train batch 1800
Avg. loss per last 100 batches: 0.027765
Train batch 1800
Avg. loss per last 100 batches: 0.027765
Epoch: 21: Step: 1801/4907, loss=0.018035, lr=0.000009
Epoch: 21: Step: 1801/4907, loss=0.018035, lr=0.000009
Train batch 1900
Avg. loss per last 100 batches: 0.041465
Train batch 1900
Avg. loss per last 100 batches: 0.041465
Epoch: 21: Step: 1901/4907, loss=0.000015, lr=0.000009
Epoch: 21: Step: 1901/4907, loss=0.000015, lr=0.000009
Train batch 2000
Avg. loss per last 100 batches: 0.037389
Train batch 2000
Avg. loss per last 100 batches: 0.037389
Epoch: 21: Step: 2001/4907, loss=0.000001, lr=0.000009
Epoch: 21: Step: 2001/4907, loss=0.000001, lr=0.000009
Train batch 2100
Avg. loss per last 100 batches: 0.040127
Train batch 2100
Avg. loss per last 100 batches: 0.040127
Epoch: 21: Step: 2101/4907, loss=0.491875, lr=0.000009
Epoch: 21: Step: 2101/4907, loss=0.491875, lr=0.000009
Train batch 2200
Avg. loss per last 100 batches: 0.038111
Train batch 2200
Avg. loss per last 100 batches: 0.038111
Epoch: 21: Step: 2201/4907, loss=0.000776, lr=0.000009
Epoch: 21: Step: 2201/4907, loss=0.000776, lr=0.000009
Train batch 2300
Avg. loss per last 100 batches: 0.027044
Train batch 2300
Avg. loss per last 100 batches: 0.027044
Epoch: 21: Step: 2301/4907, loss=0.174143, lr=0.000009
Epoch: 21: Step: 2301/4907, loss=0.174143, lr=0.000009
Train batch 2400
Avg. loss per last 100 batches: 0.043425
Train batch 2400
Avg. loss per last 100 batches: 0.043425
Epoch: 21: Step: 2401/4907, loss=0.000585, lr=0.000009
Epoch: 21: Step: 2401/4907, loss=0.000585, lr=0.000009
Train batch 2500
Avg. loss per last 100 batches: 0.031739
Train batch 2500
Avg. loss per last 100 batches: 0.031739
Epoch: 21: Step: 2501/4907, loss=0.003765, lr=0.000009
Epoch: 21: Step: 2501/4907, loss=0.003765, lr=0.000009
Train batch 2600
Avg. loss per last 100 batches: 0.032009
Train batch 2600
Avg. loss per last 100 batches: 0.032009
Epoch: 21: Step: 2601/4907, loss=0.000004, lr=0.000009
Epoch: 21: Step: 2601/4907, loss=0.000004, lr=0.000009
Train batch 2700
Avg. loss per last 100 batches: 0.029948
Train batch 2700
Avg. loss per last 100 batches: 0.029948
Epoch: 21: Step: 2701/4907, loss=0.121453, lr=0.000009
Epoch: 21: Step: 2701/4907, loss=0.121453, lr=0.000009
Train batch 2800
Avg. loss per last 100 batches: 0.019268
Train batch 2800
Avg. loss per last 100 batches: 0.019268
Epoch: 21: Step: 2801/4907, loss=0.146258, lr=0.000009
Epoch: 21: Step: 2801/4907, loss=0.146258, lr=0.000009
Train batch 2900
Train batch 2900
Avg. loss per last 100 batches: 0.019430
Avg. loss per last 100 batches: 0.019430
Epoch: 21: Step: 2901/4907, loss=0.000006, lr=0.000009
Epoch: 21: Step: 2901/4907, loss=0.000006, lr=0.000009
Train batch 3000
Avg. loss per last 100 batches: 0.049961
Train batch 3000
Avg. loss per last 100 batches: 0.049961
Epoch: 21: Step: 3001/4907, loss=0.000548, lr=0.000009
Epoch: 21: Step: 3001/4907, loss=0.000548, lr=0.000009
Train batch 3100
Avg. loss per last 100 batches: 0.037218
Train batch 3100
Avg. loss per last 100 batches: 0.037218
Epoch: 21: Step: 3101/4907, loss=0.000004, lr=0.000009
Epoch: 21: Step: 3101/4907, loss=0.000004, lr=0.000009
Train batch 3200
Avg. loss per last 100 batches: 0.034479
Train batch 3200
Avg. loss per last 100 batches: 0.034479
Epoch: 21: Step: 3201/4907, loss=0.000028, lr=0.000009
Epoch: 21: Step: 3201/4907, loss=0.000028, lr=0.000009
Train batch 3300
Avg. loss per last 100 batches: 0.028854
Train batch 3300
Avg. loss per last 100 batches: 0.028854
Epoch: 21: Step: 3301/4907, loss=0.000227, lr=0.000009
Epoch: 21: Step: 3301/4907, loss=0.000227, lr=0.000009
Train batch 3400
Avg. loss per last 100 batches: 0.050740
Train batch 3400
Avg. loss per last 100 batches: 0.050740
Epoch: 21: Step: 3401/4907, loss=0.007430, lr=0.000009
Epoch: 21: Step: 3401/4907, loss=0.007430, lr=0.000009
Train batch 3500
Avg. loss per last 100 batches: 0.033480
Train batch 3500
Avg. loss per last 100 batches: 0.033480
Epoch: 21: Step: 3501/4907, loss=0.000025, lr=0.000009
Epoch: 21: Step: 3501/4907, loss=0.000025, lr=0.000009
Train batch 3600
Avg. loss per last 100 batches: 0.013522
Train batch 3600
Avg. loss per last 100 batches: 0.013522
Epoch: 21: Step: 3601/4907, loss=0.045921, lr=0.000009
Epoch: 21: Step: 3601/4907, loss=0.045921, lr=0.000009
Train batch 3700
Avg. loss per last 100 batches: 0.043477
Train batch 3700
Avg. loss per last 100 batches: 0.043477
Epoch: 21: Step: 3701/4907, loss=0.633925, lr=0.000009
Epoch: 21: Step: 3701/4907, loss=0.633925, lr=0.000009
Train batch 3800
Avg. loss per last 100 batches: 0.049786
Train batch 3800
Avg. loss per last 100 batches: 0.049786
Epoch: 21: Step: 3801/4907, loss=0.000005, lr=0.000009
Epoch: 21: Step: 3801/4907, loss=0.000005, lr=0.000009
Train batch 3900
Avg. loss per last 100 batches: 0.024148
Train batch 3900
Avg. loss per last 100 batches: 0.024148
Epoch: 21: Step: 3901/4907, loss=0.000220, lr=0.000009
Epoch: 21: Step: 3901/4907, loss=0.000220, lr=0.000009
Train batch 4000
Avg. loss per last 100 batches: 0.036312
Train batch 4000
Avg. loss per last 100 batches: 0.036312
Epoch: 21: Step: 4001/4907, loss=0.000003, lr=0.000009
Epoch: 21: Step: 4001/4907, loss=0.000003, lr=0.000009
Train batch 4100
Avg. loss per last 100 batches: 0.031987
Train batch 4100
Avg. loss per last 100 batches: 0.031987
Epoch: 21: Step: 4101/4907, loss=0.001130, lr=0.000009
Epoch: 21: Step: 4101/4907, loss=0.001130, lr=0.000009
Train batch 4200
Avg. loss per last 100 batches: 0.045497
Train batch 4200
Avg. loss per last 100 batches: 0.045497
Epoch: 21: Step: 4201/4907, loss=0.000399, lr=0.000009
Epoch: 21: Step: 4201/4907, loss=0.000399, lr=0.000009
Train batch 4300
Avg. loss per last 100 batches: 0.049256
Train batch 4300
Avg. loss per last 100 batches: 0.049256
Epoch: 21: Step: 4301/4907, loss=0.000604, lr=0.000009
Epoch: 21: Step: 4301/4907, loss=0.000604, lr=0.000009
Train batch 4400
Avg. loss per last 100 batches: 0.053561
Train batch 4400
Avg. loss per last 100 batches: 0.053561
Epoch: 21: Step: 4401/4907, loss=0.034320, lr=0.000009
Epoch: 21: Step: 4401/4907, loss=0.034320, lr=0.000009
Train batch 4500
Avg. loss per last 100 batches: 0.043039
Train batch 4500
Avg. loss per last 100 batches: 0.043039
Epoch: 21: Step: 4501/4907, loss=0.002658, lr=0.000009
Epoch: 21: Step: 4501/4907, loss=0.002658, lr=0.000009
Train batch 4600
Avg. loss per last 100 batches: 0.029064
Train batch 4600
Avg. loss per last 100 batches: 0.029064
Epoch: 21: Step: 4601/4907, loss=0.000291, lr=0.000009
Epoch: 21: Step: 4601/4907, loss=0.000291, lr=0.000009
Train batch 4700
Avg. loss per last 100 batches: 0.028149
Train batch 4700
Avg. loss per last 100 batches: 0.028149
Epoch: 21: Step: 4701/4907, loss=0.000277, lr=0.000009
Epoch: 21: Step: 4701/4907, loss=0.000277, lr=0.000009
Train batch 4800
Avg. loss per last 100 batches: 0.050500
Train batch 4800
Avg. loss per last 100 batches: 0.050500
Epoch: 21: Step: 4801/4907, loss=0.000036, lr=0.000009
Epoch: 21: Step: 4801/4907, loss=0.000036, lr=0.000009
Train batch 4900
Avg. loss per last 100 batches: 0.080889
Train batch 4900
Avg. loss per last 100 batches: 0.080889
Epoch: 21: Step: 4901/4907, loss=0.000042, lr=0.000009
Epoch: 21: Step: 4901/4907, loss=0.000042, lr=0.000009
Validation: Epoch: 21 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 21 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 210.87737875997544, total questions=6516
Av.rank validation: average rank 210.87737875997544, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.21.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.21.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 210.87737875997544, total questions=6516
Av.rank validation: average rank 210.87737875997544, total questions=6516
Av Loss per epoch=0.036677
epoch total correct predictions=58324
***** Epoch 22 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.21.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.21.4907
Av Loss per epoch=0.036677
epoch total correct predictions=58324
***** Epoch 22 *****
Epoch: 22: Step: 1/4907, loss=0.000675, lr=0.000009
Epoch: 22: Step: 1/4907, loss=0.000675, lr=0.000009
Train batch 100
Avg. loss per last 100 batches: 0.007029
Train batch 100
Avg. loss per last 100 batches: 0.007029
Epoch: 22: Step: 101/4907, loss=0.000005, lr=0.000009
Epoch: 22: Step: 101/4907, loss=0.000005, lr=0.000009
Train batch 200
Avg. loss per last 100 batches: 0.027377
Train batch 200
Avg. loss per last 100 batches: 0.027377
Epoch: 22: Step: 201/4907, loss=0.000251, lr=0.000009
Epoch: 22: Step: 201/4907, loss=0.000251, lr=0.000009
Train batch 300
Avg. loss per last 100 batches: 0.013199
Train batch 300
Avg. loss per last 100 batches: 0.013199
Epoch: 22: Step: 301/4907, loss=0.006977, lr=0.000009
Epoch: 22: Step: 301/4907, loss=0.006977, lr=0.000009
Train batch 400
Avg. loss per last 100 batches: 0.018390
Train batch 400
Avg. loss per last 100 batches: 0.018390
Epoch: 22: Step: 401/4907, loss=0.000000, lr=0.000009
Epoch: 22: Step: 401/4907, loss=0.000000, lr=0.000009
Train batch 500
Avg. loss per last 100 batches: 0.031958
Train batch 500
Avg. loss per last 100 batches: 0.031958
Epoch: 22: Step: 501/4907, loss=0.000056, lr=0.000009
Epoch: 22: Step: 501/4907, loss=0.000056, lr=0.000009
Train batch 600
Avg. loss per last 100 batches: 0.046767
Train batch 600
Avg. loss per last 100 batches: 0.046767
Epoch: 22: Step: 601/4907, loss=0.000344, lr=0.000009
Epoch: 22: Step: 601/4907, loss=0.000344, lr=0.000009
Train batch 700
Avg. loss per last 100 batches: 0.042010
Train batch 700
Avg. loss per last 100 batches: 0.042010
Epoch: 22: Step: 701/4907, loss=0.000018, lr=0.000009
Epoch: 22: Step: 701/4907, loss=0.000018, lr=0.000009
Train batch 800
Avg. loss per last 100 batches: 0.038869
Train batch 800
Avg. loss per last 100 batches: 0.038869
Epoch: 22: Step: 801/4907, loss=0.012158, lr=0.000009
Epoch: 22: Step: 801/4907, loss=0.012158, lr=0.000009
Train batch 900
Avg. loss per last 100 batches: 0.038848
Train batch 900
Avg. loss per last 100 batches: 0.038848
Epoch: 22: Step: 901/4907, loss=0.154285, lr=0.000009
Epoch: 22: Step: 901/4907, loss=0.154285, lr=0.000009
Train batch 1000
Avg. loss per last 100 batches: 0.019921
Train batch 1000
Avg. loss per last 100 batches: 0.019921
Epoch: 22: Step: 1001/4907, loss=0.175361, lr=0.000009
Epoch: 22: Step: 1001/4907, loss=0.175361, lr=0.000009
Train batch 1100
Avg. loss per last 100 batches: 0.024353
Train batch 1100
Avg. loss per last 100 batches: 0.024353
Epoch: 22: Step: 1101/4907, loss=0.244333, lr=0.000009
Epoch: 22: Step: 1101/4907, loss=0.244333, lr=0.000009
Train batch 1200
Avg. loss per last 100 batches: 0.038775
Train batch 1200
Avg. loss per last 100 batches: 0.038775
Epoch: 22: Step: 1201/4907, loss=0.000512, lr=0.000009
Epoch: 22: Step: 1201/4907, loss=0.000512, lr=0.000009
Train batch 1300
Avg. loss per last 100 batches: 0.033258
Train batch 1300
Avg. loss per last 100 batches: 0.033258
Epoch: 22: Step: 1301/4907, loss=0.000232, lr=0.000009
Epoch: 22: Step: 1301/4907, loss=0.000232, lr=0.000009
Train batch 1400
Avg. loss per last 100 batches: 0.041218
Train batch 1400
Avg. loss per last 100 batches: 0.041218
Epoch: 22: Step: 1401/4907, loss=0.000000, lr=0.000009
Epoch: 22: Step: 1401/4907, loss=0.000000, lr=0.000009
Train batch 1500
Avg. loss per last 100 batches: 0.044604
Train batch 1500
Avg. loss per last 100 batches: 0.044604
Epoch: 22: Step: 1501/4907, loss=0.000096, lr=0.000009
Epoch: 22: Step: 1501/4907, loss=0.000096, lr=0.000009
Train batch 1600
Avg. loss per last 100 batches: 0.038047
Train batch 1600
Avg. loss per last 100 batches: 0.038047
Epoch: 22: Step: 1601/4907, loss=0.000372, lr=0.000009
Epoch: 22: Step: 1601/4907, loss=0.000372, lr=0.000009
Train batch 1700
Avg. loss per last 100 batches: 0.038045
Train batch 1700
Avg. loss per last 100 batches: 0.038045
Epoch: 22: Step: 1701/4907, loss=0.051325, lr=0.000009
Epoch: 22: Step: 1701/4907, loss=0.051325, lr=0.000009
Train batch 1800
Avg. loss per last 100 batches: 0.022341
Train batch 1800
Avg. loss per last 100 batches: 0.022341
Epoch: 22: Step: 1801/4907, loss=0.000839, lr=0.000009
Epoch: 22: Step: 1801/4907, loss=0.000839, lr=0.000009
Train batch 1900
Avg. loss per last 100 batches: 0.031787
Train batch 1900
Avg. loss per last 100 batches: 0.031787
Epoch: 22: Step: 1901/4907, loss=0.000000, lr=0.000009
Epoch: 22: Step: 1901/4907, loss=0.000000, lr=0.000009
Train batch 2000
Avg. loss per last 100 batches: 0.028159
Train batch 2000
Avg. loss per last 100 batches: 0.028159
Epoch: 22: Step: 2001/4907, loss=0.313709, lr=0.000009
Epoch: 22: Step: 2001/4907, loss=0.313709, lr=0.000009
Train batch 2100
Avg. loss per last 100 batches: 0.030495
Train batch 2100
Avg. loss per last 100 batches: 0.030495
Epoch: 22: Step: 2101/4907, loss=0.000014, lr=0.000009
Epoch: 22: Step: 2101/4907, loss=0.000014, lr=0.000009
Train batch 2200
Avg. loss per last 100 batches: 0.046754
Train batch 2200
Avg. loss per last 100 batches: 0.046754
Epoch: 22: Step: 2201/4907, loss=0.000003, lr=0.000009
Epoch: 22: Step: 2201/4907, loss=0.000003, lr=0.000009
Train batch 2300
Avg. loss per last 100 batches: 0.025610
Train batch 2300
Avg. loss per last 100 batches: 0.025610
Epoch: 22: Step: 2301/4907, loss=0.001048, lr=0.000009
Epoch: 22: Step: 2301/4907, loss=0.001048, lr=0.000009
Train batch 2400
Avg. loss per last 100 batches: 0.025209
Train batch 2400
Avg. loss per last 100 batches: 0.025209
Epoch: 22: Step: 2401/4907, loss=0.000046, lr=0.000009
Epoch: 22: Step: 2401/4907, loss=0.000046, lr=0.000009
Train batch 2500
Avg. loss per last 100 batches: 0.024366
Train batch 2500
Avg. loss per last 100 batches: 0.024366
Epoch: 22: Step: 2501/4907, loss=0.120180, lr=0.000009
Epoch: 22: Step: 2501/4907, loss=0.120180, lr=0.000009
Train batch 2600
Avg. loss per last 100 batches: 0.019365
Train batch 2600
Avg. loss per last 100 batches: 0.019365
Epoch: 22: Step: 2601/4907, loss=0.000001, lr=0.000009
Epoch: 22: Step: 2601/4907, loss=0.000001, lr=0.000009
Train batch 2700
Avg. loss per last 100 batches: 0.025018
Train batch 2700
Avg. loss per last 100 batches: 0.025018
Epoch: 22: Step: 2701/4907, loss=0.000001, lr=0.000009
Epoch: 22: Step: 2701/4907, loss=0.000001, lr=0.000009
Train batch 2800
Avg. loss per last 100 batches: 0.024779
Train batch 2800
Avg. loss per last 100 batches: 0.024779
Epoch: 22: Step: 2801/4907, loss=0.000026, lr=0.000009
Epoch: 22: Step: 2801/4907, loss=0.000026, lr=0.000009
Train batch 2900
Avg. loss per last 100 batches: 0.050531
Train batch 2900
Avg. loss per last 100 batches: 0.050531
Epoch: 22: Step: 2901/4907, loss=0.122829, lr=0.000009
Epoch: 22: Step: 2901/4907, loss=0.122829, lr=0.000009
Train batch 3000
Avg. loss per last 100 batches: 0.022554
Train batch 3000
Avg. loss per last 100 batches: 0.022554
Epoch: 22: Step: 3001/4907, loss=0.062500, lr=0.000009
Epoch: 22: Step: 3001/4907, loss=0.062500, lr=0.000009
Train batch 3100
Avg. loss per last 100 batches: 0.034458
Train batch 3100
Avg. loss per last 100 batches: 0.034458
Epoch: 22: Step: 3101/4907, loss=0.000000, lr=0.000009
Epoch: 22: Step: 3101/4907, loss=0.000000, lr=0.000009
Train batch 3200
Avg. loss per last 100 batches: 0.051492
Train batch 3200
Avg. loss per last 100 batches: 0.051492
Epoch: 22: Step: 3201/4907, loss=0.000000, lr=0.000009
Epoch: 22: Step: 3201/4907, loss=0.000000, lr=0.000009
Train batch 3300
Avg. loss per last 100 batches: 0.022700
Train batch 3300
Avg. loss per last 100 batches: 0.022700
Epoch: 22: Step: 3301/4907, loss=0.031191, lr=0.000009
Epoch: 22: Step: 3301/4907, loss=0.031191, lr=0.000009
Train batch 3400
Avg. loss per last 100 batches: 0.035296
Train batch 3400
Avg. loss per last 100 batches: 0.035296
Epoch: 22: Step: 3401/4907, loss=0.001970, lr=0.000009
Epoch: 22: Step: 3401/4907, loss=0.001970, lr=0.000009
Train batch 3500
Avg. loss per last 100 batches: 0.042216
Train batch 3500
Avg. loss per last 100 batches: 0.042216
Epoch: 22: Step: 3501/4907, loss=0.000006, lr=0.000009
Epoch: 22: Step: 3501/4907, loss=0.000006, lr=0.000009
Train batch 3600
Avg. loss per last 100 batches: 0.040127
Train batch 3600
Avg. loss per last 100 batches: 0.040127
Epoch: 22: Step: 3601/4907, loss=0.000000, lr=0.000009
Epoch: 22: Step: 3601/4907, loss=0.000000, lr=0.000009
Train batch 3700
Avg. loss per last 100 batches: 0.060503
Train batch 3700
Avg. loss per last 100 batches: 0.060503
Epoch: 22: Step: 3701/4907, loss=0.000005, lr=0.000009
Epoch: 22: Step: 3701/4907, loss=0.000005, lr=0.000009
Train batch 3800
Avg. loss per last 100 batches: 0.035681
Train batch 3800
Avg. loss per last 100 batches: 0.035681
Epoch: 22: Step: 3801/4907, loss=0.000021, lr=0.000009
Epoch: 22: Step: 3801/4907, loss=0.000021, lr=0.000009
Train batch 3900
Avg. loss per last 100 batches: 0.015871
Train batch 3900
Avg. loss per last 100 batches: 0.015871
Epoch: 22: Step: 3901/4907, loss=0.000000, lr=0.000009
Epoch: 22: Step: 3901/4907, loss=0.000000, lr=0.000009
Train batch 4000
Avg. loss per last 100 batches: 0.010676
Train batch 4000
Avg. loss per last 100 batches: 0.010676
Epoch: 22: Step: 4001/4907, loss=0.001277, lr=0.000009
Epoch: 22: Step: 4001/4907, loss=0.001277, lr=0.000009
Train batch 4100
Avg. loss per last 100 batches: 0.021684
Train batch 4100
Avg. loss per last 100 batches: 0.021684
Epoch: 22: Step: 4101/4907, loss=0.000886, lr=0.000009
Epoch: 22: Step: 4101/4907, loss=0.000886, lr=0.000009
Train batch 4200
Avg. loss per last 100 batches: 0.020757
Train batch 4200
Avg. loss per last 100 batches: 0.020757
Epoch: 22: Step: 4201/4907, loss=0.270326, lr=0.000009
Epoch: 22: Step: 4201/4907, loss=0.270326, lr=0.000009
Train batch 4300
Avg. loss per last 100 batches: 0.025578
Train batch 4300
Avg. loss per last 100 batches: 0.025578
Epoch: 22: Step: 4301/4907, loss=0.000001, lr=0.000009
Epoch: 22: Step: 4301/4907, loss=0.000001, lr=0.000009
Train batch 4400
Avg. loss per last 100 batches: 0.026561
Train batch 4400
Avg. loss per last 100 batches: 0.026561
Epoch: 22: Step: 4401/4907, loss=0.008206, lr=0.000009
Epoch: 22: Step: 4401/4907, loss=0.008206, lr=0.000009
Train batch 4500
Avg. loss per last 100 batches: 0.031605
Train batch 4500
Avg. loss per last 100 batches: 0.031605
Epoch: 22: Step: 4501/4907, loss=0.000020, lr=0.000009
Epoch: 22: Step: 4501/4907, loss=0.000020, lr=0.000009
Train batch 4600
Avg. loss per last 100 batches: 0.034913
Train batch 4600
Avg. loss per last 100 batches: 0.034913
Epoch: 22: Step: 4601/4907, loss=0.000022, lr=0.000009
Epoch: 22: Step: 4601/4907, loss=0.000022, lr=0.000009
Train batch 4700
Avg. loss per last 100 batches: 0.034431
Train batch 4700
Avg. loss per last 100 batches: 0.034431
Epoch: 22: Step: 4701/4907, loss=0.000662, lr=0.000009
Epoch: 22: Step: 4701/4907, loss=0.000662, lr=0.000009
Train batch 4800
Avg. loss per last 100 batches: 0.016516
Train batch 4800
Avg. loss per last 100 batches: 0.016516
Epoch: 22: Step: 4801/4907, loss=0.000004, lr=0.000009
Epoch: 22: Step: 4801/4907, loss=0.000004, lr=0.000009
Train batch 4900
Avg. loss per last 100 batches: 0.060542
Train batch 4900
Avg. loss per last 100 batches: 0.060542
Epoch: 22: Step: 4901/4907, loss=0.000000, lr=0.000009
Epoch: 22: Step: 4901/4907, loss=0.000000, lr=0.000009
Validation: Epoch: 22 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 22 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 235.38796807857582, total questions=6516
Av.rank validation: average rank 235.38796807857582, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.22.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.22.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 235.38796807857582, total questions=6516
Av.rank validation: average rank 235.38796807857582, total questions=6516
Av Loss per epoch=0.031409
epoch total correct predictions=58377
***** Epoch 23 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.22.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.22.4907
Av Loss per epoch=0.031409
epoch total correct predictions=58377
***** Epoch 23 *****
Epoch: 23: Step: 1/4907, loss=0.001109, lr=0.000009
Epoch: 23: Step: 1/4907, loss=0.001109, lr=0.000009
Train batch 100
Avg. loss per last 100 batches: 0.020779
Train batch 100
Avg. loss per last 100 batches: 0.020779
Epoch: 23: Step: 101/4907, loss=0.109634, lr=0.000009
Epoch: 23: Step: 101/4907, loss=0.109634, lr=0.000009
Train batch 200
Avg. loss per last 100 batches: 0.035865
Train batch 200
Avg. loss per last 100 batches: 0.035865
Epoch: 23: Step: 201/4907, loss=0.000008, lr=0.000009
Epoch: 23: Step: 201/4907, loss=0.000008, lr=0.000009
Train batch 300
Avg. loss per last 100 batches: 0.008950
Train batch 300
Avg. loss per last 100 batches: 0.008950
Epoch: 23: Step: 301/4907, loss=0.000000, lr=0.000009
Epoch: 23: Step: 301/4907, loss=0.000000, lr=0.000009
Train batch 400
Avg. loss per last 100 batches: 0.030997
Train batch 400
Avg. loss per last 100 batches: 0.030997
Epoch: 23: Step: 401/4907, loss=0.000000, lr=0.000009
Epoch: 23: Step: 401/4907, loss=0.000000, lr=0.000009
Train batch 500
Avg. loss per last 100 batches: 0.048325
Train batch 500
Avg. loss per last 100 batches: 0.048325
Epoch: 23: Step: 501/4907, loss=0.000010, lr=0.000009
Epoch: 23: Step: 501/4907, loss=0.000010, lr=0.000009
Train batch 600
Avg. loss per last 100 batches: 0.030001
Train batch 600
Avg. loss per last 100 batches: 0.030001
Epoch: 23: Step: 601/4907, loss=0.000001, lr=0.000008
Epoch: 23: Step: 601/4907, loss=0.000001, lr=0.000008
Train batch 700
Avg. loss per last 100 batches: 0.045717
Train batch 700
Avg. loss per last 100 batches: 0.045717
Epoch: 23: Step: 701/4907, loss=0.000888, lr=0.000008
Epoch: 23: Step: 701/4907, loss=0.000888, lr=0.000008
Train batch 800
Avg. loss per last 100 batches: 0.041312
Train batch 800
Avg. loss per last 100 batches: 0.041312
Epoch: 23: Step: 801/4907, loss=0.000003, lr=0.000008
Epoch: 23: Step: 801/4907, loss=0.000003, lr=0.000008
Train batch 900
Avg. loss per last 100 batches: 0.047220
Train batch 900
Avg. loss per last 100 batches: 0.047220
Epoch: 23: Step: 901/4907, loss=0.000032, lr=0.000008
Epoch: 23: Step: 901/4907, loss=0.000032, lr=0.000008
Train batch 1000
Avg. loss per last 100 batches: 0.016063
Train batch 1000
Avg. loss per last 100 batches: 0.016063
Epoch: 23: Step: 1001/4907, loss=0.004806, lr=0.000008
Epoch: 23: Step: 1001/4907, loss=0.004806, lr=0.000008
Train batch 1100
Avg. loss per last 100 batches: 0.026425
Train batch 1100
Avg. loss per last 100 batches: 0.026425
Epoch: 23: Step: 1101/4907, loss=0.000000, lr=0.000008
Epoch: 23: Step: 1101/4907, loss=0.000000, lr=0.000008
Train batch 1200
Avg. loss per last 100 batches: 0.044243
Train batch 1200
Avg. loss per last 100 batches: 0.044243
Epoch: 23: Step: 1201/4907, loss=0.000358, lr=0.000008
Epoch: 23: Step: 1201/4907, loss=0.000358, lr=0.000008
Train batch 1300
Avg. loss per last 100 batches: 0.015153
Train batch 1300
Avg. loss per last 100 batches: 0.015153
Epoch: 23: Step: 1301/4907, loss=0.068554, lr=0.000008
Epoch: 23: Step: 1301/4907, loss=0.068554, lr=0.000008
Train batch 1400
Avg. loss per last 100 batches: 0.024395
Train batch 1400
Avg. loss per last 100 batches: 0.024395
Epoch: 23: Step: 1401/4907, loss=0.000082, lr=0.000008
Epoch: 23: Step: 1401/4907, loss=0.000082, lr=0.000008
Train batch 1500
Avg. loss per last 100 batches: 0.030305
Train batch 1500
Avg. loss per last 100 batches: 0.030305
Epoch: 23: Step: 1501/4907, loss=0.229790, lr=0.000008
Epoch: 23: Step: 1501/4907, loss=0.229790, lr=0.000008
Train batch 1600
Avg. loss per last 100 batches: 0.030133
Train batch 1600
Avg. loss per last 100 batches: 0.030133
Epoch: 23: Step: 1601/4907, loss=0.000006, lr=0.000008
Epoch: 23: Step: 1601/4907, loss=0.000006, lr=0.000008
Train batch 1700
Avg. loss per last 100 batches: 0.055488
Train batch 1700
Avg. loss per last 100 batches: 0.055488
Epoch: 23: Step: 1701/4907, loss=0.001419, lr=0.000008
Epoch: 23: Step: 1701/4907, loss=0.001419, lr=0.000008
Train batch 1800
Avg. loss per last 100 batches: 0.020935
Train batch 1800
Avg. loss per last 100 batches: 0.020935
Epoch: 23: Step: 1801/4907, loss=0.031128, lr=0.000008
Epoch: 23: Step: 1801/4907, loss=0.031128, lr=0.000008
Train batch 1900
Avg. loss per last 100 batches: 0.033664
Train batch 1900
Avg. loss per last 100 batches: 0.033664
Epoch: 23: Step: 1901/4907, loss=0.004246, lr=0.000008
Epoch: 23: Step: 1901/4907, loss=0.004246, lr=0.000008
Train batch 2000
Avg. loss per last 100 batches: 0.022855
Train batch 2000
Avg. loss per last 100 batches: 0.022855
Epoch: 23: Step: 2001/4907, loss=0.000000, lr=0.000008
Epoch: 23: Step: 2001/4907, loss=0.000000, lr=0.000008
Train batch 2100
Avg. loss per last 100 batches: 0.030054
Train batch 2100
Avg. loss per last 100 batches: 0.030054
Epoch: 23: Step: 2101/4907, loss=0.000007, lr=0.000008
Epoch: 23: Step: 2101/4907, loss=0.000007, lr=0.000008
Train batch 2200
Avg. loss per last 100 batches: 0.031606
Train batch 2200
Avg. loss per last 100 batches: 0.031606
Epoch: 23: Step: 2201/4907, loss=0.002109, lr=0.000008
Epoch: 23: Step: 2201/4907, loss=0.002109, lr=0.000008
Train batch 2300
Avg. loss per last 100 batches: 0.029499
Train batch 2300
Avg. loss per last 100 batches: 0.029499
Epoch: 23: Step: 2301/4907, loss=0.004933, lr=0.000008
Epoch: 23: Step: 2301/4907, loss=0.004933, lr=0.000008
Train batch 2400
Avg. loss per last 100 batches: 0.019640
Train batch 2400
Avg. loss per last 100 batches: 0.019640
Epoch: 23: Step: 2401/4907, loss=0.000000, lr=0.000008
Epoch: 23: Step: 2401/4907, loss=0.000000, lr=0.000008
Train batch 2500
Avg. loss per last 100 batches: 0.015148
Train batch 2500
Avg. loss per last 100 batches: 0.015148
Epoch: 23: Step: 2501/4907, loss=0.000000, lr=0.000008
Epoch: 23: Step: 2501/4907, loss=0.000000, lr=0.000008
Train batch 2600
Avg. loss per last 100 batches: 0.035265
Train batch 2600
Avg. loss per last 100 batches: 0.035265
Epoch: 23: Step: 2601/4907, loss=0.000000, lr=0.000008
Epoch: 23: Step: 2601/4907, loss=0.000000, lr=0.000008
Train batch 2700
Avg. loss per last 100 batches: 0.027715
Train batch 2700
Avg. loss per last 100 batches: 0.027715
Epoch: 23: Step: 2701/4907, loss=0.000042, lr=0.000008
Epoch: 23: Step: 2701/4907, loss=0.000042, lr=0.000008
Train batch 2800
Avg. loss per last 100 batches: 0.019340
Train batch 2800
Avg. loss per last 100 batches: 0.019340
Epoch: 23: Step: 2801/4907, loss=0.000937, lr=0.000008
Epoch: 23: Step: 2801/4907, loss=0.000937, lr=0.000008
Train batch 2900
Avg. loss per last 100 batches: 0.026618
Train batch 2900
Avg. loss per last 100 batches: 0.026618
Epoch: 23: Step: 2901/4907, loss=0.000651, lr=0.000008
Epoch: 23: Step: 2901/4907, loss=0.000651, lr=0.000008
Train batch 3000
Avg. loss per last 100 batches: 0.021041
Train batch 3000
Avg. loss per last 100 batches: 0.021041
Epoch: 23: Step: 3001/4907, loss=0.001556, lr=0.000008
Epoch: 23: Step: 3001/4907, loss=0.001556, lr=0.000008
Train batch 3100
Avg. loss per last 100 batches: 0.034052
Train batch 3100
Avg. loss per last 100 batches: 0.034052
Epoch: 23: Step: 3101/4907, loss=0.000425, lr=0.000008
Epoch: 23: Step: 3101/4907, loss=0.000425, lr=0.000008
Train batch 3200
Avg. loss per last 100 batches: 0.023165
Train batch 3200
Avg. loss per last 100 batches: 0.023165
Epoch: 23: Step: 3201/4907, loss=0.000138, lr=0.000008
Epoch: 23: Step: 3201/4907, loss=0.000138, lr=0.000008
Train batch 3300
Avg. loss per last 100 batches: 0.014783
Train batch 3300
Avg. loss per last 100 batches: 0.014783
Epoch: 23: Step: 3301/4907, loss=0.000001, lr=0.000008
Epoch: 23: Step: 3301/4907, loss=0.000001, lr=0.000008
Train batch 3400
Avg. loss per last 100 batches: 0.032558
Train batch 3400
Avg. loss per last 100 batches: 0.032558
Epoch: 23: Step: 3401/4907, loss=0.000000, lr=0.000008
Epoch: 23: Step: 3401/4907, loss=0.000000, lr=0.000008
Train batch 3500
Avg. loss per last 100 batches: 0.044924
Train batch 3500
Avg. loss per last 100 batches: 0.044924
Epoch: 23: Step: 3501/4907, loss=0.000071, lr=0.000008
Epoch: 23: Step: 3501/4907, loss=0.000071, lr=0.000008
Train batch 3600
Avg. loss per last 100 batches: 0.035555
Train batch 3600
Avg. loss per last 100 batches: 0.035555
Epoch: 23: Step: 3601/4907, loss=0.000083, lr=0.000008
Epoch: 23: Step: 3601/4907, loss=0.000083, lr=0.000008
Train batch 3700
Avg. loss per last 100 batches: 0.035532
Train batch 3700
Avg. loss per last 100 batches: 0.035532
Epoch: 23: Step: 3701/4907, loss=0.000132, lr=0.000008
Epoch: 23: Step: 3701/4907, loss=0.000132, lr=0.000008
Train batch 3800
Avg. loss per last 100 batches: 0.018211
Train batch 3800
Avg. loss per last 100 batches: 0.018211
Epoch: 23: Step: 3801/4907, loss=0.000014, lr=0.000008
Epoch: 23: Step: 3801/4907, loss=0.000014, lr=0.000008
Train batch 3900
Avg. loss per last 100 batches: 0.013340
Train batch 3900
Avg. loss per last 100 batches: 0.013340
Epoch: 23: Step: 3901/4907, loss=0.000001, lr=0.000008
Epoch: 23: Step: 3901/4907, loss=0.000001, lr=0.000008
Train batch 4000
Avg. loss per last 100 batches: 0.023785
Train batch 4000
Avg. loss per last 100 batches: 0.023785
Epoch: 23: Step: 4001/4907, loss=0.026718, lr=0.000008
Epoch: 23: Step: 4001/4907, loss=0.026718, lr=0.000008
Train batch 4100
Avg. loss per last 100 batches: 0.056918
Train batch 4100
Avg. loss per last 100 batches: 0.056918
Epoch: 23: Step: 4101/4907, loss=0.000010, lr=0.000008
Epoch: 23: Step: 4101/4907, loss=0.000010, lr=0.000008
Train batch 4200
Avg. loss per last 100 batches: 0.020092
Train batch 4200
Avg. loss per last 100 batches: 0.020092
Epoch: 23: Step: 4201/4907, loss=0.242655, lr=0.000008
Epoch: 23: Step: 4201/4907, loss=0.242655, lr=0.000008
Train batch 4300
Avg. loss per last 100 batches: 0.021239
Train batch 4300
Avg. loss per last 100 batches: 0.021239
Epoch: 23: Step: 4301/4907, loss=0.000008, lr=0.000008
Epoch: 23: Step: 4301/4907, loss=0.000008, lr=0.000008
Train batch 4400
Avg. loss per last 100 batches: 0.039056
Train batch 4400
Avg. loss per last 100 batches: 0.039056
Epoch: 23: Step: 4401/4907, loss=0.000335, lr=0.000008
Epoch: 23: Step: 4401/4907, loss=0.000335, lr=0.000008
Train batch 4500
Avg. loss per last 100 batches: 0.024853
Train batch 4500
Avg. loss per last 100 batches: 0.024853
Epoch: 23: Step: 4501/4907, loss=0.000201, lr=0.000008
Epoch: 23: Step: 4501/4907, loss=0.000201, lr=0.000008
Train batch 4600
Avg. loss per last 100 batches: 0.016909
Train batch 4600
Avg. loss per last 100 batches: 0.016909
Epoch: 23: Step: 4601/4907, loss=0.133994, lr=0.000008
Epoch: 23: Step: 4601/4907, loss=0.133994, lr=0.000008
Train batch 4700
Avg. loss per last 100 batches: 0.032570
Train batch 4700
Avg. loss per last 100 batches: 0.032570
Epoch: 23: Step: 4701/4907, loss=0.000117, lr=0.000008
Epoch: 23: Step: 4701/4907, loss=0.000117, lr=0.000008
Train batch 4800
Avg. loss per last 100 batches: 0.018272
Train batch 4800
Avg. loss per last 100 batches: 0.018272
Epoch: 23: Step: 4801/4907, loss=0.000002, lr=0.000008
Epoch: 23: Step: 4801/4907, loss=0.000002, lr=0.000008
Train batch 4900
Avg. loss per last 100 batches: 0.033291
Train batch 4900
Avg. loss per last 100 batches: 0.033291
Epoch: 23: Step: 4901/4907, loss=0.000497, lr=0.000008
Epoch: 23: Step: 4901/4907, loss=0.000497, lr=0.000008
Validation: Epoch: 23 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 23 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 189.8950276243094, total questions=6516
Av.rank validation: average rank 189.8950276243094, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.23.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.23.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 189.8950276243094, total questions=6516
Av.rank validation: average rank 189.8950276243094, total questions=6516
Av Loss per epoch=0.029017
epoch total correct predictions=58413
***** Epoch 24 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.23.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.23.4907
Av Loss per epoch=0.029017
epoch total correct predictions=58413
***** Epoch 24 *****
Epoch: 24: Step: 1/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 1/4907, loss=0.000000, lr=0.000008
Train batch 100
Avg. loss per last 100 batches: 0.028335
Train batch 100
Avg. loss per last 100 batches: 0.028335
Epoch: 24: Step: 101/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 101/4907, loss=0.000000, lr=0.000008
Train batch 200
Avg. loss per last 100 batches: 0.020709
Train batch 200
Avg. loss per last 100 batches: 0.020709
Epoch: 24: Step: 201/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 201/4907, loss=0.000000, lr=0.000008
Train batch 300
Avg. loss per last 100 batches: 0.026748
Train batch 300
Avg. loss per last 100 batches: 0.026748
Epoch: 24: Step: 301/4907, loss=0.000001, lr=0.000008
Epoch: 24: Step: 301/4907, loss=0.000001, lr=0.000008
Train batch 400
Avg. loss per last 100 batches: 0.018590
Train batch 400
Avg. loss per last 100 batches: 0.018590
Epoch: 24: Step: 401/4907, loss=0.000001, lr=0.000008
Epoch: 24: Step: 401/4907, loss=0.000001, lr=0.000008
Train batch 500
Avg. loss per last 100 batches: 0.026899
Train batch 500
Avg. loss per last 100 batches: 0.026899
Epoch: 24: Step: 501/4907, loss=0.000191, lr=0.000008
Epoch: 24: Step: 501/4907, loss=0.000191, lr=0.000008
Train batch 600
Avg. loss per last 100 batches: 0.030806
Train batch 600
Avg. loss per last 100 batches: 0.030806
Epoch: 24: Step: 601/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 601/4907, loss=0.000000, lr=0.000008
Train batch 700
Avg. loss per last 100 batches: 0.025585
Train batch 700
Avg. loss per last 100 batches: 0.025585
Epoch: 24: Step: 701/4907, loss=0.000044, lr=0.000008
Epoch: 24: Step: 701/4907, loss=0.000044, lr=0.000008
Train batch 800
Avg. loss per last 100 batches: 0.035856
Train batch 800
Avg. loss per last 100 batches: 0.035856
Epoch: 24: Step: 801/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 801/4907, loss=0.000000, lr=0.000008
Train batch 900
Avg. loss per last 100 batches: 0.029856
Train batch 900
Avg. loss per last 100 batches: 0.029856
Epoch: 24: Step: 901/4907, loss=0.000002, lr=0.000008
Epoch: 24: Step: 901/4907, loss=0.000002, lr=0.000008
Train batch 1000
Avg. loss per last 100 batches: 0.021030
Train batch 1000
Avg. loss per last 100 batches: 0.021030
Epoch: 24: Step: 1001/4907, loss=0.000011, lr=0.000008
Epoch: 24: Step: 1001/4907, loss=0.000011, lr=0.000008
Train batch 1100
Avg. loss per last 100 batches: 0.029991
Train batch 1100
Avg. loss per last 100 batches: 0.029991
Epoch: 24: Step: 1101/4907, loss=0.005190, lr=0.000008
Epoch: 24: Step: 1101/4907, loss=0.005190, lr=0.000008
Train batch 1200
Avg. loss per last 100 batches: 0.032943
Train batch 1200
Avg. loss per last 100 batches: 0.032943
Epoch: 24: Step: 1201/4907, loss=0.000004, lr=0.000008
Epoch: 24: Step: 1201/4907, loss=0.000004, lr=0.000008
Train batch 1300
Avg. loss per last 100 batches: 0.013709
Train batch 1300
Avg. loss per last 100 batches: 0.013709
Epoch: 24: Step: 1301/4907, loss=0.012988, lr=0.000008
Epoch: 24: Step: 1301/4907, loss=0.012988, lr=0.000008
Train batch 1400
Avg. loss per last 100 batches: 0.031440
Train batch 1400
Avg. loss per last 100 batches: 0.031440
Epoch: 24: Step: 1401/4907, loss=0.000010, lr=0.000008
Epoch: 24: Step: 1401/4907, loss=0.000010, lr=0.000008
Train batch 1500
Avg. loss per last 100 batches: 0.016481
Train batch 1500
Avg. loss per last 100 batches: 0.016481
Epoch: 24: Step: 1501/4907, loss=0.000007, lr=0.000008
Epoch: 24: Step: 1501/4907, loss=0.000007, lr=0.000008
Train batch 1600
Avg. loss per last 100 batches: 0.030012
Train batch 1600
Avg. loss per last 100 batches: 0.030012
Epoch: 24: Step: 1601/4907, loss=0.051051, lr=0.000008
Epoch: 24: Step: 1601/4907, loss=0.051051, lr=0.000008
Train batch 1700
Avg. loss per last 100 batches: 0.028770
Train batch 1700
Avg. loss per last 100 batches: 0.028770
Epoch: 24: Step: 1701/4907, loss=0.000002, lr=0.000008
Epoch: 24: Step: 1701/4907, loss=0.000002, lr=0.000008
Train batch 1800
Avg. loss per last 100 batches: 0.030141
Train batch 1800
Avg. loss per last 100 batches: 0.030141
Epoch: 24: Step: 1801/4907, loss=0.001375, lr=0.000008
Epoch: 24: Step: 1801/4907, loss=0.001375, lr=0.000008
Train batch 1900
Avg. loss per last 100 batches: 0.043353
Train batch 1900
Avg. loss per last 100 batches: 0.043353
Epoch: 24: Step: 1901/4907, loss=0.000012, lr=0.000008
Epoch: 24: Step: 1901/4907, loss=0.000012, lr=0.000008
Train batch 2000
Avg. loss per last 100 batches: 0.025537
Train batch 2000
Avg. loss per last 100 batches: 0.025537
Epoch: 24: Step: 2001/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 2001/4907, loss=0.000000, lr=0.000008
Train batch 2100
Avg. loss per last 100 batches: 0.030757
Train batch 2100
Avg. loss per last 100 batches: 0.030757
Epoch: 24: Step: 2101/4907, loss=0.264505, lr=0.000008
Epoch: 24: Step: 2101/4907, loss=0.264505, lr=0.000008
Train batch 2200
Avg. loss per last 100 batches: 0.034548
Train batch 2200
Avg. loss per last 100 batches: 0.034548
Epoch: 24: Step: 2201/4907, loss=0.002000, lr=0.000008
Epoch: 24: Step: 2201/4907, loss=0.002000, lr=0.000008
Train batch 2300
Avg. loss per last 100 batches: 0.022977
Train batch 2300
Avg. loss per last 100 batches: 0.022977
Epoch: 24: Step: 2301/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 2301/4907, loss=0.000000, lr=0.000008
Train batch 2400
Avg. loss per last 100 batches: 0.037608
Train batch 2400
Avg. loss per last 100 batches: 0.037608
Epoch: 24: Step: 2401/4907, loss=0.000001, lr=0.000008
Epoch: 24: Step: 2401/4907, loss=0.000001, lr=0.000008
Train batch 2500
Avg. loss per last 100 batches: 0.033173
Train batch 2500
Avg. loss per last 100 batches: 0.033173
Epoch: 24: Step: 2501/4907, loss=0.002633, lr=0.000008
Epoch: 24: Step: 2501/4907, loss=0.002633, lr=0.000008
Train batch 2600
Avg. loss per last 100 batches: 0.030055
Train batch 2600
Avg. loss per last 100 batches: 0.030055
Epoch: 24: Step: 2601/4907, loss=0.000005, lr=0.000008
Epoch: 24: Step: 2601/4907, loss=0.000005, lr=0.000008
Train batch 2700
Avg. loss per last 100 batches: 0.024213
Train batch 2700
Avg. loss per last 100 batches: 0.024213
Epoch: 24: Step: 2701/4907, loss=0.004987, lr=0.000008
Epoch: 24: Step: 2701/4907, loss=0.004987, lr=0.000008
Train batch 2800
Avg. loss per last 100 batches: 0.023870
Train batch 2800
Avg. loss per last 100 batches: 0.023870
Epoch: 24: Step: 2801/4907, loss=0.000002, lr=0.000008
Epoch: 24: Step: 2801/4907, loss=0.000002, lr=0.000008
Train batch 2900
Avg. loss per last 100 batches: 0.024750
Train batch 2900
Avg. loss per last 100 batches: 0.024750
Epoch: 24: Step: 2901/4907, loss=0.000039, lr=0.000008
Epoch: 24: Step: 2901/4907, loss=0.000039, lr=0.000008
Train batch 3000
Avg. loss per last 100 batches: 0.058175
Train batch 3000
Avg. loss per last 100 batches: 0.058175
Epoch: 24: Step: 3001/4907, loss=0.000112, lr=0.000008
Epoch: 24: Step: 3001/4907, loss=0.000112, lr=0.000008
Train batch 3100
Avg. loss per last 100 batches: 0.036708
Train batch 3100
Avg. loss per last 100 batches: 0.036708
Epoch: 24: Step: 3101/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 3101/4907, loss=0.000000, lr=0.000008
Train batch 3200
Avg. loss per last 100 batches: 0.014567
Train batch 3200
Avg. loss per last 100 batches: 0.014567
Epoch: 24: Step: 3201/4907, loss=0.000103, lr=0.000008
Epoch: 24: Step: 3201/4907, loss=0.000103, lr=0.000008
Train batch 3300
Avg. loss per last 100 batches: 0.046064
Train batch 3300
Avg. loss per last 100 batches: 0.046064
Epoch: 24: Step: 3301/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 3301/4907, loss=0.000000, lr=0.000008
Train batch 3400
Avg. loss per last 100 batches: 0.019852
Train batch 3400
Avg. loss per last 100 batches: 0.019852
Epoch: 24: Step: 3401/4907, loss=0.002396, lr=0.000008
Epoch: 24: Step: 3401/4907, loss=0.002396, lr=0.000008
Train batch 3500
Avg. loss per last 100 batches: 0.026547
Train batch 3500
Avg. loss per last 100 batches: 0.026547
Epoch: 24: Step: 3501/4907, loss=0.000001, lr=0.000008
Epoch: 24: Step: 3501/4907, loss=0.000001, lr=0.000008
Train batch 3600
Avg. loss per last 100 batches: 0.020490
Train batch 3600
Avg. loss per last 100 batches: 0.020490
Epoch: 24: Step: 3601/4907, loss=0.000013, lr=0.000008
Epoch: 24: Step: 3601/4907, loss=0.000013, lr=0.000008
Train batch 3700
Avg. loss per last 100 batches: 0.033786
Train batch 3700
Avg. loss per last 100 batches: 0.033786
Epoch: 24: Step: 3701/4907, loss=0.000003, lr=0.000008
Epoch: 24: Step: 3701/4907, loss=0.000003, lr=0.000008
Train batch 3800
Avg. loss per last 100 batches: 0.020808
Train batch 3800
Avg. loss per last 100 batches: 0.020808
Epoch: 24: Step: 3801/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 3801/4907, loss=0.000000, lr=0.000008
Train batch 3900
Avg. loss per last 100 batches: 0.032342
Train batch 3900
Avg. loss per last 100 batches: 0.032342
Epoch: 24: Step: 3901/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 3901/4907, loss=0.000000, lr=0.000008
Train batch 4000
Avg. loss per last 100 batches: 0.018165
Train batch 4000
Avg. loss per last 100 batches: 0.018165
Epoch: 24: Step: 4001/4907, loss=0.000060, lr=0.000008
Epoch: 24: Step: 4001/4907, loss=0.000060, lr=0.000008
Train batch 4100
Avg. loss per last 100 batches: 0.023654
Train batch 4100
Avg. loss per last 100 batches: 0.023654
Epoch: 24: Step: 4101/4907, loss=0.000001, lr=0.000008
Epoch: 24: Step: 4101/4907, loss=0.000001, lr=0.000008
Train batch 4200
Avg. loss per last 100 batches: 0.038243
Train batch 4200
Avg. loss per last 100 batches: 0.038243
Epoch: 24: Step: 4201/4907, loss=0.000868, lr=0.000008
Epoch: 24: Step: 4201/4907, loss=0.000868, lr=0.000008
Train batch 4300
Avg. loss per last 100 batches: 0.007959
Train batch 4300
Avg. loss per last 100 batches: 0.007959
Epoch: 24: Step: 4301/4907, loss=0.001920, lr=0.000008
Epoch: 24: Step: 4301/4907, loss=0.001920, lr=0.000008
Train batch 4400
Avg. loss per last 100 batches: 0.025975
Train batch 4400
Avg. loss per last 100 batches: 0.025975
Epoch: 24: Step: 4401/4907, loss=0.018994, lr=0.000008
Epoch: 24: Step: 4401/4907, loss=0.018994, lr=0.000008
Train batch 4500
Avg. loss per last 100 batches: 0.023119
Train batch 4500
Avg. loss per last 100 batches: 0.023119
Epoch: 24: Step: 4501/4907, loss=0.000092, lr=0.000008
Epoch: 24: Step: 4501/4907, loss=0.000092, lr=0.000008
Train batch 4600
Avg. loss per last 100 batches: 0.019381
Train batch 4600
Avg. loss per last 100 batches: 0.019381
Epoch: 24: Step: 4601/4907, loss=0.016987, lr=0.000008
Epoch: 24: Step: 4601/4907, loss=0.016987, lr=0.000008
Train batch 4700
Avg. loss per last 100 batches: 0.017378
Train batch 4700
Avg. loss per last 100 batches: 0.017378
Epoch: 24: Step: 4701/4907, loss=0.000000, lr=0.000008
Epoch: 24: Step: 4701/4907, loss=0.000000, lr=0.000008
Train batch 4800
Avg. loss per last 100 batches: 0.033162
Train batch 4800
Avg. loss per last 100 batches: 0.033162
Epoch: 24: Step: 4801/4907, loss=0.000711, lr=0.000008
Epoch: 24: Step: 4801/4907, loss=0.000711, lr=0.000008
Train batch 4900
Avg. loss per last 100 batches: 0.044602
Train batch 4900
Avg. loss per last 100 batches: 0.044602
Epoch: 24: Step: 4901/4907, loss=0.000063, lr=0.000008
Epoch: 24: Step: 4901/4907, loss=0.000063, lr=0.000008
Validation: Epoch: 24 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 24 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 226.66390423572744, total questions=6516
Av.rank validation: average rank 226.66390423572744, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.24.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.24.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 226.66390423572744, total questions=6516
Av.rank validation: average rank 226.66390423572744, total questions=6516
Av Loss per epoch=0.027994
epoch total correct predictions=58443
***** Epoch 25 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.24.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.24.4907
Av Loss per epoch=0.027994
epoch total correct predictions=58443
***** Epoch 25 *****
Epoch: 25: Step: 1/4907, loss=0.464496, lr=0.000008
Epoch: 25: Step: 1/4907, loss=0.464496, lr=0.000008
Train batch 100
Avg. loss per last 100 batches: 0.013539
Train batch 100
Avg. loss per last 100 batches: 0.013539
Epoch: 25: Step: 101/4907, loss=0.001977, lr=0.000008
Epoch: 25: Step: 101/4907, loss=0.001977, lr=0.000008
Train batch 200
Avg. loss per last 100 batches: 0.023887
Train batch 200
Avg. loss per last 100 batches: 0.023887
Epoch: 25: Step: 201/4907, loss=0.000000, lr=0.000008
Epoch: 25: Step: 201/4907, loss=0.000000, lr=0.000008
Train batch 300
Avg. loss per last 100 batches: 0.028000
Train batch 300
Avg. loss per last 100 batches: 0.028000
Epoch: 25: Step: 301/4907, loss=0.000021, lr=0.000008
Epoch: 25: Step: 301/4907, loss=0.000021, lr=0.000008
Train batch 400
Avg. loss per last 100 batches: 0.026518
Train batch 400
Avg. loss per last 100 batches: 0.026518
Epoch: 25: Step: 401/4907, loss=0.000023, lr=0.000008
Epoch: 25: Step: 401/4907, loss=0.000023, lr=0.000008
Train batch 500
Avg. loss per last 100 batches: 0.040166
Train batch 500
Avg. loss per last 100 batches: 0.040166
Epoch: 25: Step: 501/4907, loss=0.000015, lr=0.000007
Epoch: 25: Step: 501/4907, loss=0.000015, lr=0.000007
Train batch 600
Avg. loss per last 100 batches: 0.022120
Train batch 600
Avg. loss per last 100 batches: 0.022120
Epoch: 25: Step: 601/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 601/4907, loss=0.000000, lr=0.000007
Train batch 700
Avg. loss per last 100 batches: 0.028620
Train batch 700
Avg. loss per last 100 batches: 0.028620
Epoch: 25: Step: 701/4907, loss=0.000204, lr=0.000007
Epoch: 25: Step: 701/4907, loss=0.000204, lr=0.000007
Train batch 800
Avg. loss per last 100 batches: 0.046688
Train batch 800
Avg. loss per last 100 batches: 0.046688
Epoch: 25: Step: 801/4907, loss=0.037994, lr=0.000007
Epoch: 25: Step: 801/4907, loss=0.037994, lr=0.000007
Train batch 900
Avg. loss per last 100 batches: 0.024798
Train batch 900
Avg. loss per last 100 batches: 0.024798
Epoch: 25: Step: 901/4907, loss=0.175068, lr=0.000007
Epoch: 25: Step: 901/4907, loss=0.175068, lr=0.000007
Train batch 1000
Avg. loss per last 100 batches: 0.032681
Train batch 1000
Avg. loss per last 100 batches: 0.032681
Epoch: 25: Step: 1001/4907, loss=0.000008, lr=0.000007
Epoch: 25: Step: 1001/4907, loss=0.000008, lr=0.000007
Train batch 1100
Avg. loss per last 100 batches: 0.030786
Train batch 1100
Avg. loss per last 100 batches: 0.030786
Epoch: 25: Step: 1101/4907, loss=0.000001, lr=0.000007
Epoch: 25: Step: 1101/4907, loss=0.000001, lr=0.000007
Train batch 1200
Avg. loss per last 100 batches: 0.030908
Train batch 1200
Avg. loss per last 100 batches: 0.030908
Epoch: 25: Step: 1201/4907, loss=0.080647, lr=0.000007
Epoch: 25: Step: 1201/4907, loss=0.080647, lr=0.000007
Train batch 1300
Avg. loss per last 100 batches: 0.012484
Train batch 1300
Avg. loss per last 100 batches: 0.012484
Epoch: 25: Step: 1301/4907, loss=0.012388, lr=0.000007
Epoch: 25: Step: 1301/4907, loss=0.012388, lr=0.000007
Train batch 1400
Avg. loss per last 100 batches: 0.013405
Train batch 1400
Avg. loss per last 100 batches: 0.013405
Epoch: 25: Step: 1401/4907, loss=0.000049, lr=0.000007
Epoch: 25: Step: 1401/4907, loss=0.000049, lr=0.000007
Train batch 1500
Avg. loss per last 100 batches: 0.008289
Train batch 1500
Avg. loss per last 100 batches: 0.008289
Epoch: 25: Step: 1501/4907, loss=0.000010, lr=0.000007
Epoch: 25: Step: 1501/4907, loss=0.000010, lr=0.000007
Train batch 1600
Avg. loss per last 100 batches: 0.020080
Train batch 1600
Avg. loss per last 100 batches: 0.020080
Epoch: 25: Step: 1601/4907, loss=0.000005, lr=0.000007
Epoch: 25: Step: 1601/4907, loss=0.000005, lr=0.000007
Train batch 1700
Avg. loss per last 100 batches: 0.042613
Train batch 1700
Avg. loss per last 100 batches: 0.042613
Epoch: 25: Step: 1701/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 1701/4907, loss=0.000000, lr=0.000007
Train batch 1800
Avg. loss per last 100 batches: 0.036910
Train batch 1800
Avg. loss per last 100 batches: 0.036910
Epoch: 25: Step: 1801/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 1801/4907, loss=0.000000, lr=0.000007
Train batch 1900
Avg. loss per last 100 batches: 0.032289
Train batch 1900
Avg. loss per last 100 batches: 0.032289
Epoch: 25: Step: 1901/4907, loss=0.000024, lr=0.000007
Epoch: 25: Step: 1901/4907, loss=0.000024, lr=0.000007
Train batch 2000
Avg. loss per last 100 batches: 0.021750
Train batch 2000
Avg. loss per last 100 batches: 0.021750
Epoch: 25: Step: 2001/4907, loss=0.006581, lr=0.000007
Epoch: 25: Step: 2001/4907, loss=0.006581, lr=0.000007
Train batch 2100
Avg. loss per last 100 batches: 0.027862
Train batch 2100
Avg. loss per last 100 batches: 0.027862
Epoch: 25: Step: 2101/4907, loss=0.002800, lr=0.000007
Epoch: 25: Step: 2101/4907, loss=0.002800, lr=0.000007
Train batch 2200
Avg. loss per last 100 batches: 0.022639
Train batch 2200
Avg. loss per last 100 batches: 0.022639
Epoch: 25: Step: 2201/4907, loss=0.000064, lr=0.000007
Epoch: 25: Step: 2201/4907, loss=0.000064, lr=0.000007
Train batch 2300
Avg. loss per last 100 batches: 0.053206
Train batch 2300
Avg. loss per last 100 batches: 0.053206
Epoch: 25: Step: 2301/4907, loss=0.000001, lr=0.000007
Epoch: 25: Step: 2301/4907, loss=0.000001, lr=0.000007
Train batch 2400
Avg. loss per last 100 batches: 0.029595
Train batch 2400
Avg. loss per last 100 batches: 0.029595
Epoch: 25: Step: 2401/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 2401/4907, loss=0.000000, lr=0.000007
Train batch 2500
Avg. loss per last 100 batches: 0.017315
Train batch 2500
Avg. loss per last 100 batches: 0.017315
Epoch: 25: Step: 2501/4907, loss=0.001261, lr=0.000007
Epoch: 25: Step: 2501/4907, loss=0.001261, lr=0.000007
Train batch 2600
Avg. loss per last 100 batches: 0.006730
Train batch 2600
Avg. loss per last 100 batches: 0.006730
Epoch: 25: Step: 2601/4907, loss=0.004665, lr=0.000007
Epoch: 25: Step: 2601/4907, loss=0.004665, lr=0.000007
Train batch 2700
Avg. loss per last 100 batches: 0.026306
Train batch 2700
Avg. loss per last 100 batches: 0.026306
Epoch: 25: Step: 2701/4907, loss=0.000007, lr=0.000007
Epoch: 25: Step: 2701/4907, loss=0.000007, lr=0.000007
Train batch 2800
Avg. loss per last 100 batches: 0.033898
Train batch 2800
Avg. loss per last 100 batches: 0.033898
Epoch: 25: Step: 2801/4907, loss=0.671795, lr=0.000007
Epoch: 25: Step: 2801/4907, loss=0.671795, lr=0.000007
Train batch 2900
Avg. loss per last 100 batches: 0.018245
Train batch 2900
Avg. loss per last 100 batches: 0.018245
Epoch: 25: Step: 2901/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 2901/4907, loss=0.000000, lr=0.000007
Train batch 3000
Avg. loss per last 100 batches: 0.031388
Train batch 3000
Avg. loss per last 100 batches: 0.031388
Epoch: 25: Step: 3001/4907, loss=0.000005, lr=0.000007
Epoch: 25: Step: 3001/4907, loss=0.000005, lr=0.000007
Train batch 3100
Avg. loss per last 100 batches: 0.025227
Train batch 3100
Avg. loss per last 100 batches: 0.025227
Epoch: 25: Step: 3101/4907, loss=0.000001, lr=0.000007
Epoch: 25: Step: 3101/4907, loss=0.000001, lr=0.000007
Train batch 3200
Avg. loss per last 100 batches: 0.032743
Train batch 3200
Avg. loss per last 100 batches: 0.032743
Epoch: 25: Step: 3201/4907, loss=0.000001, lr=0.000007
Epoch: 25: Step: 3201/4907, loss=0.000001, lr=0.000007
Train batch 3300
Avg. loss per last 100 batches: 0.017447
Train batch 3300
Avg. loss per last 100 batches: 0.017447
Epoch: 25: Step: 3301/4907, loss=0.000010, lr=0.000007
Epoch: 25: Step: 3301/4907, loss=0.000010, lr=0.000007
Train batch 3400
Avg. loss per last 100 batches: 0.014058
Train batch 3400
Avg. loss per last 100 batches: 0.014058
Epoch: 25: Step: 3401/4907, loss=0.068527, lr=0.000007
Epoch: 25: Step: 3401/4907, loss=0.068527, lr=0.000007
Train batch 3500
Avg. loss per last 100 batches: 0.015238
Train batch 3500
Avg. loss per last 100 batches: 0.015238
Epoch: 25: Step: 3501/4907, loss=0.000340, lr=0.000007
Epoch: 25: Step: 3501/4907, loss=0.000340, lr=0.000007
Train batch 3600
Avg. loss per last 100 batches: 0.032869
Train batch 3600
Avg. loss per last 100 batches: 0.032869
Epoch: 25: Step: 3601/4907, loss=0.000002, lr=0.000007
Epoch: 25: Step: 3601/4907, loss=0.000002, lr=0.000007
Train batch 3700
Avg. loss per last 100 batches: 0.019229
Train batch 3700
Avg. loss per last 100 batches: 0.019229
Epoch: 25: Step: 3701/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 3701/4907, loss=0.000000, lr=0.000007
Train batch 3800
Avg. loss per last 100 batches: 0.008229
Train batch 3800
Avg. loss per last 100 batches: 0.008229
Epoch: 25: Step: 3801/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 3801/4907, loss=0.000000, lr=0.000007
Train batch 3900
Avg. loss per last 100 batches: 0.021683
Train batch 3900
Avg. loss per last 100 batches: 0.021683
Epoch: 25: Step: 3901/4907, loss=0.001318, lr=0.000007
Epoch: 25: Step: 3901/4907, loss=0.001318, lr=0.000007
Train batch 4000
Avg. loss per last 100 batches: 0.015401
Train batch 4000
Avg. loss per last 100 batches: 0.015401
Epoch: 25: Step: 4001/4907, loss=0.000103, lr=0.000007
Epoch: 25: Step: 4001/4907, loss=0.000103, lr=0.000007
Train batch 4100
Avg. loss per last 100 batches: 0.013527
Train batch 4100
Avg. loss per last 100 batches: 0.013527
Epoch: 25: Step: 4101/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 4101/4907, loss=0.000000, lr=0.000007
Train batch 4200
Avg. loss per last 100 batches: 0.025369
Train batch 4200
Avg. loss per last 100 batches: 0.025369
Epoch: 25: Step: 4201/4907, loss=0.000912, lr=0.000007
Epoch: 25: Step: 4201/4907, loss=0.000912, lr=0.000007
Train batch 4300
Avg. loss per last 100 batches: 0.017341
Train batch 4300
Avg. loss per last 100 batches: 0.017341
Epoch: 25: Step: 4301/4907, loss=0.003956, lr=0.000007
Epoch: 25: Step: 4301/4907, loss=0.003956, lr=0.000007
Train batch 4400
Avg. loss per last 100 batches: 0.025976
Train batch 4400
Avg. loss per last 100 batches: 0.025976
Epoch: 25: Step: 4401/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 4401/4907, loss=0.000000, lr=0.000007
Train batch 4500
Avg. loss per last 100 batches: 0.048347
Train batch 4500
Avg. loss per last 100 batches: 0.048347
Epoch: 25: Step: 4501/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 4501/4907, loss=0.000000, lr=0.000007
Train batch 4600
Avg. loss per last 100 batches: 0.036646
Train batch 4600
Avg. loss per last 100 batches: 0.036646
Epoch: 25: Step: 4601/4907, loss=0.000000, lr=0.000007
Epoch: 25: Step: 4601/4907, loss=0.000000, lr=0.000007
Train batch 4700
Avg. loss per last 100 batches: 0.027982
Train batch 4700
Avg. loss per last 100 batches: 0.027982
Epoch: 25: Step: 4701/4907, loss=0.000013, lr=0.000007
Epoch: 25: Step: 4701/4907, loss=0.000013, lr=0.000007
Train batch 4800
Avg. loss per last 100 batches: 0.035051
Train batch 4800
Avg. loss per last 100 batches: 0.035051
Epoch: 25: Step: 4801/4907, loss=0.000156, lr=0.000007
Epoch: 25: Step: 4801/4907, loss=0.000156, lr=0.000007
Train batch 4900
Avg. loss per last 100 batches: 0.026573
Train batch 4900
Avg. loss per last 100 batches: 0.026573
Epoch: 25: Step: 4901/4907, loss=0.198977, lr=0.000007
Epoch: 25: Step: 4901/4907, loss=0.198977, lr=0.000007
Validation: Epoch: 25 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 25 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 200.70042971147944, total questions=6516
Av.rank validation: average rank 200.70042971147944, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.25.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.25.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 200.70042971147944, total questions=6516
Av.rank validation: average rank 200.70042971147944, total questions=6516
Av Loss per epoch=0.025777
epoch total correct predictions=58453
***** Epoch 26 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.25.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.25.4907
Av Loss per epoch=0.025777
epoch total correct predictions=58453
***** Epoch 26 *****
Epoch: 26: Step: 1/4907, loss=0.000211, lr=0.000007
Epoch: 26: Step: 1/4907, loss=0.000211, lr=0.000007
Train batch 100
Avg. loss per last 100 batches: 0.027308
Train batch 100
Avg. loss per last 100 batches: 0.027308
Epoch: 26: Step: 101/4907, loss=0.319873, lr=0.000007
Epoch: 26: Step: 101/4907, loss=0.319873, lr=0.000007
Train batch 200
Avg. loss per last 100 batches: 0.039825
Train batch 200
Avg. loss per last 100 batches: 0.039825
Epoch: 26: Step: 201/4907, loss=0.011141, lr=0.000007
Epoch: 26: Step: 201/4907, loss=0.011141, lr=0.000007
Train batch 300
Avg. loss per last 100 batches: 0.012727
Train batch 300
Avg. loss per last 100 batches: 0.012727
Epoch: 26: Step: 301/4907, loss=0.455786, lr=0.000007
Epoch: 26: Step: 301/4907, loss=0.455786, lr=0.000007
Train batch 400
Avg. loss per last 100 batches: 0.049437
Train batch 400
Avg. loss per last 100 batches: 0.049437
Epoch: 26: Step: 401/4907, loss=0.076892, lr=0.000007
Epoch: 26: Step: 401/4907, loss=0.076892, lr=0.000007
Train batch 500
Avg. loss per last 100 batches: 0.031205
Train batch 500
Avg. loss per last 100 batches: 0.031205
Epoch: 26: Step: 501/4907, loss=0.000001, lr=0.000007
Epoch: 26: Step: 501/4907, loss=0.000001, lr=0.000007
Train batch 600
Avg. loss per last 100 batches: 0.031140
Train batch 600
Avg. loss per last 100 batches: 0.031140
Epoch: 26: Step: 601/4907, loss=0.000005, lr=0.000007
Epoch: 26: Step: 601/4907, loss=0.000005, lr=0.000007
Train batch 700
Avg. loss per last 100 batches: 0.036301
Train batch 700
Avg. loss per last 100 batches: 0.036301
Epoch: 26: Step: 701/4907, loss=0.000028, lr=0.000007
Epoch: 26: Step: 701/4907, loss=0.000028, lr=0.000007
Train batch 800
Avg. loss per last 100 batches: 0.020406
Train batch 800
Avg. loss per last 100 batches: 0.020406
Epoch: 26: Step: 801/4907, loss=0.000199, lr=0.000007
Epoch: 26: Step: 801/4907, loss=0.000199, lr=0.000007
Train batch 900
Avg. loss per last 100 batches: 0.026917
Train batch 900
Avg. loss per last 100 batches: 0.026917
Epoch: 26: Step: 901/4907, loss=0.000137, lr=0.000007
Epoch: 26: Step: 901/4907, loss=0.000137, lr=0.000007
Train batch 1000
Avg. loss per last 100 batches: 0.019244
Train batch 1000
Avg. loss per last 100 batches: 0.019244
Epoch: 26: Step: 1001/4907, loss=0.000073, lr=0.000007
Epoch: 26: Step: 1001/4907, loss=0.000073, lr=0.000007
Train batch 1100
Avg. loss per last 100 batches: 0.019617
Train batch 1100
Avg. loss per last 100 batches: 0.019617
Epoch: 26: Step: 1101/4907, loss=0.001664, lr=0.000007
Epoch: 26: Step: 1101/4907, loss=0.001664, lr=0.000007
Train batch 1200
Avg. loss per last 100 batches: 0.047620
Train batch 1200
Avg. loss per last 100 batches: 0.047620
Epoch: 26: Step: 1201/4907, loss=0.000000, lr=0.000007
Epoch: 26: Step: 1201/4907, loss=0.000000, lr=0.000007
Train batch 1300
Avg. loss per last 100 batches: 0.032358
Train batch 1300
Avg. loss per last 100 batches: 0.032358
Epoch: 26: Step: 1301/4907, loss=0.179319, lr=0.000007
Epoch: 26: Step: 1301/4907, loss=0.179319, lr=0.000007
Train batch 1400
Avg. loss per last 100 batches: 0.013739
Train batch 1400
Avg. loss per last 100 batches: 0.013739
Epoch: 26: Step: 1401/4907, loss=0.000001, lr=0.000007
Epoch: 26: Step: 1401/4907, loss=0.000001, lr=0.000007
Train batch 1500
Avg. loss per last 100 batches: 0.024492
Train batch 1500
Avg. loss per last 100 batches: 0.024492
Epoch: 26: Step: 1501/4907, loss=0.000004, lr=0.000007
Epoch: 26: Step: 1501/4907, loss=0.000004, lr=0.000007
Train batch 1600
Avg. loss per last 100 batches: 0.022590
Train batch 1600
Avg. loss per last 100 batches: 0.022590
Epoch: 26: Step: 1601/4907, loss=0.000001, lr=0.000007
Epoch: 26: Step: 1601/4907, loss=0.000001, lr=0.000007
Train batch 1700
Avg. loss per last 100 batches: 0.013149
Train batch 1700
Avg. loss per last 100 batches: 0.013149
Epoch: 26: Step: 1701/4907, loss=0.203532, lr=0.000007
Epoch: 26: Step: 1701/4907, loss=0.203532, lr=0.000007
Train batch 1800
Avg. loss per last 100 batches: 0.033481
Train batch 1800
Avg. loss per last 100 batches: 0.033481
Epoch: 26: Step: 1801/4907, loss=0.000004, lr=0.000007
Epoch: 26: Step: 1801/4907, loss=0.000004, lr=0.000007
Train batch 1900
Avg. loss per last 100 batches: 0.048739
Train batch 1900
Avg. loss per last 100 batches: 0.048739
Epoch: 26: Step: 1901/4907, loss=0.000543, lr=0.000007
Epoch: 26: Step: 1901/4907, loss=0.000543, lr=0.000007
Train batch 2000
Avg. loss per last 100 batches: 0.029595
Train batch 2000
Avg. loss per last 100 batches: 0.029595
Epoch: 26: Step: 2001/4907, loss=0.000022, lr=0.000007
Epoch: 26: Step: 2001/4907, loss=0.000022, lr=0.000007
Train batch 2100
Avg. loss per last 100 batches: 0.029658
Train batch 2100
Avg. loss per last 100 batches: 0.029658
Epoch: 26: Step: 2101/4907, loss=0.038121, lr=0.000007
Epoch: 26: Step: 2101/4907, loss=0.038121, lr=0.000007
Train batch 2200
Avg. loss per last 100 batches: 0.023779
Train batch 2200
Avg. loss per last 100 batches: 0.023779
Epoch: 26: Step: 2201/4907, loss=0.000077, lr=0.000007
Epoch: 26: Step: 2201/4907, loss=0.000077, lr=0.000007
Train batch 2300
Avg. loss per last 100 batches: 0.051458
Train batch 2300
Avg. loss per last 100 batches: 0.051458
Epoch: 26: Step: 2301/4907, loss=0.000016, lr=0.000007
Epoch: 26: Step: 2301/4907, loss=0.000016, lr=0.000007
Train batch 2400
Avg. loss per last 100 batches: 0.028200
Train batch 2400
Avg. loss per last 100 batches: 0.028200
Epoch: 26: Step: 2401/4907, loss=0.000003, lr=0.000007
Epoch: 26: Step: 2401/4907, loss=0.000003, lr=0.000007
Train batch 2500
Avg. loss per last 100 batches: 0.025322
Train batch 2500
Avg. loss per last 100 batches: 0.025322
Epoch: 26: Step: 2501/4907, loss=0.000086, lr=0.000007
Epoch: 26: Step: 2501/4907, loss=0.000086, lr=0.000007
Train batch 2600
Avg. loss per last 100 batches: 0.010039
Train batch 2600
Avg. loss per last 100 batches: 0.010039
Epoch: 26: Step: 2601/4907, loss=0.023691, lr=0.000007
Epoch: 26: Step: 2601/4907, loss=0.023691, lr=0.000007
Train batch 2700
Avg. loss per last 100 batches: 0.020122
Train batch 2700
Avg. loss per last 100 batches: 0.020122
Epoch: 26: Step: 2701/4907, loss=0.000001, lr=0.000007
Epoch: 26: Step: 2701/4907, loss=0.000001, lr=0.000007
Train batch 2800
Avg. loss per last 100 batches: 0.017862
Train batch 2800
Avg. loss per last 100 batches: 0.017862
Epoch: 26: Step: 2801/4907, loss=0.000317, lr=0.000007
Epoch: 26: Step: 2801/4907, loss=0.000317, lr=0.000007
Train batch 2900
Avg. loss per last 100 batches: 0.028642
Train batch 2900
Avg. loss per last 100 batches: 0.028642
Epoch: 26: Step: 2901/4907, loss=0.000049, lr=0.000007
Epoch: 26: Step: 2901/4907, loss=0.000049, lr=0.000007
Train batch 3000
Avg. loss per last 100 batches: 0.037407
Train batch 3000
Avg. loss per last 100 batches: 0.037407
Epoch: 26: Step: 3001/4907, loss=0.000006, lr=0.000007
Epoch: 26: Step: 3001/4907, loss=0.000006, lr=0.000007
Train batch 3100
Avg. loss per last 100 batches: 0.013508
Train batch 3100
Avg. loss per last 100 batches: 0.013508
Epoch: 26: Step: 3101/4907, loss=0.000001, lr=0.000007
Epoch: 26: Step: 3101/4907, loss=0.000001, lr=0.000007
Train batch 3200
Avg. loss per last 100 batches: 0.032281
Train batch 3200
Avg. loss per last 100 batches: 0.032281
Epoch: 26: Step: 3201/4907, loss=0.000101, lr=0.000007
Epoch: 26: Step: 3201/4907, loss=0.000101, lr=0.000007
Train batch 3300
Avg. loss per last 100 batches: 0.042964
Train batch 3300
Avg. loss per last 100 batches: 0.042964
Epoch: 26: Step: 3301/4907, loss=0.001921, lr=0.000007
Epoch: 26: Step: 3301/4907, loss=0.001921, lr=0.000007
Train batch 3400
Avg. loss per last 100 batches: 0.037590
Train batch 3400
Avg. loss per last 100 batches: 0.037590
Epoch: 26: Step: 3401/4907, loss=0.000006, lr=0.000007
Epoch: 26: Step: 3401/4907, loss=0.000006, lr=0.000007
Train batch 3500
Avg. loss per last 100 batches: 0.012992
Train batch 3500
Avg. loss per last 100 batches: 0.012992
Epoch: 26: Step: 3501/4907, loss=0.000194, lr=0.000007
Epoch: 26: Step: 3501/4907, loss=0.000194, lr=0.000007
Train batch 3600
Avg. loss per last 100 batches: 0.019415
Train batch 3600
Avg. loss per last 100 batches: 0.019415
Epoch: 26: Step: 3601/4907, loss=0.103290, lr=0.000007
Epoch: 26: Step: 3601/4907, loss=0.103290, lr=0.000007
Train batch 3700
Avg. loss per last 100 batches: 0.016709
Train batch 3700
Avg. loss per last 100 batches: 0.016709
Epoch: 26: Step: 3701/4907, loss=0.011559, lr=0.000007
Epoch: 26: Step: 3701/4907, loss=0.011559, lr=0.000007
Train batch 3800
Avg. loss per last 100 batches: 0.018276
Train batch 3800
Avg. loss per last 100 batches: 0.018276
Epoch: 26: Step: 3801/4907, loss=0.000000, lr=0.000007
Epoch: 26: Step: 3801/4907, loss=0.000000, lr=0.000007
Train batch 3900
Avg. loss per last 100 batches: 0.024448
Train batch 3900
Avg. loss per last 100 batches: 0.024448
Epoch: 26: Step: 3901/4907, loss=0.113046, lr=0.000007
Epoch: 26: Step: 3901/4907, loss=0.113046, lr=0.000007
Train batch 4000
Avg. loss per last 100 batches: 0.031577
Train batch 4000
Avg. loss per last 100 batches: 0.031577
Epoch: 26: Step: 4001/4907, loss=0.001711, lr=0.000007
Epoch: 26: Step: 4001/4907, loss=0.001711, lr=0.000007
Train batch 4100
Avg. loss per last 100 batches: 0.033282
Train batch 4100
Avg. loss per last 100 batches: 0.033282
Epoch: 26: Step: 4101/4907, loss=0.000000, lr=0.000007
Epoch: 26: Step: 4101/4907, loss=0.000000, lr=0.000007
Train batch 4200
Avg. loss per last 100 batches: 0.030636
Train batch 4200
Avg. loss per last 100 batches: 0.030636
Epoch: 26: Step: 4201/4907, loss=0.000000, lr=0.000007
Epoch: 26: Step: 4201/4907, loss=0.000000, lr=0.000007
Train batch 4300
Avg. loss per last 100 batches: 0.027370
Train batch 4300
Avg. loss per last 100 batches: 0.027370
Epoch: 26: Step: 4301/4907, loss=0.000017, lr=0.000007
Epoch: 26: Step: 4301/4907, loss=0.000017, lr=0.000007
Train batch 4400
Avg. loss per last 100 batches: 0.014658
Train batch 4400
Avg. loss per last 100 batches: 0.014658
Epoch: 26: Step: 4401/4907, loss=0.000000, lr=0.000007
Epoch: 26: Step: 4401/4907, loss=0.000000, lr=0.000007
Train batch 4500
Avg. loss per last 100 batches: 0.037112
Train batch 4500
Avg. loss per last 100 batches: 0.037112
Epoch: 26: Step: 4501/4907, loss=0.000006, lr=0.000007
Epoch: 26: Step: 4501/4907, loss=0.000006, lr=0.000007
Train batch 4600
Avg. loss per last 100 batches: 0.034841
Train batch 4600
Avg. loss per last 100 batches: 0.034841
Epoch: 26: Step: 4601/4907, loss=0.000000, lr=0.000007
Epoch: 26: Step: 4601/4907, loss=0.000000, lr=0.000007
Train batch 4700
Avg. loss per last 100 batches: 0.016409
Train batch 4700
Avg. loss per last 100 batches: 0.016409
Epoch: 26: Step: 4701/4907, loss=0.000000, lr=0.000007
Epoch: 26: Step: 4701/4907, loss=0.000000, lr=0.000007
Train batch 4800
Avg. loss per last 100 batches: 0.020880
Train batch 4800
Avg. loss per last 100 batches: 0.020880
Epoch: 26: Step: 4801/4907, loss=0.000000, lr=0.000007
Epoch: 26: Step: 4801/4907, loss=0.000000, lr=0.000007
Train batch 4900
Avg. loss per last 100 batches: 0.019591
Train batch 4900
Avg. loss per last 100 batches: 0.019591
Epoch: 26: Step: 4901/4907, loss=0.733440, lr=0.000007
Epoch: 26: Step: 4901/4907, loss=0.733440, lr=0.000007
Validation: Epoch: 26 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 26 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 171.2744014732965, total questions=6516
Av.rank validation: average rank 171.2744014732965, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.26.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.26.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 171.2744014732965, total questions=6516
Av.rank validation: average rank 171.2744014732965, total questions=6516
Av Loss per epoch=0.027395
epoch total correct predictions=58454
***** Epoch 27 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.26.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.26.4907
Av Loss per epoch=0.027395
epoch total correct predictions=58454
***** Epoch 27 *****
Epoch: 27: Step: 1/4907, loss=0.000003, lr=0.000007
Epoch: 27: Step: 1/4907, loss=0.000003, lr=0.000007
Train batch 100
Avg. loss per last 100 batches: 0.027720
Train batch 100
Avg. loss per last 100 batches: 0.027720
Epoch: 27: Step: 101/4907, loss=0.000224, lr=0.000007
Epoch: 27: Step: 101/4907, loss=0.000224, lr=0.000007
Train batch 200
Avg. loss per last 100 batches: 0.014187
Train batch 200
Avg. loss per last 100 batches: 0.014187
Epoch: 27: Step: 201/4907, loss=0.000001, lr=0.000007
Epoch: 27: Step: 201/4907, loss=0.000001, lr=0.000007
Train batch 300
Avg. loss per last 100 batches: 0.015818
Train batch 300
Avg. loss per last 100 batches: 0.015818
Epoch: 27: Step: 301/4907, loss=0.000000, lr=0.000007
Epoch: 27: Step: 301/4907, loss=0.000000, lr=0.000007
Train batch 400
Avg. loss per last 100 batches: 0.023346
Train batch 400
Avg. loss per last 100 batches: 0.023346
Epoch: 27: Step: 401/4907, loss=0.000013, lr=0.000007
Epoch: 27: Step: 401/4907, loss=0.000013, lr=0.000007
Train batch 500
Avg. loss per last 100 batches: 0.010786
Train batch 500
Avg. loss per last 100 batches: 0.010786
Epoch: 27: Step: 501/4907, loss=0.010212, lr=0.000006
Epoch: 27: Step: 501/4907, loss=0.010212, lr=0.000006
Train batch 600
Avg. loss per last 100 batches: 0.045995
Train batch 600
Avg. loss per last 100 batches: 0.045995
Epoch: 27: Step: 601/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 601/4907, loss=0.000000, lr=0.000006
Train batch 700
Avg. loss per last 100 batches: 0.015765
Train batch 700
Avg. loss per last 100 batches: 0.015765
Epoch: 27: Step: 701/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 701/4907, loss=0.000000, lr=0.000006
Train batch 800
Avg. loss per last 100 batches: 0.022306
Train batch 800
Avg. loss per last 100 batches: 0.022306
Epoch: 27: Step: 801/4907, loss=0.001616, lr=0.000006
Epoch: 27: Step: 801/4907, loss=0.001616, lr=0.000006
Train batch 900
Avg. loss per last 100 batches: 0.020736
Train batch 900
Avg. loss per last 100 batches: 0.020736
Epoch: 27: Step: 901/4907, loss=0.000763, lr=0.000006
Epoch: 27: Step: 901/4907, loss=0.000763, lr=0.000006
Train batch 1000
Avg. loss per last 100 batches: 0.007580
Train batch 1000
Avg. loss per last 100 batches: 0.007580
Epoch: 27: Step: 1001/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 1001/4907, loss=0.000000, lr=0.000006
Train batch 1100
Avg. loss per last 100 batches: 0.024813
Train batch 1100
Avg. loss per last 100 batches: 0.024813
Epoch: 27: Step: 1101/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 1101/4907, loss=0.000000, lr=0.000006
Train batch 1200
Avg. loss per last 100 batches: 0.012002
Train batch 1200
Avg. loss per last 100 batches: 0.012002
Epoch: 27: Step: 1201/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 1201/4907, loss=0.000000, lr=0.000006
Train batch 1300
Avg. loss per last 100 batches: 0.022182
Train batch 1300
Avg. loss per last 100 batches: 0.022182
Epoch: 27: Step: 1301/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 1301/4907, loss=0.000000, lr=0.000006
Train batch 1400
Avg. loss per last 100 batches: 0.027573
Train batch 1400
Avg. loss per last 100 batches: 0.027573
Epoch: 27: Step: 1401/4907, loss=0.000017, lr=0.000006
Epoch: 27: Step: 1401/4907, loss=0.000017, lr=0.000006
Train batch 1500
Avg. loss per last 100 batches: 0.018913
Train batch 1500
Avg. loss per last 100 batches: 0.018913
Epoch: 27: Step: 1501/4907, loss=0.000203, lr=0.000006
Epoch: 27: Step: 1501/4907, loss=0.000203, lr=0.000006
Train batch 1600
Avg. loss per last 100 batches: 0.012140
Train batch 1600
Avg. loss per last 100 batches: 0.012140
Epoch: 27: Step: 1601/4907, loss=0.000008, lr=0.000006
Epoch: 27: Step: 1601/4907, loss=0.000008, lr=0.000006
Train batch 1700
Avg. loss per last 100 batches: 0.025233
Train batch 1700
Avg. loss per last 100 batches: 0.025233
Epoch: 27: Step: 1701/4907, loss=0.001769, lr=0.000006
Epoch: 27: Step: 1701/4907, loss=0.001769, lr=0.000006
Train batch 1800
Avg. loss per last 100 batches: 0.041758
Train batch 1800
Avg. loss per last 100 batches: 0.041758
Epoch: 27: Step: 1801/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 1801/4907, loss=0.000000, lr=0.000006
Train batch 1900
Avg. loss per last 100 batches: 0.011963
Train batch 1900
Avg. loss per last 100 batches: 0.011963
Epoch: 27: Step: 1901/4907, loss=0.000116, lr=0.000006
Epoch: 27: Step: 1901/4907, loss=0.000116, lr=0.000006
Train batch 2000
Avg. loss per last 100 batches: 0.007781
Train batch 2000
Avg. loss per last 100 batches: 0.007781
Epoch: 27: Step: 2001/4907, loss=0.000001, lr=0.000006
Epoch: 27: Step: 2001/4907, loss=0.000001, lr=0.000006
Train batch 2100
Avg. loss per last 100 batches: 0.025873
Train batch 2100
Avg. loss per last 100 batches: 0.025873
Epoch: 27: Step: 2101/4907, loss=0.000347, lr=0.000006
Epoch: 27: Step: 2101/4907, loss=0.000347, lr=0.000006
Train batch 2200
Avg. loss per last 100 batches: 0.026328
Train batch 2200
Avg. loss per last 100 batches: 0.026328
Epoch: 27: Step: 2201/4907, loss=0.000081, lr=0.000006
Epoch: 27: Step: 2201/4907, loss=0.000081, lr=0.000006
Train batch 2300
Avg. loss per last 100 batches: 0.019672
Train batch 2300
Avg. loss per last 100 batches: 0.019672
Epoch: 27: Step: 2301/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 2301/4907, loss=0.000000, lr=0.000006
Train batch 2400
Avg. loss per last 100 batches: 0.049217
Train batch 2400
Avg. loss per last 100 batches: 0.049217
Epoch: 27: Step: 2401/4907, loss=0.002144, lr=0.000006
Epoch: 27: Step: 2401/4907, loss=0.002144, lr=0.000006
Train batch 2500
Avg. loss per last 100 batches: 0.021749
Train batch 2500
Avg. loss per last 100 batches: 0.021749
Epoch: 27: Step: 2501/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 2501/4907, loss=0.000000, lr=0.000006
Train batch 2600
Avg. loss per last 100 batches: 0.033364
Train batch 2600
Avg. loss per last 100 batches: 0.033364
Epoch: 27: Step: 2601/4907, loss=0.000048, lr=0.000006
Epoch: 27: Step: 2601/4907, loss=0.000048, lr=0.000006
Train batch 2700
Avg. loss per last 100 batches: 0.021604
Train batch 2700
Avg. loss per last 100 batches: 0.021604
Epoch: 27: Step: 2701/4907, loss=0.000623, lr=0.000006
Epoch: 27: Step: 2701/4907, loss=0.000623, lr=0.000006
Train batch 2800
Avg. loss per last 100 batches: 0.016439
Train batch 2800
Avg. loss per last 100 batches: 0.016439
Epoch: 27: Step: 2801/4907, loss=0.000790, lr=0.000006
Epoch: 27: Step: 2801/4907, loss=0.000790, lr=0.000006
Train batch 2900
Avg. loss per last 100 batches: 0.025141
Train batch 2900
Avg. loss per last 100 batches: 0.025141
Epoch: 27: Step: 2901/4907, loss=0.000006, lr=0.000006
Epoch: 27: Step: 2901/4907, loss=0.000006, lr=0.000006
Train batch 3000
Avg. loss per last 100 batches: 0.024510
Train batch 3000
Avg. loss per last 100 batches: 0.024510
Epoch: 27: Step: 3001/4907, loss=0.000170, lr=0.000006
Epoch: 27: Step: 3001/4907, loss=0.000170, lr=0.000006
Train batch 3100
Avg. loss per last 100 batches: 0.021465
Train batch 3100
Avg. loss per last 100 batches: 0.021465
Epoch: 27: Step: 3101/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 3101/4907, loss=0.000000, lr=0.000006
Train batch 3200
Avg. loss per last 100 batches: 0.011743
Train batch 3200
Avg. loss per last 100 batches: 0.011743
Epoch: 27: Step: 3201/4907, loss=0.000006, lr=0.000006
Epoch: 27: Step: 3201/4907, loss=0.000006, lr=0.000006
Train batch 3300
Avg. loss per last 100 batches: 0.026351
Train batch 3300
Avg. loss per last 100 batches: 0.026351
Epoch: 27: Step: 3301/4907, loss=0.000002, lr=0.000006
Epoch: 27: Step: 3301/4907, loss=0.000002, lr=0.000006
Train batch 3400
Avg. loss per last 100 batches: 0.020390
Train batch 3400
Avg. loss per last 100 batches: 0.020390
Epoch: 27: Step: 3401/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 3401/4907, loss=0.000000, lr=0.000006
Train batch 3500
Avg. loss per last 100 batches: 0.037313
Train batch 3500
Avg. loss per last 100 batches: 0.037313
Epoch: 27: Step: 3501/4907, loss=0.000028, lr=0.000006
Epoch: 27: Step: 3501/4907, loss=0.000028, lr=0.000006
Train batch 3600
Avg. loss per last 100 batches: 0.025812
Train batch 3600
Avg. loss per last 100 batches: 0.025812
Epoch: 27: Step: 3601/4907, loss=1.296700, lr=0.000006
Epoch: 27: Step: 3601/4907, loss=1.296700, lr=0.000006
Train batch 3700
Avg. loss per last 100 batches: 0.055013
Train batch 3700
Avg. loss per last 100 batches: 0.055013
Epoch: 27: Step: 3701/4907, loss=0.074734, lr=0.000006
Epoch: 27: Step: 3701/4907, loss=0.074734, lr=0.000006
Train batch 3800
Train batch 3800
Avg. loss per last 100 batches: 0.029730
Avg. loss per last 100 batches: 0.029730
Epoch: 27: Step: 3801/4907, loss=0.000055, lr=0.000006
Epoch: 27: Step: 3801/4907, loss=0.000055, lr=0.000006
Train batch 3900
Avg. loss per last 100 batches: 0.017502
Train batch 3900
Avg. loss per last 100 batches: 0.017502
Epoch: 27: Step: 3901/4907, loss=0.000006, lr=0.000006
Epoch: 27: Step: 3901/4907, loss=0.000006, lr=0.000006
Train batch 4000
Avg. loss per last 100 batches: 0.086775
Train batch 4000
Avg. loss per last 100 batches: 0.086775
Epoch: 27: Step: 4001/4907, loss=0.000009, lr=0.000006
Epoch: 27: Step: 4001/4907, loss=0.000009, lr=0.000006
Train batch 4100
Avg. loss per last 100 batches: 0.014774
Train batch 4100
Avg. loss per last 100 batches: 0.014774
Epoch: 27: Step: 4101/4907, loss=0.000112, lr=0.000006
Epoch: 27: Step: 4101/4907, loss=0.000112, lr=0.000006
Train batch 4200
Avg. loss per last 100 batches: 0.018218
Train batch 4200
Avg. loss per last 100 batches: 0.018218
Epoch: 27: Step: 4201/4907, loss=0.000001, lr=0.000006
Epoch: 27: Step: 4201/4907, loss=0.000001, lr=0.000006
Train batch 4300
Avg. loss per last 100 batches: 0.003542
Train batch 4300
Avg. loss per last 100 batches: 0.003542
Epoch: 27: Step: 4301/4907, loss=0.000000, lr=0.000006
Epoch: 27: Step: 4301/4907, loss=0.000000, lr=0.000006
Train batch 4400
Avg. loss per last 100 batches: 0.024573
Train batch 4400
Avg. loss per last 100 batches: 0.024573
Epoch: 27: Step: 4401/4907, loss=0.000001, lr=0.000006
Epoch: 27: Step: 4401/4907, loss=0.000001, lr=0.000006
Train batch 4500
Avg. loss per last 100 batches: 0.008753
Train batch 4500
Avg. loss per last 100 batches: 0.008753
Epoch: 27: Step: 4501/4907, loss=0.000003, lr=0.000006
Epoch: 27: Step: 4501/4907, loss=0.000003, lr=0.000006
Train batch 4600
Avg. loss per last 100 batches: 0.032851
Train batch 4600
Avg. loss per last 100 batches: 0.032851
Epoch: 27: Step: 4601/4907, loss=0.000248, lr=0.000006
Epoch: 27: Step: 4601/4907, loss=0.000248, lr=0.000006
Train batch 4700
Avg. loss per last 100 batches: 0.017974
Train batch 4700
Avg. loss per last 100 batches: 0.017974
Epoch: 27: Step: 4701/4907, loss=0.000038, lr=0.000006
Epoch: 27: Step: 4701/4907, loss=0.000038, lr=0.000006
Train batch 4800
Avg. loss per last 100 batches: 0.021818
Train batch 4800
Avg. loss per last 100 batches: 0.021818
Epoch: 27: Step: 4801/4907, loss=0.000045, lr=0.000006
Epoch: 27: Step: 4801/4907, loss=0.000045, lr=0.000006
Train batch 4900
Avg. loss per last 100 batches: 0.009977
Train batch 4900
Avg. loss per last 100 batches: 0.009977
Epoch: 27: Step: 4901/4907, loss=0.000138, lr=0.000006
Epoch: 27: Step: 4901/4907, loss=0.000138, lr=0.000006
Validation: Epoch: 27 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 27 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 188.03944137507673, total questions=6516
Av.rank validation: average rank 188.03944137507673, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.27.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.27.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 188.03944137507673, total questions=6516
Av.rank validation: average rank 188.03944137507673, total questions=6516
Av Loss per epoch=0.023581
epoch total correct predictions=58514
***** Epoch 28 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.27.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.27.4907
Av Loss per epoch=0.023581
epoch total correct predictions=58514
***** Epoch 28 *****
Epoch: 28: Step: 1/4907, loss=0.000028, lr=0.000006
Epoch: 28: Step: 1/4907, loss=0.000028, lr=0.000006
Train batch 100
Avg. loss per last 100 batches: 0.025847
Train batch 100
Avg. loss per last 100 batches: 0.025847
Epoch: 28: Step: 101/4907, loss=0.000011, lr=0.000006
Epoch: 28: Step: 101/4907, loss=0.000011, lr=0.000006
Train batch 200
Avg. loss per last 100 batches: 0.016999
Train batch 200
Avg. loss per last 100 batches: 0.016999
Epoch: 28: Step: 201/4907, loss=0.000908, lr=0.000006
Epoch: 28: Step: 201/4907, loss=0.000908, lr=0.000006
Train batch 300
Avg. loss per last 100 batches: 0.029941
Train batch 300
Avg. loss per last 100 batches: 0.029941
Epoch: 28: Step: 301/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 301/4907, loss=0.000000, lr=0.000006
Train batch 400
Avg. loss per last 100 batches: 0.022969
Train batch 400
Avg. loss per last 100 batches: 0.022969
Epoch: 28: Step: 401/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 401/4907, loss=0.000000, lr=0.000006
Train batch 500
Avg. loss per last 100 batches: 0.024955
Train batch 500
Avg. loss per last 100 batches: 0.024955
Epoch: 28: Step: 501/4907, loss=0.002088, lr=0.000006
Epoch: 28: Step: 501/4907, loss=0.002088, lr=0.000006
Train batch 600
Avg. loss per last 100 batches: 0.030589
Train batch 600
Avg. loss per last 100 batches: 0.030589
Epoch: 28: Step: 601/4907, loss=0.000003, lr=0.000006
Epoch: 28: Step: 601/4907, loss=0.000003, lr=0.000006
Train batch 700
Avg. loss per last 100 batches: 0.009357
Train batch 700
Avg. loss per last 100 batches: 0.009357
Epoch: 28: Step: 701/4907, loss=0.000332, lr=0.000006
Epoch: 28: Step: 701/4907, loss=0.000332, lr=0.000006
Train batch 800
Avg. loss per last 100 batches: 0.025946
Train batch 800
Avg. loss per last 100 batches: 0.025946
Epoch: 28: Step: 801/4907, loss=0.000005, lr=0.000006
Epoch: 28: Step: 801/4907, loss=0.000005, lr=0.000006
Train batch 900
Avg. loss per last 100 batches: 0.041911
Train batch 900
Avg. loss per last 100 batches: 0.041911
Epoch: 28: Step: 901/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 901/4907, loss=0.000000, lr=0.000006
Train batch 1000
Avg. loss per last 100 batches: 0.016802
Train batch 1000
Avg. loss per last 100 batches: 0.016802
Epoch: 28: Step: 1001/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 1001/4907, loss=0.000000, lr=0.000006
Train batch 1100
Avg. loss per last 100 batches: 0.017745
Train batch 1100
Avg. loss per last 100 batches: 0.017745
Epoch: 28: Step: 1101/4907, loss=0.000001, lr=0.000006
Epoch: 28: Step: 1101/4907, loss=0.000001, lr=0.000006
Train batch 1200
Avg. loss per last 100 batches: 0.015255
Train batch 1200
Avg. loss per last 100 batches: 0.015255
Epoch: 28: Step: 1201/4907, loss=0.000583, lr=0.000006
Epoch: 28: Step: 1201/4907, loss=0.000583, lr=0.000006
Train batch 1300
Avg. loss per last 100 batches: 0.021604
Train batch 1300
Avg. loss per last 100 batches: 0.021604
Epoch: 28: Step: 1301/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 1301/4907, loss=0.000000, lr=0.000006
Train batch 1400
Avg. loss per last 100 batches: 0.014279
Train batch 1400
Avg. loss per last 100 batches: 0.014279
Epoch: 28: Step: 1401/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 1401/4907, loss=0.000000, lr=0.000006
Train batch 1500
Avg. loss per last 100 batches: 0.047041
Train batch 1500
Avg. loss per last 100 batches: 0.047041
Epoch: 28: Step: 1501/4907, loss=0.000294, lr=0.000006
Epoch: 28: Step: 1501/4907, loss=0.000294, lr=0.000006
Train batch 1600
Avg. loss per last 100 batches: 0.044429
Train batch 1600
Avg. loss per last 100 batches: 0.044429
Epoch: 28: Step: 1601/4907, loss=0.000132, lr=0.000006
Epoch: 28: Step: 1601/4907, loss=0.000132, lr=0.000006
Train batch 1700
Avg. loss per last 100 batches: 0.020871
Train batch 1700
Avg. loss per last 100 batches: 0.020871
Epoch: 28: Step: 1701/4907, loss=0.000016, lr=0.000006
Epoch: 28: Step: 1701/4907, loss=0.000016, lr=0.000006
Train batch 1800
Avg. loss per last 100 batches: 0.030504
Train batch 1800
Avg. loss per last 100 batches: 0.030504
Epoch: 28: Step: 1801/4907, loss=0.000178, lr=0.000006
Epoch: 28: Step: 1801/4907, loss=0.000178, lr=0.000006
Train batch 1900
Avg. loss per last 100 batches: 0.028658
Train batch 1900
Avg. loss per last 100 batches: 0.028658
Epoch: 28: Step: 1901/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 1901/4907, loss=0.000000, lr=0.000006
Train batch 2000
Avg. loss per last 100 batches: 0.021924
Train batch 2000
Avg. loss per last 100 batches: 0.021924
Epoch: 28: Step: 2001/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 2001/4907, loss=0.000000, lr=0.000006
Train batch 2100
Avg. loss per last 100 batches: 0.049179
Train batch 2100
Avg. loss per last 100 batches: 0.049179
Epoch: 28: Step: 2101/4907, loss=0.004671, lr=0.000006
Epoch: 28: Step: 2101/4907, loss=0.004671, lr=0.000006
Train batch 2200
Avg. loss per last 100 batches: 0.013259
Train batch 2200
Avg. loss per last 100 batches: 0.013259
Epoch: 28: Step: 2201/4907, loss=0.000018, lr=0.000006
Epoch: 28: Step: 2201/4907, loss=0.000018, lr=0.000006
Train batch 2300
Avg. loss per last 100 batches: 0.040347
Train batch 2300
Avg. loss per last 100 batches: 0.040347
Epoch: 28: Step: 2301/4907, loss=0.000006, lr=0.000006
Epoch: 28: Step: 2301/4907, loss=0.000006, lr=0.000006
Train batch 2400
Avg. loss per last 100 batches: 0.024668
Train batch 2400
Avg. loss per last 100 batches: 0.024668
Epoch: 28: Step: 2401/4907, loss=0.000074, lr=0.000006
Epoch: 28: Step: 2401/4907, loss=0.000074, lr=0.000006
Train batch 2500
Avg. loss per last 100 batches: 0.025622
Train batch 2500
Avg. loss per last 100 batches: 0.025622
Epoch: 28: Step: 2501/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 2501/4907, loss=0.000000, lr=0.000006
Train batch 2600
Avg. loss per last 100 batches: 0.018558
Train batch 2600
Avg. loss per last 100 batches: 0.018558
Epoch: 28: Step: 2601/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 2601/4907, loss=0.000000, lr=0.000006
Train batch 2700
Avg. loss per last 100 batches: 0.021037
Train batch 2700
Avg. loss per last 100 batches: 0.021037
Epoch: 28: Step: 2701/4907, loss=0.001012, lr=0.000006
Epoch: 28: Step: 2701/4907, loss=0.001012, lr=0.000006
Train batch 2800
Avg. loss per last 100 batches: 0.012271
Train batch 2800
Avg. loss per last 100 batches: 0.012271
Epoch: 28: Step: 2801/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 2801/4907, loss=0.000000, lr=0.000006
Train batch 2900
Avg. loss per last 100 batches: 0.016905
Train batch 2900
Avg. loss per last 100 batches: 0.016905
Epoch: 28: Step: 2901/4907, loss=0.003928, lr=0.000006
Epoch: 28: Step: 2901/4907, loss=0.003928, lr=0.000006
Train batch 3000
Avg. loss per last 100 batches: 0.046756
Train batch 3000
Avg. loss per last 100 batches: 0.046756
Epoch: 28: Step: 3001/4907, loss=0.031073, lr=0.000006
Epoch: 28: Step: 3001/4907, loss=0.031073, lr=0.000006
Train batch 3100
Avg. loss per last 100 batches: 0.019295
Train batch 3100
Avg. loss per last 100 batches: 0.019295
Epoch: 28: Step: 3101/4907, loss=0.000056, lr=0.000006
Epoch: 28: Step: 3101/4907, loss=0.000056, lr=0.000006
Train batch 3200
Avg. loss per last 100 batches: 0.035309
Train batch 3200
Avg. loss per last 100 batches: 0.035309
Epoch: 28: Step: 3201/4907, loss=0.000381, lr=0.000006
Epoch: 28: Step: 3201/4907, loss=0.000381, lr=0.000006
Train batch 3300
Avg. loss per last 100 batches: 0.014928
Train batch 3300
Avg. loss per last 100 batches: 0.014928
Epoch: 28: Step: 3301/4907, loss=0.000103, lr=0.000006
Epoch: 28: Step: 3301/4907, loss=0.000103, lr=0.000006
Train batch 3400
Avg. loss per last 100 batches: 0.017228
Train batch 3400
Avg. loss per last 100 batches: 0.017228
Epoch: 28: Step: 3401/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 3401/4907, loss=0.000000, lr=0.000006
Train batch 3500
Avg. loss per last 100 batches: 0.027587
Train batch 3500
Avg. loss per last 100 batches: 0.027587
Epoch: 28: Step: 3501/4907, loss=0.039135, lr=0.000006
Epoch: 28: Step: 3501/4907, loss=0.039135, lr=0.000006
Train batch 3600
Avg. loss per last 100 batches: 0.039723
Train batch 3600
Avg. loss per last 100 batches: 0.039723
Epoch: 28: Step: 3601/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 3601/4907, loss=0.000000, lr=0.000006
Train batch 3700
Avg. loss per last 100 batches: 0.028937
Train batch 3700
Avg. loss per last 100 batches: 0.028937
Epoch: 28: Step: 3701/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 3701/4907, loss=0.000000, lr=0.000006
Train batch 3800
Avg. loss per last 100 batches: 0.019697
Train batch 3800
Avg. loss per last 100 batches: 0.019697
Epoch: 28: Step: 3801/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 3801/4907, loss=0.000000, lr=0.000006
Train batch 3900
Avg. loss per last 100 batches: 0.015693
Train batch 3900
Avg. loss per last 100 batches: 0.015693
Epoch: 28: Step: 3901/4907, loss=0.000001, lr=0.000006
Epoch: 28: Step: 3901/4907, loss=0.000001, lr=0.000006
Train batch 4000
Avg. loss per last 100 batches: 0.030179
Train batch 4000
Avg. loss per last 100 batches: 0.030179
Epoch: 28: Step: 4001/4907, loss=0.000267, lr=0.000006
Epoch: 28: Step: 4001/4907, loss=0.000267, lr=0.000006
Train batch 4100
Avg. loss per last 100 batches: 0.039409
Train batch 4100
Avg. loss per last 100 batches: 0.039409
Epoch: 28: Step: 4101/4907, loss=0.002693, lr=0.000006
Epoch: 28: Step: 4101/4907, loss=0.002693, lr=0.000006
Train batch 4200
Avg. loss per last 100 batches: 0.038116
Train batch 4200
Avg. loss per last 100 batches: 0.038116
Epoch: 28: Step: 4201/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 4201/4907, loss=0.000000, lr=0.000006
Train batch 4300
Avg. loss per last 100 batches: 0.022934
Train batch 4300
Avg. loss per last 100 batches: 0.022934
Epoch: 28: Step: 4301/4907, loss=0.001091, lr=0.000006
Epoch: 28: Step: 4301/4907, loss=0.001091, lr=0.000006
Train batch 4400
Avg. loss per last 100 batches: 0.023803
Train batch 4400
Avg. loss per last 100 batches: 0.023803
Epoch: 28: Step: 4401/4907, loss=0.000002, lr=0.000006
Epoch: 28: Step: 4401/4907, loss=0.000002, lr=0.000006
Train batch 4500
Avg. loss per last 100 batches: 0.007897
Train batch 4500
Avg. loss per last 100 batches: 0.007897
Epoch: 28: Step: 4501/4907, loss=0.000005, lr=0.000006
Epoch: 28: Step: 4501/4907, loss=0.000005, lr=0.000006
Train batch 4600
Avg. loss per last 100 batches: 0.034293
Train batch 4600
Avg. loss per last 100 batches: 0.034293
Epoch: 28: Step: 4601/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 4601/4907, loss=0.000000, lr=0.000006
Train batch 4700
Avg. loss per last 100 batches: 0.030479
Train batch 4700
Avg. loss per last 100 batches: 0.030479
Epoch: 28: Step: 4701/4907, loss=0.000000, lr=0.000006
Epoch: 28: Step: 4701/4907, loss=0.000000, lr=0.000006
Train batch 4800
Avg. loss per last 100 batches: 0.041676
Train batch 4800
Avg. loss per last 100 batches: 0.041676
Epoch: 28: Step: 4801/4907, loss=0.000003, lr=0.000006
Epoch: 28: Step: 4801/4907, loss=0.000003, lr=0.000006
Train batch 4900
Avg. loss per last 100 batches: 0.017047
Train batch 4900
Avg. loss per last 100 batches: 0.017047
Epoch: 28: Step: 4901/4907, loss=0.000044, lr=0.000006
Epoch: 28: Step: 4901/4907, loss=0.000044, lr=0.000006
Validation: Epoch: 28 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 28 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 187.18723143032534, total questions=6516
Av.rank validation: average rank 187.18723143032534, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.28.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.28.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 187.18723143032534, total questions=6516
Av.rank validation: average rank 187.18723143032534, total questions=6516
Av Loss per epoch=0.026234
epoch total correct predictions=58468
***** Epoch 29 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.28.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.28.4907
Av Loss per epoch=0.026234
epoch total correct predictions=58468
***** Epoch 29 *****
Epoch: 29: Step: 1/4907, loss=0.000000, lr=0.000006
Epoch: 29: Step: 1/4907, loss=0.000000, lr=0.000006
Train batch 100
Avg. loss per last 100 batches: 0.020918
Train batch 100
Avg. loss per last 100 batches: 0.020918
Epoch: 29: Step: 101/4907, loss=0.218217, lr=0.000006
Epoch: 29: Step: 101/4907, loss=0.218217, lr=0.000006
Train batch 200
Avg. loss per last 100 batches: 0.020293
Train batch 200
Avg. loss per last 100 batches: 0.020293
Epoch: 29: Step: 201/4907, loss=0.001151, lr=0.000006
Epoch: 29: Step: 201/4907, loss=0.001151, lr=0.000006
Train batch 300
Avg. loss per last 100 batches: 0.015400
Train batch 300
Avg. loss per last 100 batches: 0.015400
Epoch: 29: Step: 301/4907, loss=0.000010, lr=0.000006
Epoch: 29: Step: 301/4907, loss=0.000010, lr=0.000006
Train batch 400
Avg. loss per last 100 batches: 0.008741
Train batch 400
Avg. loss per last 100 batches: 0.008741
Epoch: 29: Step: 401/4907, loss=0.115737, lr=0.000005
Epoch: 29: Step: 401/4907, loss=0.115737, lr=0.000005
Train batch 500
Avg. loss per last 100 batches: 0.009748
Train batch 500
Avg. loss per last 100 batches: 0.009748
Epoch: 29: Step: 501/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 501/4907, loss=0.000000, lr=0.000005
Train batch 600
Avg. loss per last 100 batches: 0.005133
Train batch 600
Avg. loss per last 100 batches: 0.005133
Epoch: 29: Step: 601/4907, loss=0.000002, lr=0.000005
Epoch: 29: Step: 601/4907, loss=0.000002, lr=0.000005
Train batch 700
Avg. loss per last 100 batches: 0.016159
Train batch 700
Avg. loss per last 100 batches: 0.016159
Epoch: 29: Step: 701/4907, loss=0.000092, lr=0.000005
Epoch: 29: Step: 701/4907, loss=0.000092, lr=0.000005
Train batch 800
Avg. loss per last 100 batches: 0.017932
Train batch 800
Avg. loss per last 100 batches: 0.017932
Epoch: 29: Step: 801/4907, loss=0.000038, lr=0.000005
Epoch: 29: Step: 801/4907, loss=0.000038, lr=0.000005
Train batch 900
Avg. loss per last 100 batches: 0.013248
Train batch 900
Avg. loss per last 100 batches: 0.013248
Epoch: 29: Step: 901/4907, loss=0.000002, lr=0.000005
Epoch: 29: Step: 901/4907, loss=0.000002, lr=0.000005
Train batch 1000
Avg. loss per last 100 batches: 0.026487
Train batch 1000
Avg. loss per last 100 batches: 0.026487
Epoch: 29: Step: 1001/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 1001/4907, loss=0.000000, lr=0.000005
Train batch 1100
Avg. loss per last 100 batches: 0.010892
Train batch 1100
Avg. loss per last 100 batches: 0.010892
Epoch: 29: Step: 1101/4907, loss=0.000918, lr=0.000005
Epoch: 29: Step: 1101/4907, loss=0.000918, lr=0.000005
Train batch 1200
Avg. loss per last 100 batches: 0.021657
Train batch 1200
Avg. loss per last 100 batches: 0.021657
Epoch: 29: Step: 1201/4907, loss=0.000002, lr=0.000005
Epoch: 29: Step: 1201/4907, loss=0.000002, lr=0.000005
Train batch 1300
Avg. loss per last 100 batches: 0.042783
Train batch 1300
Avg. loss per last 100 batches: 0.042783
Epoch: 29: Step: 1301/4907, loss=0.000001, lr=0.000005
Epoch: 29: Step: 1301/4907, loss=0.000001, lr=0.000005
Train batch 1400
Avg. loss per last 100 batches: 0.013117
Train batch 1400
Avg. loss per last 100 batches: 0.013117
Epoch: 29: Step: 1401/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 1401/4907, loss=0.000000, lr=0.000005
Train batch 1500
Avg. loss per last 100 batches: 0.012551
Train batch 1500
Avg. loss per last 100 batches: 0.012551
Epoch: 29: Step: 1501/4907, loss=0.000028, lr=0.000005
Epoch: 29: Step: 1501/4907, loss=0.000028, lr=0.000005
Train batch 1600
Avg. loss per last 100 batches: 0.004658
Train batch 1600
Avg. loss per last 100 batches: 0.004658
Epoch: 29: Step: 1601/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 1601/4907, loss=0.000000, lr=0.000005
Train batch 1700
Avg. loss per last 100 batches: 0.009857
Train batch 1700
Avg. loss per last 100 batches: 0.009857
Epoch: 29: Step: 1701/4907, loss=0.000001, lr=0.000005
Epoch: 29: Step: 1701/4907, loss=0.000001, lr=0.000005
Train batch 1800
Avg. loss per last 100 batches: 0.007768
Train batch 1800
Avg. loss per last 100 batches: 0.007768
Epoch: 29: Step: 1801/4907, loss=0.000640, lr=0.000005
Epoch: 29: Step: 1801/4907, loss=0.000640, lr=0.000005
Train batch 1900
Avg. loss per last 100 batches: 0.024725
Train batch 1900
Avg. loss per last 100 batches: 0.024725
Epoch: 29: Step: 1901/4907, loss=0.000034, lr=0.000005
Epoch: 29: Step: 1901/4907, loss=0.000034, lr=0.000005
Train batch 2000
Avg. loss per last 100 batches: 0.008921
Train batch 2000
Avg. loss per last 100 batches: 0.008921
Epoch: 29: Step: 2001/4907, loss=0.076437, lr=0.000005
Epoch: 29: Step: 2001/4907, loss=0.076437, lr=0.000005
Train batch 2100
Avg. loss per last 100 batches: 0.043457
Train batch 2100
Avg. loss per last 100 batches: 0.043457
Epoch: 29: Step: 2101/4907, loss=0.000003, lr=0.000005
Epoch: 29: Step: 2101/4907, loss=0.000003, lr=0.000005
Train batch 2200
Avg. loss per last 100 batches: 0.023051
Train batch 2200
Avg. loss per last 100 batches: 0.023051
Epoch: 29: Step: 2201/4907, loss=0.003982, lr=0.000005
Epoch: 29: Step: 2201/4907, loss=0.003982, lr=0.000005
Train batch 2300
Avg. loss per last 100 batches: 0.016200
Train batch 2300
Avg. loss per last 100 batches: 0.016200
Epoch: 29: Step: 2301/4907, loss=0.037023, lr=0.000005
Epoch: 29: Step: 2301/4907, loss=0.037023, lr=0.000005
Train batch 2400
Avg. loss per last 100 batches: 0.018963
Train batch 2400
Avg. loss per last 100 batches: 0.018963
Epoch: 29: Step: 2401/4907, loss=0.000153, lr=0.000005
Epoch: 29: Step: 2401/4907, loss=0.000153, lr=0.000005
Train batch 2500
Avg. loss per last 100 batches: 0.031711
Train batch 2500
Avg. loss per last 100 batches: 0.031711
Epoch: 29: Step: 2501/4907, loss=0.023369, lr=0.000005
Epoch: 29: Step: 2501/4907, loss=0.023369, lr=0.000005
Train batch 2600
Avg. loss per last 100 batches: 0.017759
Train batch 2600
Avg. loss per last 100 batches: 0.017759
Epoch: 29: Step: 2601/4907, loss=0.157652, lr=0.000005
Epoch: 29: Step: 2601/4907, loss=0.157652, lr=0.000005
Train batch 2700
Avg. loss per last 100 batches: 0.017994
Train batch 2700
Avg. loss per last 100 batches: 0.017994
Epoch: 29: Step: 2701/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 2701/4907, loss=0.000000, lr=0.000005
Train batch 2800
Avg. loss per last 100 batches: 0.011948
Train batch 2800
Avg. loss per last 100 batches: 0.011948
Epoch: 29: Step: 2801/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 2801/4907, loss=0.000000, lr=0.000005
Train batch 2900
Avg. loss per last 100 batches: 0.018528
Train batch 2900
Avg. loss per last 100 batches: 0.018528
Epoch: 29: Step: 2901/4907, loss=0.000711, lr=0.000005
Epoch: 29: Step: 2901/4907, loss=0.000711, lr=0.000005
Train batch 3000
Avg. loss per last 100 batches: 0.008306
Train batch 3000
Avg. loss per last 100 batches: 0.008306
Epoch: 29: Step: 3001/4907, loss=0.000637, lr=0.000005
Epoch: 29: Step: 3001/4907, loss=0.000637, lr=0.000005
Train batch 3100
Avg. loss per last 100 batches: 0.019945
Train batch 3100
Avg. loss per last 100 batches: 0.019945
Epoch: 29: Step: 3101/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 3101/4907, loss=0.000000, lr=0.000005
Train batch 3200
Avg. loss per last 100 batches: 0.038186
Train batch 3200
Avg. loss per last 100 batches: 0.038186
Epoch: 29: Step: 3201/4907, loss=0.000001, lr=0.000005
Epoch: 29: Step: 3201/4907, loss=0.000001, lr=0.000005
Train batch 3300
Avg. loss per last 100 batches: 0.009035
Train batch 3300
Avg. loss per last 100 batches: 0.009035
Epoch: 29: Step: 3301/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 3301/4907, loss=0.000000, lr=0.000005
Train batch 3400
Avg. loss per last 100 batches: 0.018817
Train batch 3400
Avg. loss per last 100 batches: 0.018817
Epoch: 29: Step: 3401/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 3401/4907, loss=0.000000, lr=0.000005
Train batch 3500
Avg. loss per last 100 batches: 0.029900
Train batch 3500
Avg. loss per last 100 batches: 0.029900
Epoch: 29: Step: 3501/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 3501/4907, loss=0.000000, lr=0.000005
Train batch 3600
Avg. loss per last 100 batches: 0.010482
Train batch 3600
Avg. loss per last 100 batches: 0.010482
Epoch: 29: Step: 3601/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 3601/4907, loss=0.000000, lr=0.000005
Train batch 3700
Avg. loss per last 100 batches: 0.015513
Train batch 3700
Avg. loss per last 100 batches: 0.015513
Epoch: 29: Step: 3701/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 3701/4907, loss=0.000000, lr=0.000005
Train batch 3800
Avg. loss per last 100 batches: 0.023189
Train batch 3800
Avg. loss per last 100 batches: 0.023189
Epoch: 29: Step: 3801/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 3801/4907, loss=0.000000, lr=0.000005
Train batch 3900
Avg. loss per last 100 batches: 0.028248
Train batch 3900
Avg. loss per last 100 batches: 0.028248
Epoch: 29: Step: 3901/4907, loss=0.000011, lr=0.000005
Epoch: 29: Step: 3901/4907, loss=0.000011, lr=0.000005
Train batch 4000
Avg. loss per last 100 batches: 0.016302
Train batch 4000
Avg. loss per last 100 batches: 0.016302
Epoch: 29: Step: 4001/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 4001/4907, loss=0.000000, lr=0.000005
Train batch 4100
Avg. loss per last 100 batches: 0.026132
Train batch 4100
Avg. loss per last 100 batches: 0.026132
Epoch: 29: Step: 4101/4907, loss=0.000259, lr=0.000005
Epoch: 29: Step: 4101/4907, loss=0.000259, lr=0.000005
Train batch 4200
Avg. loss per last 100 batches: 0.037102
Train batch 4200
Avg. loss per last 100 batches: 0.037102
Epoch: 29: Step: 4201/4907, loss=0.000182, lr=0.000005
Epoch: 29: Step: 4201/4907, loss=0.000182, lr=0.000005
Train batch 4300
Avg. loss per last 100 batches: 0.017533
Train batch 4300
Avg. loss per last 100 batches: 0.017533
Epoch: 29: Step: 4301/4907, loss=0.000002, lr=0.000005
Epoch: 29: Step: 4301/4907, loss=0.000002, lr=0.000005
Train batch 4400
Avg. loss per last 100 batches: 0.031162
Train batch 4400
Avg. loss per last 100 batches: 0.031162
Epoch: 29: Step: 4401/4907, loss=0.000010, lr=0.000005
Epoch: 29: Step: 4401/4907, loss=0.000010, lr=0.000005
Train batch 4500
Avg. loss per last 100 batches: 0.036081
Train batch 4500
Avg. loss per last 100 batches: 0.036081
Epoch: 29: Step: 4501/4907, loss=0.000232, lr=0.000005
Epoch: 29: Step: 4501/4907, loss=0.000232, lr=0.000005
Train batch 4600
Avg. loss per last 100 batches: 0.014323
Train batch 4600
Avg. loss per last 100 batches: 0.014323
Epoch: 29: Step: 4601/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 4601/4907, loss=0.000000, lr=0.000005
Train batch 4700
Avg. loss per last 100 batches: 0.022955
Train batch 4700
Avg. loss per last 100 batches: 0.022955
Epoch: 29: Step: 4701/4907, loss=0.000000, lr=0.000005
Epoch: 29: Step: 4701/4907, loss=0.000000, lr=0.000005
Train batch 4800
Avg. loss per last 100 batches: 0.017632
Train batch 4800
Avg. loss per last 100 batches: 0.017632
Epoch: 29: Step: 4801/4907, loss=0.012154, lr=0.000005
Epoch: 29: Step: 4801/4907, loss=0.012154, lr=0.000005
Train batch 4900
Avg. loss per last 100 batches: 0.030216
Train batch 4900
Avg. loss per last 100 batches: 0.030216
Epoch: 29: Step: 4901/4907, loss=0.361346, lr=0.000005
Epoch: 29: Step: 4901/4907, loss=0.361346, lr=0.000005
Validation: Epoch: 29 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 29 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 155.3951810926949, total questions=6516
Av.rank validation: average rank 155.3951810926949, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.29.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.29.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.29.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 155.3951810926949, total questions=6516
Av.rank validation: average rank 155.3951810926949, total questions=6516
Av Loss per epoch=0.019702
epoch total correct predictions=58544
***** Epoch 30 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.29.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.29.4907
Av Loss per epoch=0.019702
epoch total correct predictions=58544
***** Epoch 30 *****
Epoch: 30: Step: 1/4907, loss=0.000101, lr=0.000005
Epoch: 30: Step: 1/4907, loss=0.000101, lr=0.000005
Train batch 100
Avg. loss per last 100 batches: 0.022257
Train batch 100
Avg. loss per last 100 batches: 0.022257
Epoch: 30: Step: 101/4907, loss=0.000007, lr=0.000005
Epoch: 30: Step: 101/4907, loss=0.000007, lr=0.000005
Train batch 200
Avg. loss per last 100 batches: 0.028176
Train batch 200
Avg. loss per last 100 batches: 0.028176
Epoch: 30: Step: 201/4907, loss=0.000001, lr=0.000005
Epoch: 30: Step: 201/4907, loss=0.000001, lr=0.000005
Train batch 300
Avg. loss per last 100 batches: 0.027493
Train batch 300
Avg. loss per last 100 batches: 0.027493
Epoch: 30: Step: 301/4907, loss=0.038808, lr=0.000005
Epoch: 30: Step: 301/4907, loss=0.038808, lr=0.000005
Train batch 400
Avg. loss per last 100 batches: 0.014485
Train batch 400
Avg. loss per last 100 batches: 0.014485
Epoch: 30: Step: 401/4907, loss=0.002481, lr=0.000005
Epoch: 30: Step: 401/4907, loss=0.002481, lr=0.000005
Train batch 500
Avg. loss per last 100 batches: 0.012767
Train batch 500
Avg. loss per last 100 batches: 0.012767
Epoch: 30: Step: 501/4907, loss=0.000072, lr=0.000005
Epoch: 30: Step: 501/4907, loss=0.000072, lr=0.000005
Train batch 600
Avg. loss per last 100 batches: 0.008818
Train batch 600
Avg. loss per last 100 batches: 0.008818
Epoch: 30: Step: 601/4907, loss=0.185487, lr=0.000005
Epoch: 30: Step: 601/4907, loss=0.185487, lr=0.000005
Train batch 700
Avg. loss per last 100 batches: 0.046916
Train batch 700
Avg. loss per last 100 batches: 0.046916
Epoch: 30: Step: 701/4907, loss=0.000769, lr=0.000005
Epoch: 30: Step: 701/4907, loss=0.000769, lr=0.000005
Train batch 800
Avg. loss per last 100 batches: 0.011423
Train batch 800
Avg. loss per last 100 batches: 0.011423
Epoch: 30: Step: 801/4907, loss=0.003735, lr=0.000005
Epoch: 30: Step: 801/4907, loss=0.003735, lr=0.000005
Train batch 900
Avg. loss per last 100 batches: 0.018405
Train batch 900
Avg. loss per last 100 batches: 0.018405
Epoch: 30: Step: 901/4907, loss=0.000030, lr=0.000005
Epoch: 30: Step: 901/4907, loss=0.000030, lr=0.000005
Train batch 1000
Avg. loss per last 100 batches: 0.027863
Train batch 1000
Avg. loss per last 100 batches: 0.027863
Epoch: 30: Step: 1001/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 1001/4907, loss=0.000000, lr=0.000005
Train batch 1100
Avg. loss per last 100 batches: 0.031539
Train batch 1100
Avg. loss per last 100 batches: 0.031539
Epoch: 30: Step: 1101/4907, loss=0.000380, lr=0.000005
Epoch: 30: Step: 1101/4907, loss=0.000380, lr=0.000005
Train batch 1200
Avg. loss per last 100 batches: 0.022542
Train batch 1200
Avg. loss per last 100 batches: 0.022542
Epoch: 30: Step: 1201/4907, loss=0.000059, lr=0.000005
Epoch: 30: Step: 1201/4907, loss=0.000059, lr=0.000005
Train batch 1300
Avg. loss per last 100 batches: 0.017685
Train batch 1300
Avg. loss per last 100 batches: 0.017685
Epoch: 30: Step: 1301/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 1301/4907, loss=0.000000, lr=0.000005
Train batch 1400
Avg. loss per last 100 batches: 0.016971
Train batch 1400
Avg. loss per last 100 batches: 0.016971
Epoch: 30: Step: 1401/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 1401/4907, loss=0.000000, lr=0.000005
Train batch 1500
Avg. loss per last 100 batches: 0.014278
Train batch 1500
Avg. loss per last 100 batches: 0.014278
Epoch: 30: Step: 1501/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 1501/4907, loss=0.000000, lr=0.000005
Train batch 1600
Avg. loss per last 100 batches: 0.005166
Train batch 1600
Avg. loss per last 100 batches: 0.005166
Epoch: 30: Step: 1601/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 1601/4907, loss=0.000000, lr=0.000005
Train batch 1700
Avg. loss per last 100 batches: 0.016127
Train batch 1700
Avg. loss per last 100 batches: 0.016127
Epoch: 30: Step: 1701/4907, loss=0.000108, lr=0.000005
Epoch: 30: Step: 1701/4907, loss=0.000108, lr=0.000005
Train batch 1800
Avg. loss per last 100 batches: 0.028181
Train batch 1800
Avg. loss per last 100 batches: 0.028181
Epoch: 30: Step: 1801/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 1801/4907, loss=0.000000, lr=0.000005
Train batch 1900
Avg. loss per last 100 batches: 0.015668
Train batch 1900
Avg. loss per last 100 batches: 0.015668
Epoch: 30: Step: 1901/4907, loss=0.000682, lr=0.000005
Epoch: 30: Step: 1901/4907, loss=0.000682, lr=0.000005
Train batch 2000
Avg. loss per last 100 batches: 0.022145
Train batch 2000
Avg. loss per last 100 batches: 0.022145
Epoch: 30: Step: 2001/4907, loss=0.000782, lr=0.000005
Epoch: 30: Step: 2001/4907, loss=0.000782, lr=0.000005
Train batch 2100
Avg. loss per last 100 batches: 0.020827
Train batch 2100
Avg. loss per last 100 batches: 0.020827
Epoch: 30: Step: 2101/4907, loss=0.000232, lr=0.000005
Epoch: 30: Step: 2101/4907, loss=0.000232, lr=0.000005
Train batch 2200
Avg. loss per last 100 batches: 0.015414
Train batch 2200
Avg. loss per last 100 batches: 0.015414
Epoch: 30: Step: 2201/4907, loss=0.001566, lr=0.000005
Epoch: 30: Step: 2201/4907, loss=0.001566, lr=0.000005
Train batch 2300
Avg. loss per last 100 batches: 0.027463
Train batch 2300
Avg. loss per last 100 batches: 0.027463
Epoch: 30: Step: 2301/4907, loss=0.000003, lr=0.000005
Epoch: 30: Step: 2301/4907, loss=0.000003, lr=0.000005
Train batch 2400
Avg. loss per last 100 batches: 0.030970
Train batch 2400
Avg. loss per last 100 batches: 0.030970
Epoch: 30: Step: 2401/4907, loss=0.003577, lr=0.000005
Epoch: 30: Step: 2401/4907, loss=0.003577, lr=0.000005
Train batch 2500
Avg. loss per last 100 batches: 0.019250
Train batch 2500
Avg. loss per last 100 batches: 0.019250
Epoch: 30: Step: 2501/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 2501/4907, loss=0.000000, lr=0.000005
Train batch 2600
Avg. loss per last 100 batches: 0.036012
Train batch 2600
Avg. loss per last 100 batches: 0.036012
Epoch: 30: Step: 2601/4907, loss=0.000013, lr=0.000005
Epoch: 30: Step: 2601/4907, loss=0.000013, lr=0.000005
Train batch 2700
Avg. loss per last 100 batches: 0.028315
Train batch 2700
Avg. loss per last 100 batches: 0.028315
Epoch: 30: Step: 2701/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 2701/4907, loss=0.000000, lr=0.000005
Train batch 2800
Avg. loss per last 100 batches: 0.032772
Train batch 2800
Avg. loss per last 100 batches: 0.032772
Epoch: 30: Step: 2801/4907, loss=0.000236, lr=0.000005
Epoch: 30: Step: 2801/4907, loss=0.000236, lr=0.000005
Train batch 2900
Avg. loss per last 100 batches: 0.015733
Train batch 2900
Avg. loss per last 100 batches: 0.015733
Epoch: 30: Step: 2901/4907, loss=0.000020, lr=0.000005
Epoch: 30: Step: 2901/4907, loss=0.000020, lr=0.000005
Train batch 3000
Avg. loss per last 100 batches: 0.010760
Train batch 3000
Avg. loss per last 100 batches: 0.010760
Epoch: 30: Step: 3001/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 3001/4907, loss=0.000000, lr=0.000005
Train batch 3100
Avg. loss per last 100 batches: 0.025120
Train batch 3100
Avg. loss per last 100 batches: 0.025120
Epoch: 30: Step: 3101/4907, loss=0.000176, lr=0.000005
Epoch: 30: Step: 3101/4907, loss=0.000176, lr=0.000005
Train batch 3200
Avg. loss per last 100 batches: 0.015619
Train batch 3200
Avg. loss per last 100 batches: 0.015619
Epoch: 30: Step: 3201/4907, loss=0.000866, lr=0.000005
Epoch: 30: Step: 3201/4907, loss=0.000866, lr=0.000005
Train batch 3300
Avg. loss per last 100 batches: 0.025520
Train batch 3300
Avg. loss per last 100 batches: 0.025520
Epoch: 30: Step: 3301/4907, loss=0.002112, lr=0.000005
Epoch: 30: Step: 3301/4907, loss=0.002112, lr=0.000005
Train batch 3400
Avg. loss per last 100 batches: 0.027622
Train batch 3400
Avg. loss per last 100 batches: 0.027622
Epoch: 30: Step: 3401/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 3401/4907, loss=0.000000, lr=0.000005
Train batch 3500
Avg. loss per last 100 batches: 0.007928
Train batch 3500
Avg. loss per last 100 batches: 0.007928
Epoch: 30: Step: 3501/4907, loss=0.000007, lr=0.000005
Epoch: 30: Step: 3501/4907, loss=0.000007, lr=0.000005
Train batch 3600
Avg. loss per last 100 batches: 0.015368
Train batch 3600
Avg. loss per last 100 batches: 0.015368
Epoch: 30: Step: 3601/4907, loss=0.000002, lr=0.000005
Epoch: 30: Step: 3601/4907, loss=0.000002, lr=0.000005
Train batch 3700
Avg. loss per last 100 batches: 0.015589
Train batch 3700
Avg. loss per last 100 batches: 0.015589
Epoch: 30: Step: 3701/4907, loss=0.000001, lr=0.000005
Epoch: 30: Step: 3701/4907, loss=0.000001, lr=0.000005
Train batch 3800
Avg. loss per last 100 batches: 0.035293
Train batch 3800
Avg. loss per last 100 batches: 0.035293
Epoch: 30: Step: 3801/4907, loss=0.015538, lr=0.000005
Epoch: 30: Step: 3801/4907, loss=0.015538, lr=0.000005
Train batch 3900
Avg. loss per last 100 batches: 0.021302
Train batch 3900
Avg. loss per last 100 batches: 0.021302
Epoch: 30: Step: 3901/4907, loss=0.000021, lr=0.000005
Epoch: 30: Step: 3901/4907, loss=0.000021, lr=0.000005
Train batch 4000
Avg. loss per last 100 batches: 0.015331
Train batch 4000
Avg. loss per last 100 batches: 0.015331
Epoch: 30: Step: 4001/4907, loss=0.000063, lr=0.000005
Epoch: 30: Step: 4001/4907, loss=0.000063, lr=0.000005
Train batch 4100
Avg. loss per last 100 batches: 0.005590
Train batch 4100
Avg. loss per last 100 batches: 0.005590
Epoch: 30: Step: 4101/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 4101/4907, loss=0.000000, lr=0.000005
Train batch 4200
Avg. loss per last 100 batches: 0.025177
Train batch 4200
Avg. loss per last 100 batches: 0.025177
Epoch: 30: Step: 4201/4907, loss=0.000794, lr=0.000005
Epoch: 30: Step: 4201/4907, loss=0.000794, lr=0.000005
Train batch 4300
Avg. loss per last 100 batches: 0.015639
Train batch 4300
Avg. loss per last 100 batches: 0.015639
Epoch: 30: Step: 4301/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 4301/4907, loss=0.000000, lr=0.000005
Train batch 4400
Avg. loss per last 100 batches: 0.023324
Train batch 4400
Avg. loss per last 100 batches: 0.023324
Epoch: 30: Step: 4401/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 4401/4907, loss=0.000000, lr=0.000005
Train batch 4500
Avg. loss per last 100 batches: 0.022452
Train batch 4500
Avg. loss per last 100 batches: 0.022452
Epoch: 30: Step: 4501/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 4501/4907, loss=0.000000, lr=0.000005
Train batch 4600
Avg. loss per last 100 batches: 0.021566
Train batch 4600
Avg. loss per last 100 batches: 0.021566
Epoch: 30: Step: 4601/4907, loss=0.000003, lr=0.000005
Epoch: 30: Step: 4601/4907, loss=0.000003, lr=0.000005
Train batch 4700
Avg. loss per last 100 batches: 0.028931
Train batch 4700
Avg. loss per last 100 batches: 0.028931
Epoch: 30: Step: 4701/4907, loss=0.000001, lr=0.000005
Epoch: 30: Step: 4701/4907, loss=0.000001, lr=0.000005
Train batch 4800
Avg. loss per last 100 batches: 0.015886
Train batch 4800
Avg. loss per last 100 batches: 0.015886
Epoch: 30: Step: 4801/4907, loss=0.000000, lr=0.000005
Epoch: 30: Step: 4801/4907, loss=0.000000, lr=0.000005
Train batch 4900
Avg. loss per last 100 batches: 0.020702
Train batch 4900
Avg. loss per last 100 batches: 0.020702
Epoch: 30: Step: 4901/4907, loss=0.267382, lr=0.000005
Epoch: 30: Step: 4901/4907, loss=0.267382, lr=0.000005
Validation: Epoch: 30 Step: 4907/4907
Validation: Epoch: 30 Step: 4907/4907
Average rank validation ...
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 164.97820748925722, total questions=6516
Av.rank validation: average rank 164.97820748925722, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.30.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.30.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 164.97820748925722, total questions=6516
Av.rank validation: average rank 164.97820748925722, total questions=6516
Av Loss per epoch=0.021053
epoch total correct predictions=58543
***** Epoch 31 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.30.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.30.4907
Av Loss per epoch=0.021053
epoch total correct predictions=58543
***** Epoch 31 *****
Epoch: 31: Step: 1/4907, loss=0.000001, lr=0.000005
Epoch: 31: Step: 1/4907, loss=0.000001, lr=0.000005
Train batch 100
Avg. loss per last 100 batches: 0.029686
Train batch 100
Avg. loss per last 100 batches: 0.029686
Epoch: 31: Step: 101/4907, loss=0.002407, lr=0.000005
Epoch: 31: Step: 101/4907, loss=0.002407, lr=0.000005
Train batch 200
Avg. loss per last 100 batches: 0.031060
Train batch 200
Avg. loss per last 100 batches: 0.031060
Epoch: 31: Step: 201/4907, loss=0.000000, lr=0.000005
Epoch: 31: Step: 201/4907, loss=0.000000, lr=0.000005
Train batch 300
Avg. loss per last 100 batches: 0.014205
Train batch 300
Avg. loss per last 100 batches: 0.014205
Epoch: 31: Step: 301/4907, loss=0.000008, lr=0.000004
Epoch: 31: Step: 301/4907, loss=0.000008, lr=0.000004
Train batch 400
Avg. loss per last 100 batches: 0.041517
Train batch 400
Avg. loss per last 100 batches: 0.041517
Epoch: 31: Step: 401/4907, loss=0.005466, lr=0.000004
Epoch: 31: Step: 401/4907, loss=0.005466, lr=0.000004
Train batch 500
Avg. loss per last 100 batches: 0.039280
Train batch 500
Avg. loss per last 100 batches: 0.039280
Epoch: 31: Step: 501/4907, loss=0.000003, lr=0.000004
Epoch: 31: Step: 501/4907, loss=0.000003, lr=0.000004
Train batch 600
Avg. loss per last 100 batches: 0.024199
Train batch 600
Avg. loss per last 100 batches: 0.024199
Epoch: 31: Step: 601/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 601/4907, loss=0.000000, lr=0.000004
Train batch 700
Avg. loss per last 100 batches: 0.029058
Train batch 700
Avg. loss per last 100 batches: 0.029058
Epoch: 31: Step: 701/4907, loss=0.066070, lr=0.000004
Epoch: 31: Step: 701/4907, loss=0.066070, lr=0.000004
Train batch 800
Avg. loss per last 100 batches: 0.016109
Train batch 800
Avg. loss per last 100 batches: 0.016109
Epoch: 31: Step: 801/4907, loss=0.000310, lr=0.000004
Epoch: 31: Step: 801/4907, loss=0.000310, lr=0.000004
Train batch 900
Avg. loss per last 100 batches: 0.026422
Train batch 900
Avg. loss per last 100 batches: 0.026422
Epoch: 31: Step: 901/4907, loss=0.000001, lr=0.000004
Epoch: 31: Step: 901/4907, loss=0.000001, lr=0.000004
Train batch 1000
Avg. loss per last 100 batches: 0.035487
Train batch 1000
Avg. loss per last 100 batches: 0.035487
Epoch: 31: Step: 1001/4907, loss=0.009178, lr=0.000004
Epoch: 31: Step: 1001/4907, loss=0.009178, lr=0.000004
Train batch 1100
Avg. loss per last 100 batches: 0.007699
Train batch 1100
Avg. loss per last 100 batches: 0.007699
Epoch: 31: Step: 1101/4907, loss=0.000002, lr=0.000004
Epoch: 31: Step: 1101/4907, loss=0.000002, lr=0.000004
Train batch 1200
Avg. loss per last 100 batches: 0.039934
Train batch 1200
Avg. loss per last 100 batches: 0.039934
Epoch: 31: Step: 1201/4907, loss=0.000354, lr=0.000004
Epoch: 31: Step: 1201/4907, loss=0.000354, lr=0.000004
Train batch 1300
Avg. loss per last 100 batches: 0.010297
Train batch 1300
Avg. loss per last 100 batches: 0.010297
Epoch: 31: Step: 1301/4907, loss=0.005112, lr=0.000004
Epoch: 31: Step: 1301/4907, loss=0.005112, lr=0.000004
Train batch 1400
Avg. loss per last 100 batches: 0.005603
Train batch 1400
Avg. loss per last 100 batches: 0.005603
Epoch: 31: Step: 1401/4907, loss=0.000002, lr=0.000004
Epoch: 31: Step: 1401/4907, loss=0.000002, lr=0.000004
Train batch 1500
Train batch 1500
Avg. loss per last 100 batches: 0.018535
Avg. loss per last 100 batches: 0.018535
Epoch: 31: Step: 1501/4907, loss=0.000005, lr=0.000004
Epoch: 31: Step: 1501/4907, loss=0.000005, lr=0.000004
Train batch 1600
Avg. loss per last 100 batches: 0.013026
Train batch 1600
Avg. loss per last 100 batches: 0.013026
Epoch: 31: Step: 1601/4907, loss=0.000009, lr=0.000004
Epoch: 31: Step: 1601/4907, loss=0.000009, lr=0.000004
Train batch 1700
Avg. loss per last 100 batches: 0.003836
Train batch 1700
Avg. loss per last 100 batches: 0.003836
Epoch: 31: Step: 1701/4907, loss=0.000496, lr=0.000004
Epoch: 31: Step: 1701/4907, loss=0.000496, lr=0.000004
Train batch 1800
Avg. loss per last 100 batches: 0.020100
Train batch 1800
Avg. loss per last 100 batches: 0.020100
Epoch: 31: Step: 1801/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 1801/4907, loss=0.000000, lr=0.000004
Train batch 1900
Avg. loss per last 100 batches: 0.017148
Train batch 1900
Avg. loss per last 100 batches: 0.017148
Epoch: 31: Step: 1901/4907, loss=0.000728, lr=0.000004
Epoch: 31: Step: 1901/4907, loss=0.000728, lr=0.000004
Train batch 2000
Avg. loss per last 100 batches: 0.008500
Train batch 2000
Avg. loss per last 100 batches: 0.008500
Epoch: 31: Step: 2001/4907, loss=0.000002, lr=0.000004
Epoch: 31: Step: 2001/4907, loss=0.000002, lr=0.000004
Train batch 2100
Avg. loss per last 100 batches: 0.017011
Train batch 2100
Avg. loss per last 100 batches: 0.017011
Epoch: 31: Step: 2101/4907, loss=0.000147, lr=0.000004
Epoch: 31: Step: 2101/4907, loss=0.000147, lr=0.000004
Train batch 2200
Avg. loss per last 100 batches: 0.017689
Train batch 2200
Avg. loss per last 100 batches: 0.017689
Epoch: 31: Step: 2201/4907, loss=0.001770, lr=0.000004
Epoch: 31: Step: 2201/4907, loss=0.001770, lr=0.000004
Train batch 2300
Avg. loss per last 100 batches: 0.024107
Train batch 2300
Avg. loss per last 100 batches: 0.024107
Epoch: 31: Step: 2301/4907, loss=0.000002, lr=0.000004
Epoch: 31: Step: 2301/4907, loss=0.000002, lr=0.000004
Train batch 2400
Avg. loss per last 100 batches: 0.021613
Train batch 2400
Avg. loss per last 100 batches: 0.021613
Epoch: 31: Step: 2401/4907, loss=0.000001, lr=0.000004
Epoch: 31: Step: 2401/4907, loss=0.000001, lr=0.000004
Train batch 2500
Avg. loss per last 100 batches: 0.012643
Train batch 2500
Avg. loss per last 100 batches: 0.012643
Epoch: 31: Step: 2501/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 2501/4907, loss=0.000000, lr=0.000004
Train batch 2600
Avg. loss per last 100 batches: 0.009224
Train batch 2600
Avg. loss per last 100 batches: 0.009224
Epoch: 31: Step: 2601/4907, loss=0.000385, lr=0.000004
Epoch: 31: Step: 2601/4907, loss=0.000385, lr=0.000004
Train batch 2700
Avg. loss per last 100 batches: 0.014781
Train batch 2700
Avg. loss per last 100 batches: 0.014781
Epoch: 31: Step: 2701/4907, loss=0.000001, lr=0.000004
Epoch: 31: Step: 2701/4907, loss=0.000001, lr=0.000004
Train batch 2800
Avg. loss per last 100 batches: 0.011661
Train batch 2800
Avg. loss per last 100 batches: 0.011661
Epoch: 31: Step: 2801/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 2801/4907, loss=0.000000, lr=0.000004
Train batch 2900
Avg. loss per last 100 batches: 0.006029
Train batch 2900
Avg. loss per last 100 batches: 0.006029
Epoch: 31: Step: 2901/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 2901/4907, loss=0.000000, lr=0.000004
Train batch 3000
Avg. loss per last 100 batches: 0.015257
Train batch 3000
Avg. loss per last 100 batches: 0.015257
Epoch: 31: Step: 3001/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 3001/4907, loss=0.000000, lr=0.000004
Train batch 3100
Avg. loss per last 100 batches: 0.026992
Train batch 3100
Avg. loss per last 100 batches: 0.026992
Epoch: 31: Step: 3101/4907, loss=0.000001, lr=0.000004
Epoch: 31: Step: 3101/4907, loss=0.000001, lr=0.000004
Train batch 3200
Avg. loss per last 100 batches: 0.008309
Train batch 3200
Avg. loss per last 100 batches: 0.008309
Epoch: 31: Step: 3201/4907, loss=0.000001, lr=0.000004
Epoch: 31: Step: 3201/4907, loss=0.000001, lr=0.000004
Train batch 3300
Avg. loss per last 100 batches: 0.027738
Train batch 3300
Avg. loss per last 100 batches: 0.027738
Epoch: 31: Step: 3301/4907, loss=0.000176, lr=0.000004
Epoch: 31: Step: 3301/4907, loss=0.000176, lr=0.000004
Train batch 3400
Avg. loss per last 100 batches: 0.015973
Train batch 3400
Avg. loss per last 100 batches: 0.015973
Epoch: 31: Step: 3401/4907, loss=0.000007, lr=0.000004
Epoch: 31: Step: 3401/4907, loss=0.000007, lr=0.000004
Train batch 3500
Avg. loss per last 100 batches: 0.017680
Train batch 3500
Avg. loss per last 100 batches: 0.017680
Epoch: 31: Step: 3501/4907, loss=0.079319, lr=0.000004
Epoch: 31: Step: 3501/4907, loss=0.079319, lr=0.000004
Train batch 3600
Avg. loss per last 100 batches: 0.018586
Train batch 3600
Avg. loss per last 100 batches: 0.018586
Epoch: 31: Step: 3601/4907, loss=0.000049, lr=0.000004
Epoch: 31: Step: 3601/4907, loss=0.000049, lr=0.000004
Train batch 3700
Avg. loss per last 100 batches: 0.003538
Train batch 3700
Avg. loss per last 100 batches: 0.003538
Epoch: 31: Step: 3701/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 3701/4907, loss=0.000000, lr=0.000004
Train batch 3800
Avg. loss per last 100 batches: 0.029315
Train batch 3800
Avg. loss per last 100 batches: 0.029315
Epoch: 31: Step: 3801/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 3801/4907, loss=0.000000, lr=0.000004
Train batch 3900
Avg. loss per last 100 batches: 0.005835
Train batch 3900
Avg. loss per last 100 batches: 0.005835
Epoch: 31: Step: 3901/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 3901/4907, loss=0.000000, lr=0.000004
Train batch 4000
Avg. loss per last 100 batches: 0.013631
Train batch 4000
Avg. loss per last 100 batches: 0.013631
Epoch: 31: Step: 4001/4907, loss=0.140784, lr=0.000004
Epoch: 31: Step: 4001/4907, loss=0.140784, lr=0.000004
Train batch 4100
Avg. loss per last 100 batches: 0.029055
Train batch 4100
Avg. loss per last 100 batches: 0.029055
Epoch: 31: Step: 4101/4907, loss=0.235609, lr=0.000004
Epoch: 31: Step: 4101/4907, loss=0.235609, lr=0.000004
Train batch 4200
Avg. loss per last 100 batches: 0.010455
Train batch 4200
Avg. loss per last 100 batches: 0.010455
Epoch: 31: Step: 4201/4907, loss=0.000182, lr=0.000004
Epoch: 31: Step: 4201/4907, loss=0.000182, lr=0.000004
Train batch 4300
Avg. loss per last 100 batches: 0.029319
Train batch 4300
Avg. loss per last 100 batches: 0.029319
Epoch: 31: Step: 4301/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 4301/4907, loss=0.000000, lr=0.000004
Train batch 4400
Avg. loss per last 100 batches: 0.015637
Train batch 4400
Avg. loss per last 100 batches: 0.015637
Epoch: 31: Step: 4401/4907, loss=0.017100, lr=0.000004
Epoch: 31: Step: 4401/4907, loss=0.017100, lr=0.000004
Train batch 4500
Avg. loss per last 100 batches: 0.018313
Train batch 4500
Avg. loss per last 100 batches: 0.018313
Epoch: 31: Step: 4501/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 4501/4907, loss=0.000000, lr=0.000004
Train batch 4600
Avg. loss per last 100 batches: 0.023093
Train batch 4600
Avg. loss per last 100 batches: 0.023093
Epoch: 31: Step: 4601/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 4601/4907, loss=0.000000, lr=0.000004
Train batch 4700
Avg. loss per last 100 batches: 0.010702
Train batch 4700
Avg. loss per last 100 batches: 0.010702
Epoch: 31: Step: 4701/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 4701/4907, loss=0.000000, lr=0.000004
Train batch 4800
Avg. loss per last 100 batches: 0.035849
Train batch 4800
Avg. loss per last 100 batches: 0.035849
Epoch: 31: Step: 4801/4907, loss=0.000000, lr=0.000004
Epoch: 31: Step: 4801/4907, loss=0.000000, lr=0.000004
Train batch 4900
Avg. loss per last 100 batches: 0.015343
Train batch 4900
Avg. loss per last 100 batches: 0.015343
Epoch: 31: Step: 4901/4907, loss=0.000048, lr=0.000004
Epoch: 31: Step: 4901/4907, loss=0.000048, lr=0.000004
Validation: Epoch: 31 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 31 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 181.97882136279927, total questions=6516
Av.rank validation: average rank 181.97882136279927, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.31.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.31.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 181.97882136279927, total questions=6516
Av.rank validation: average rank 181.97882136279927, total questions=6516
Av Loss per epoch=0.019097
epoch total correct predictions=58566
***** Epoch 32 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.31.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.31.4907
Av Loss per epoch=0.019097
epoch total correct predictions=58566
***** Epoch 32 *****
Epoch: 32: Step: 1/4907, loss=0.000003, lr=0.000004
Epoch: 32: Step: 1/4907, loss=0.000003, lr=0.000004
Train batch 100
Avg. loss per last 100 batches: 0.013667
Train batch 100
Avg. loss per last 100 batches: 0.013667
Epoch: 32: Step: 101/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 101/4907, loss=0.000000, lr=0.000004
Train batch 200
Avg. loss per last 100 batches: 0.017289
Train batch 200
Avg. loss per last 100 batches: 0.017289
Epoch: 32: Step: 201/4907, loss=0.001200, lr=0.000004
Epoch: 32: Step: 201/4907, loss=0.001200, lr=0.000004
Train batch 300
Avg. loss per last 100 batches: 0.045750
Train batch 300
Avg. loss per last 100 batches: 0.045750
Epoch: 32: Step: 301/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 301/4907, loss=0.000000, lr=0.000004
Train batch 400
Avg. loss per last 100 batches: 0.019264
Train batch 400
Avg. loss per last 100 batches: 0.019264
Epoch: 32: Step: 401/4907, loss=0.000012, lr=0.000004
Epoch: 32: Step: 401/4907, loss=0.000012, lr=0.000004
Train batch 500
Avg. loss per last 100 batches: 0.025969
Train batch 500
Avg. loss per last 100 batches: 0.025969
Epoch: 32: Step: 501/4907, loss=0.000105, lr=0.000004
Epoch: 32: Step: 501/4907, loss=0.000105, lr=0.000004
Train batch 600
Avg. loss per last 100 batches: 0.024544
Train batch 600
Avg. loss per last 100 batches: 0.024544
Epoch: 32: Step: 601/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 601/4907, loss=0.000000, lr=0.000004
Train batch 700
Avg. loss per last 100 batches: 0.030724
Train batch 700
Avg. loss per last 100 batches: 0.030724
Epoch: 32: Step: 701/4907, loss=0.000063, lr=0.000004
Epoch: 32: Step: 701/4907, loss=0.000063, lr=0.000004
Train batch 800
Avg. loss per last 100 batches: 0.007761
Train batch 800
Avg. loss per last 100 batches: 0.007761
Epoch: 32: Step: 801/4907, loss=0.000001, lr=0.000004
Epoch: 32: Step: 801/4907, loss=0.000001, lr=0.000004
Train batch 900
Avg. loss per last 100 batches: 0.056937
Train batch 900
Avg. loss per last 100 batches: 0.056937
Epoch: 32: Step: 901/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 901/4907, loss=0.000000, lr=0.000004
Train batch 1000
Avg. loss per last 100 batches: 0.018664
Train batch 1000
Avg. loss per last 100 batches: 0.018664
Epoch: 32: Step: 1001/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 1001/4907, loss=0.000000, lr=0.000004
Train batch 1100
Avg. loss per last 100 batches: 0.020444
Train batch 1100
Avg. loss per last 100 batches: 0.020444
Epoch: 32: Step: 1101/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 1101/4907, loss=0.000000, lr=0.000004
Train batch 1200
Avg. loss per last 100 batches: 0.022888
Train batch 1200
Avg. loss per last 100 batches: 0.022888
Epoch: 32: Step: 1201/4907, loss=0.000501, lr=0.000004
Epoch: 32: Step: 1201/4907, loss=0.000501, lr=0.000004
Train batch 1300
Avg. loss per last 100 batches: 0.014834
Train batch 1300
Avg. loss per last 100 batches: 0.014834
Epoch: 32: Step: 1301/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 1301/4907, loss=0.000000, lr=0.000004
Train batch 1400
Avg. loss per last 100 batches: 0.012336
Train batch 1400
Avg. loss per last 100 batches: 0.012336
Epoch: 32: Step: 1401/4907, loss=0.056696, lr=0.000004
Epoch: 32: Step: 1401/4907, loss=0.056696, lr=0.000004
Train batch 1500
Avg. loss per last 100 batches: 0.023552
Train batch 1500
Avg. loss per last 100 batches: 0.023552
Epoch: 32: Step: 1501/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 1501/4907, loss=0.000000, lr=0.000004
Train batch 1600
Avg. loss per last 100 batches: 0.023538
Train batch 1600
Avg. loss per last 100 batches: 0.023538
Epoch: 32: Step: 1601/4907, loss=0.000005, lr=0.000004
Epoch: 32: Step: 1601/4907, loss=0.000005, lr=0.000004
Train batch 1700
Avg. loss per last 100 batches: 0.022473
Train batch 1700
Avg. loss per last 100 batches: 0.022473
Epoch: 32: Step: 1701/4907, loss=0.001882, lr=0.000004
Epoch: 32: Step: 1701/4907, loss=0.001882, lr=0.000004
Train batch 1800
Avg. loss per last 100 batches: 0.017550
Train batch 1800
Avg. loss per last 100 batches: 0.017550
Epoch: 32: Step: 1801/4907, loss=0.001043, lr=0.000004
Epoch: 32: Step: 1801/4907, loss=0.001043, lr=0.000004
Train batch 1900
Avg. loss per last 100 batches: 0.010898
Train batch 1900
Avg. loss per last 100 batches: 0.010898
Epoch: 32: Step: 1901/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 1901/4907, loss=0.000000, lr=0.000004
Train batch 2000
Avg. loss per last 100 batches: 0.018744
Train batch 2000
Avg. loss per last 100 batches: 0.018744
Epoch: 32: Step: 2001/4907, loss=0.003329, lr=0.000004
Epoch: 32: Step: 2001/4907, loss=0.003329, lr=0.000004
Train batch 2100
Avg. loss per last 100 batches: 0.009517
Train batch 2100
Avg. loss per last 100 batches: 0.009517
Epoch: 32: Step: 2101/4907, loss=0.006670, lr=0.000004
Epoch: 32: Step: 2101/4907, loss=0.006670, lr=0.000004
Train batch 2200
Avg. loss per last 100 batches: 0.016375
Train batch 2200
Avg. loss per last 100 batches: 0.016375
Epoch: 32: Step: 2201/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 2201/4907, loss=0.000000, lr=0.000004
Train batch 2300
Avg. loss per last 100 batches: 0.012671
Train batch 2300
Avg. loss per last 100 batches: 0.012671
Epoch: 32: Step: 2301/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 2301/4907, loss=0.000000, lr=0.000004
Train batch 2400
Avg. loss per last 100 batches: 0.014877
Train batch 2400
Avg. loss per last 100 batches: 0.014877
Epoch: 32: Step: 2401/4907, loss=0.002967, lr=0.000004
Epoch: 32: Step: 2401/4907, loss=0.002967, lr=0.000004
Train batch 2500
Avg. loss per last 100 batches: 0.021130
Train batch 2500
Avg. loss per last 100 batches: 0.021130
Epoch: 32: Step: 2501/4907, loss=0.000023, lr=0.000004
Epoch: 32: Step: 2501/4907, loss=0.000023, lr=0.000004
Train batch 2600
Avg. loss per last 100 batches: 0.022157
Train batch 2600
Avg. loss per last 100 batches: 0.022157
Epoch: 32: Step: 2601/4907, loss=0.000067, lr=0.000004
Epoch: 32: Step: 2601/4907, loss=0.000067, lr=0.000004
Train batch 2700
Avg. loss per last 100 batches: 0.006801
Train batch 2700
Avg. loss per last 100 batches: 0.006801
Epoch: 32: Step: 2701/4907, loss=0.000645, lr=0.000004
Epoch: 32: Step: 2701/4907, loss=0.000645, lr=0.000004
Train batch 2800
Avg. loss per last 100 batches: 0.006875
Train batch 2800
Avg. loss per last 100 batches: 0.006875
Epoch: 32: Step: 2801/4907, loss=0.002046, lr=0.000004
Epoch: 32: Step: 2801/4907, loss=0.002046, lr=0.000004
Train batch 2900
Avg. loss per last 100 batches: 0.032317
Train batch 2900
Avg. loss per last 100 batches: 0.032317
Epoch: 32: Step: 2901/4907, loss=0.000611, lr=0.000004
Epoch: 32: Step: 2901/4907, loss=0.000611, lr=0.000004
Train batch 3000
Avg. loss per last 100 batches: 0.021430
Train batch 3000
Avg. loss per last 100 batches: 0.021430
Epoch: 32: Step: 3001/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 3001/4907, loss=0.000000, lr=0.000004
Train batch 3100
Avg. loss per last 100 batches: 0.013764
Train batch 3100
Avg. loss per last 100 batches: 0.013764
Epoch: 32: Step: 3101/4907, loss=0.112286, lr=0.000004
Epoch: 32: Step: 3101/4907, loss=0.112286, lr=0.000004
Train batch 3200
Avg. loss per last 100 batches: 0.042584
Train batch 3200
Avg. loss per last 100 batches: 0.042584
Epoch: 32: Step: 3201/4907, loss=0.000001, lr=0.000004
Epoch: 32: Step: 3201/4907, loss=0.000001, lr=0.000004
Train batch 3300
Avg. loss per last 100 batches: 0.007863
Train batch 3300
Avg. loss per last 100 batches: 0.007863
Epoch: 32: Step: 3301/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 3301/4907, loss=0.000000, lr=0.000004
Train batch 3400
Avg. loss per last 100 batches: 0.013085
Train batch 3400
Avg. loss per last 100 batches: 0.013085
Epoch: 32: Step: 3401/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 3401/4907, loss=0.000000, lr=0.000004
Train batch 3500
Avg. loss per last 100 batches: 0.037896
Train batch 3500
Avg. loss per last 100 batches: 0.037896
Epoch: 32: Step: 3501/4907, loss=0.000003, lr=0.000004
Epoch: 32: Step: 3501/4907, loss=0.000003, lr=0.000004
Train batch 3600
Avg. loss per last 100 batches: 0.011054
Train batch 3600
Avg. loss per last 100 batches: 0.011054
Epoch: 32: Step: 3601/4907, loss=0.000017, lr=0.000004
Epoch: 32: Step: 3601/4907, loss=0.000017, lr=0.000004
Train batch 3700
Avg. loss per last 100 batches: 0.016535
Train batch 3700
Avg. loss per last 100 batches: 0.016535
Epoch: 32: Step: 3701/4907, loss=0.000006, lr=0.000004
Epoch: 32: Step: 3701/4907, loss=0.000006, lr=0.000004
Train batch 3800
Avg. loss per last 100 batches: 0.031290
Train batch 3800
Avg. loss per last 100 batches: 0.031290
Epoch: 32: Step: 3801/4907, loss=0.000054, lr=0.000004
Epoch: 32: Step: 3801/4907, loss=0.000054, lr=0.000004
Train batch 3900
Avg. loss per last 100 batches: 0.049084
Train batch 3900
Avg. loss per last 100 batches: 0.049084
Epoch: 32: Step: 3901/4907, loss=0.000002, lr=0.000004
Epoch: 32: Step: 3901/4907, loss=0.000002, lr=0.000004
Train batch 4000
Avg. loss per last 100 batches: 0.020732
Train batch 4000
Avg. loss per last 100 batches: 0.020732
Epoch: 32: Step: 4001/4907, loss=0.000557, lr=0.000004
Epoch: 32: Step: 4001/4907, loss=0.000557, lr=0.000004
Train batch 4100
Avg. loss per last 100 batches: 0.028105
Train batch 4100
Avg. loss per last 100 batches: 0.028105
Epoch: 32: Step: 4101/4907, loss=0.000001, lr=0.000004
Epoch: 32: Step: 4101/4907, loss=0.000001, lr=0.000004
Train batch 4200
Avg. loss per last 100 batches: 0.026905
Train batch 4200
Avg. loss per last 100 batches: 0.026905
Epoch: 32: Step: 4201/4907, loss=0.007405, lr=0.000004
Epoch: 32: Step: 4201/4907, loss=0.007405, lr=0.000004
Train batch 4300
Avg. loss per last 100 batches: 0.020935
Train batch 4300
Avg. loss per last 100 batches: 0.020935
Epoch: 32: Step: 4301/4907, loss=0.000006, lr=0.000004
Epoch: 32: Step: 4301/4907, loss=0.000006, lr=0.000004
Train batch 4400
Avg. loss per last 100 batches: 0.006355
Train batch 4400
Avg. loss per last 100 batches: 0.006355
Epoch: 32: Step: 4401/4907, loss=0.012492, lr=0.000004
Epoch: 32: Step: 4401/4907, loss=0.012492, lr=0.000004
Train batch 4500
Avg. loss per last 100 batches: 0.013873
Train batch 4500
Avg. loss per last 100 batches: 0.013873
Epoch: 32: Step: 4501/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 4501/4907, loss=0.000000, lr=0.000004
Train batch 4600
Avg. loss per last 100 batches: 0.010115
Train batch 4600
Avg. loss per last 100 batches: 0.010115
Epoch: 32: Step: 4601/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 4601/4907, loss=0.000000, lr=0.000004
Train batch 4700
Avg. loss per last 100 batches: 0.031260
Train batch 4700
Avg. loss per last 100 batches: 0.031260
Epoch: 32: Step: 4701/4907, loss=0.000000, lr=0.000004
Epoch: 32: Step: 4701/4907, loss=0.000000, lr=0.000004
Train batch 4800
Avg. loss per last 100 batches: 0.014472
Train batch 4800
Avg. loss per last 100 batches: 0.014472
Epoch: 32: Step: 4801/4907, loss=0.000004, lr=0.000004
Epoch: 32: Step: 4801/4907, loss=0.000004, lr=0.000004
Train batch 4900
Avg. loss per last 100 batches: 0.019295
Train batch 4900
Avg. loss per last 100 batches: 0.019295
Epoch: 32: Step: 4901/4907, loss=0.225633, lr=0.000004
Epoch: 32: Step: 4901/4907, loss=0.225633, lr=0.000004
Validation: Epoch: 32 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 32 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 165.37615101289134, total questions=6516
Av.rank validation: average rank 165.37615101289134, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.32.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.32.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 165.37615101289134, total questions=6516
Av.rank validation: average rank 165.37615101289134, total questions=6516
Av Loss per epoch=0.020985
epoch total correct predictions=58555
***** Epoch 33 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.32.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.32.4907
Av Loss per epoch=0.020985
epoch total correct predictions=58555
***** Epoch 33 *****
Epoch: 33: Step: 1/4907, loss=0.000010, lr=0.000004
Epoch: 33: Step: 1/4907, loss=0.000010, lr=0.000004
Train batch 100
Avg. loss per last 100 batches: 0.018425
Train batch 100
Avg. loss per last 100 batches: 0.018425
Epoch: 33: Step: 101/4907, loss=0.000006, lr=0.000004
Epoch: 33: Step: 101/4907, loss=0.000006, lr=0.000004
Train batch 200
Avg. loss per last 100 batches: 0.018068
Train batch 200
Avg. loss per last 100 batches: 0.018068
Epoch: 33: Step: 201/4907, loss=0.000000, lr=0.000004
Epoch: 33: Step: 201/4907, loss=0.000000, lr=0.000004
Train batch 300
Avg. loss per last 100 batches: 0.016483
Train batch 300
Avg. loss per last 100 batches: 0.016483
Epoch: 33: Step: 301/4907, loss=0.348396, lr=0.000003
Epoch: 33: Step: 301/4907, loss=0.348396, lr=0.000003
Train batch 400
Avg. loss per last 100 batches: 0.024845
Train batch 400
Avg. loss per last 100 batches: 0.024845
Epoch: 33: Step: 401/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 401/4907, loss=0.000000, lr=0.000003
Train batch 500
Avg. loss per last 100 batches: 0.013443
Train batch 500
Avg. loss per last 100 batches: 0.013443
Epoch: 33: Step: 501/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 501/4907, loss=0.000000, lr=0.000003
Train batch 600
Avg. loss per last 100 batches: 0.024929
Train batch 600
Avg. loss per last 100 batches: 0.024929
Epoch: 33: Step: 601/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 601/4907, loss=0.000000, lr=0.000003
Train batch 700
Avg. loss per last 100 batches: 0.028358
Train batch 700
Avg. loss per last 100 batches: 0.028358
Epoch: 33: Step: 701/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 701/4907, loss=0.000000, lr=0.000003
Train batch 800
Avg. loss per last 100 batches: 0.019915
Train batch 800
Avg. loss per last 100 batches: 0.019915
Epoch: 33: Step: 801/4907, loss=0.000005, lr=0.000003
Epoch: 33: Step: 801/4907, loss=0.000005, lr=0.000003
Train batch 900
Avg. loss per last 100 batches: 0.023877
Train batch 900
Avg. loss per last 100 batches: 0.023877
Epoch: 33: Step: 901/4907, loss=0.000329, lr=0.000003
Epoch: 33: Step: 901/4907, loss=0.000329, lr=0.000003
Train batch 1000
Avg. loss per last 100 batches: 0.016726
Train batch 1000
Avg. loss per last 100 batches: 0.016726
Epoch: 33: Step: 1001/4907, loss=0.000001, lr=0.000003
Epoch: 33: Step: 1001/4907, loss=0.000001, lr=0.000003
Train batch 1100
Avg. loss per last 100 batches: 0.018673
Train batch 1100
Avg. loss per last 100 batches: 0.018673
Epoch: 33: Step: 1101/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 1101/4907, loss=0.000000, lr=0.000003
Train batch 1200
Avg. loss per last 100 batches: 0.009435
Train batch 1200
Avg. loss per last 100 batches: 0.009435
Epoch: 33: Step: 1201/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 1201/4907, loss=0.000000, lr=0.000003
Train batch 1300
Avg. loss per last 100 batches: 0.019097
Train batch 1300
Avg. loss per last 100 batches: 0.019097
Epoch: 33: Step: 1301/4907, loss=0.000002, lr=0.000003
Epoch: 33: Step: 1301/4907, loss=0.000002, lr=0.000003
Train batch 1400
Avg. loss per last 100 batches: 0.028365
Train batch 1400
Avg. loss per last 100 batches: 0.028365
Epoch: 33: Step: 1401/4907, loss=0.000002, lr=0.000003
Epoch: 33: Step: 1401/4907, loss=0.000002, lr=0.000003
Train batch 1500
Avg. loss per last 100 batches: 0.014555
Train batch 1500
Avg. loss per last 100 batches: 0.014555
Epoch: 33: Step: 1501/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 1501/4907, loss=0.000000, lr=0.000003
Train batch 1600
Avg. loss per last 100 batches: 0.013375
Train batch 1600
Avg. loss per last 100 batches: 0.013375
Epoch: 33: Step: 1601/4907, loss=0.000185, lr=0.000003
Epoch: 33: Step: 1601/4907, loss=0.000185, lr=0.000003
Train batch 1700
Avg. loss per last 100 batches: 0.005342
Train batch 1700
Avg. loss per last 100 batches: 0.005342
Epoch: 33: Step: 1701/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 1701/4907, loss=0.000000, lr=0.000003
Train batch 1800
Avg. loss per last 100 batches: 0.010032
Train batch 1800
Avg. loss per last 100 batches: 0.010032
Epoch: 33: Step: 1801/4907, loss=0.000092, lr=0.000003
Epoch: 33: Step: 1801/4907, loss=0.000092, lr=0.000003
Train batch 1900
Avg. loss per last 100 batches: 0.026499
Train batch 1900
Avg. loss per last 100 batches: 0.026499
Epoch: 33: Step: 1901/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 1901/4907, loss=0.000000, lr=0.000003
Train batch 2000
Avg. loss per last 100 batches: 0.019675
Train batch 2000
Avg. loss per last 100 batches: 0.019675
Epoch: 33: Step: 2001/4907, loss=0.000026, lr=0.000003
Epoch: 33: Step: 2001/4907, loss=0.000026, lr=0.000003
Train batch 2100
Avg. loss per last 100 batches: 0.014484
Train batch 2100
Avg. loss per last 100 batches: 0.014484
Epoch: 33: Step: 2101/4907, loss=0.000279, lr=0.000003
Epoch: 33: Step: 2101/4907, loss=0.000279, lr=0.000003
Train batch 2200
Avg. loss per last 100 batches: 0.015274
Train batch 2200
Avg. loss per last 100 batches: 0.015274
Epoch: 33: Step: 2201/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 2201/4907, loss=0.000000, lr=0.000003
Train batch 2300
Avg. loss per last 100 batches: 0.015136
Train batch 2300
Avg. loss per last 100 batches: 0.015136
Epoch: 33: Step: 2301/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 2301/4907, loss=0.000000, lr=0.000003
Train batch 2400
Avg. loss per last 100 batches: 0.009187
Train batch 2400
Avg. loss per last 100 batches: 0.009187
Epoch: 33: Step: 2401/4907, loss=0.000003, lr=0.000003
Epoch: 33: Step: 2401/4907, loss=0.000003, lr=0.000003
Train batch 2500
Avg. loss per last 100 batches: 0.010937
Train batch 2500
Avg. loss per last 100 batches: 0.010937
Epoch: 33: Step: 2501/4907, loss=0.011492, lr=0.000003
Epoch: 33: Step: 2501/4907, loss=0.011492, lr=0.000003
Train batch 2600
Avg. loss per last 100 batches: 0.004141
Train batch 2600
Avg. loss per last 100 batches: 0.004141
Epoch: 33: Step: 2601/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 2601/4907, loss=0.000000, lr=0.000003
Train batch 2700
Avg. loss per last 100 batches: 0.012724
Train batch 2700
Avg. loss per last 100 batches: 0.012724
Epoch: 33: Step: 2701/4907, loss=0.000071, lr=0.000003
Epoch: 33: Step: 2701/4907, loss=0.000071, lr=0.000003
Train batch 2800
Avg. loss per last 100 batches: 0.026761
Train batch 2800
Avg. loss per last 100 batches: 0.026761
Epoch: 33: Step: 2801/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 2801/4907, loss=0.000000, lr=0.000003
Train batch 2900
Avg. loss per last 100 batches: 0.010833
Train batch 2900
Avg. loss per last 100 batches: 0.010833
Epoch: 33: Step: 2901/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 2901/4907, loss=0.000000, lr=0.000003
Train batch 3000
Avg. loss per last 100 batches: 0.013060
Train batch 3000
Avg. loss per last 100 batches: 0.013060
Epoch: 33: Step: 3001/4907, loss=0.006104, lr=0.000003
Epoch: 33: Step: 3001/4907, loss=0.006104, lr=0.000003
Train batch 3100
Avg. loss per last 100 batches: 0.018083
Train batch 3100
Avg. loss per last 100 batches: 0.018083
Epoch: 33: Step: 3101/4907, loss=0.000024, lr=0.000003
Epoch: 33: Step: 3101/4907, loss=0.000024, lr=0.000003
Train batch 3200
Avg. loss per last 100 batches: 0.014898
Train batch 3200
Avg. loss per last 100 batches: 0.014898
Epoch: 33: Step: 3201/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 3201/4907, loss=0.000000, lr=0.000003
Train batch 3300
Avg. loss per last 100 batches: 0.036386
Train batch 3300
Avg. loss per last 100 batches: 0.036386
Epoch: 33: Step: 3301/4907, loss=0.000009, lr=0.000003
Epoch: 33: Step: 3301/4907, loss=0.000009, lr=0.000003
Train batch 3400
Avg. loss per last 100 batches: 0.020608
Train batch 3400
Avg. loss per last 100 batches: 0.020608
Epoch: 33: Step: 3401/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 3401/4907, loss=0.000000, lr=0.000003
Train batch 3500
Avg. loss per last 100 batches: 0.001992
Train batch 3500
Avg. loss per last 100 batches: 0.001992
Epoch: 33: Step: 3501/4907, loss=0.000112, lr=0.000003
Epoch: 33: Step: 3501/4907, loss=0.000112, lr=0.000003
Train batch 3600
Avg. loss per last 100 batches: 0.014731
Train batch 3600
Avg. loss per last 100 batches: 0.014731
Epoch: 33: Step: 3601/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 3601/4907, loss=0.000000, lr=0.000003
Train batch 3700
Avg. loss per last 100 batches: 0.021586
Train batch 3700
Avg. loss per last 100 batches: 0.021586
Epoch: 33: Step: 3701/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 3701/4907, loss=0.000000, lr=0.000003
Train batch 3800
Avg. loss per last 100 batches: 0.016012
Train batch 3800
Avg. loss per last 100 batches: 0.016012
Epoch: 33: Step: 3801/4907, loss=0.006656, lr=0.000003
Epoch: 33: Step: 3801/4907, loss=0.006656, lr=0.000003
Train batch 3900
Avg. loss per last 100 batches: 0.029293
Train batch 3900
Avg. loss per last 100 batches: 0.029293
Epoch: 33: Step: 3901/4907, loss=0.013599, lr=0.000003
Epoch: 33: Step: 3901/4907, loss=0.013599, lr=0.000003
Train batch 4000
Avg. loss per last 100 batches: 0.038742
Train batch 4000
Avg. loss per last 100 batches: 0.038742
Epoch: 33: Step: 4001/4907, loss=0.000026, lr=0.000003
Epoch: 33: Step: 4001/4907, loss=0.000026, lr=0.000003
Train batch 4100
Avg. loss per last 100 batches: 0.031039
Train batch 4100
Avg. loss per last 100 batches: 0.031039
Epoch: 33: Step: 4101/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 4101/4907, loss=0.000000, lr=0.000003
Train batch 4200
Avg. loss per last 100 batches: 0.002510
Train batch 4200
Avg. loss per last 100 batches: 0.002510
Epoch: 33: Step: 4201/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 4201/4907, loss=0.000000, lr=0.000003
Train batch 4300
Avg. loss per last 100 batches: 0.012222
Train batch 4300
Avg. loss per last 100 batches: 0.012222
Epoch: 33: Step: 4301/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 4301/4907, loss=0.000000, lr=0.000003
Train batch 4400
Avg. loss per last 100 batches: 0.026864
Train batch 4400
Avg. loss per last 100 batches: 0.026864
Epoch: 33: Step: 4401/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 4401/4907, loss=0.000000, lr=0.000003
Train batch 4500
Avg. loss per last 100 batches: 0.012880
Train batch 4500
Avg. loss per last 100 batches: 0.012880
Epoch: 33: Step: 4501/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 4501/4907, loss=0.000000, lr=0.000003
Train batch 4600
Avg. loss per last 100 batches: 0.020989
Train batch 4600
Avg. loss per last 100 batches: 0.020989
Epoch: 33: Step: 4601/4907, loss=0.000002, lr=0.000003
Epoch: 33: Step: 4601/4907, loss=0.000002, lr=0.000003
Train batch 4700
Avg. loss per last 100 batches: 0.023342
Train batch 4700
Avg. loss per last 100 batches: 0.023342
Epoch: 33: Step: 4701/4907, loss=0.000003, lr=0.000003
Epoch: 33: Step: 4701/4907, loss=0.000003, lr=0.000003
Train batch 4800
Avg. loss per last 100 batches: 0.001454
Train batch 4800
Avg. loss per last 100 batches: 0.001454
Epoch: 33: Step: 4801/4907, loss=0.000000, lr=0.000003
Epoch: 33: Step: 4801/4907, loss=0.000000, lr=0.000003
Train batch 4900
Avg. loss per last 100 batches: 0.006210
Train batch 4900
Avg. loss per last 100 batches: 0.006210
Epoch: 33: Step: 4901/4907, loss=0.000135, lr=0.000003
Epoch: 33: Step: 4901/4907, loss=0.000135, lr=0.000003
Validation: Epoch: 33 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 33 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 171.25383670963782, total questions=6516
Av.rank validation: average rank 171.25383670963782, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.33.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.33.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: average rank 171.25383670963782, total questions=6516
Av.rank validation: average rank 171.25383670963782, total questions=6516
Av Loss per epoch=0.017547
epoch total correct predictions=58614
***** Epoch 34 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.33.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.33.4907
Av Loss per epoch=0.017547
epoch total correct predictions=58614
***** Epoch 34 *****
Epoch: 34: Step: 1/4907, loss=0.000004, lr=0.000003
Epoch: 34: Step: 1/4907, loss=0.000004, lr=0.000003
Train batch 100
Avg. loss per last 100 batches: 0.011357
Train batch 100
Avg. loss per last 100 batches: 0.011357
Epoch: 34: Step: 101/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 101/4907, loss=0.000000, lr=0.000003
Train batch 200
Avg. loss per last 100 batches: 0.021494
Train batch 200
Avg. loss per last 100 batches: 0.021494
Epoch: 34: Step: 201/4907, loss=0.000082, lr=0.000003
Epoch: 34: Step: 201/4907, loss=0.000082, lr=0.000003
Train batch 300
Avg. loss per last 100 batches: 0.007490
Train batch 300
Avg. loss per last 100 batches: 0.007490
Epoch: 34: Step: 301/4907, loss=0.000004, lr=0.000003
Epoch: 34: Step: 301/4907, loss=0.000004, lr=0.000003
Train batch 400
Avg. loss per last 100 batches: 0.018097
Train batch 400
Avg. loss per last 100 batches: 0.018097
Epoch: 34: Step: 401/4907, loss=0.000001, lr=0.000003
Epoch: 34: Step: 401/4907, loss=0.000001, lr=0.000003
Train batch 500
Avg. loss per last 100 batches: 0.018096
Train batch 500
Avg. loss per last 100 batches: 0.018096
Epoch: 34: Step: 501/4907, loss=0.000051, lr=0.000003
Epoch: 34: Step: 501/4907, loss=0.000051, lr=0.000003
Train batch 600
Avg. loss per last 100 batches: 0.029073
Train batch 600
Avg. loss per last 100 batches: 0.029073
Epoch: 34: Step: 601/4907, loss=0.003713, lr=0.000003
Epoch: 34: Step: 601/4907, loss=0.003713, lr=0.000003
Train batch 700
Avg. loss per last 100 batches: 0.024211
Train batch 700
Avg. loss per last 100 batches: 0.024211
Epoch: 34: Step: 701/4907, loss=0.010850, lr=0.000003
Epoch: 34: Step: 701/4907, loss=0.010850, lr=0.000003
Train batch 800
Avg. loss per last 100 batches: 0.019746
Train batch 800
Avg. loss per last 100 batches: 0.019746
Epoch: 34: Step: 801/4907, loss=0.000073, lr=0.000003
Epoch: 34: Step: 801/4907, loss=0.000073, lr=0.000003
Train batch 900
Avg. loss per last 100 batches: 0.004844
Train batch 900
Avg. loss per last 100 batches: 0.004844
Epoch: 34: Step: 901/4907, loss=0.000002, lr=0.000003
Epoch: 34: Step: 901/4907, loss=0.000002, lr=0.000003
Train batch 1000
Avg. loss per last 100 batches: 0.010704
Train batch 1000
Avg. loss per last 100 batches: 0.010704
Epoch: 34: Step: 1001/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 1001/4907, loss=0.000000, lr=0.000003
Train batch 1100
Avg. loss per last 100 batches: 0.023685
Train batch 1100
Avg. loss per last 100 batches: 0.023685
Epoch: 34: Step: 1101/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 1101/4907, loss=0.000000, lr=0.000003
Train batch 1200
Avg. loss per last 100 batches: 0.022631
Train batch 1200
Avg. loss per last 100 batches: 0.022631
Epoch: 34: Step: 1201/4907, loss=0.000012, lr=0.000003
Epoch: 34: Step: 1201/4907, loss=0.000012, lr=0.000003
Train batch 1300
Avg. loss per last 100 batches: 0.013072
Train batch 1300
Avg. loss per last 100 batches: 0.013072
Epoch: 34: Step: 1301/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 1301/4907, loss=0.000000, lr=0.000003
Train batch 1400
Avg. loss per last 100 batches: 0.014994
Train batch 1400
Avg. loss per last 100 batches: 0.014994
Epoch: 34: Step: 1401/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 1401/4907, loss=0.000000, lr=0.000003
Train batch 1500
Avg. loss per last 100 batches: 0.006930
Train batch 1500
Avg. loss per last 100 batches: 0.006930
Epoch: 34: Step: 1501/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 1501/4907, loss=0.000000, lr=0.000003
Train batch 1600
Avg. loss per last 100 batches: 0.006436
Train batch 1600
Avg. loss per last 100 batches: 0.006436
Epoch: 34: Step: 1601/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 1601/4907, loss=0.000000, lr=0.000003
Train batch 1700
Avg. loss per last 100 batches: 0.048630
Train batch 1700
Avg. loss per last 100 batches: 0.048630
Epoch: 34: Step: 1701/4907, loss=0.000173, lr=0.000003
Epoch: 34: Step: 1701/4907, loss=0.000173, lr=0.000003
Train batch 1800
Avg. loss per last 100 batches: 0.004803
Train batch 1800
Avg. loss per last 100 batches: 0.004803
Epoch: 34: Step: 1801/4907, loss=0.000001, lr=0.000003
Epoch: 34: Step: 1801/4907, loss=0.000001, lr=0.000003
Train batch 1900
Avg. loss per last 100 batches: 0.020356
Train batch 1900
Avg. loss per last 100 batches: 0.020356
Epoch: 34: Step: 1901/4907, loss=0.000001, lr=0.000003
Epoch: 34: Step: 1901/4907, loss=0.000001, lr=0.000003
Train batch 2000
Avg. loss per last 100 batches: 0.033773
Train batch 2000
Avg. loss per last 100 batches: 0.033773
Epoch: 34: Step: 2001/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 2001/4907, loss=0.000000, lr=0.000003
Train batch 2100
Avg. loss per last 100 batches: 0.011173
Train batch 2100
Avg. loss per last 100 batches: 0.011173
Epoch: 34: Step: 2101/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 2101/4907, loss=0.000000, lr=0.000003
Train batch 2200
Avg. loss per last 100 batches: 0.016690
Train batch 2200
Avg. loss per last 100 batches: 0.016690
Epoch: 34: Step: 2201/4907, loss=0.188146, lr=0.000003
Epoch: 34: Step: 2201/4907, loss=0.188146, lr=0.000003
Train batch 2300
Avg. loss per last 100 batches: 0.011687
Train batch 2300
Avg. loss per last 100 batches: 0.011687
Epoch: 34: Step: 2301/4907, loss=0.000001, lr=0.000003
Epoch: 34: Step: 2301/4907, loss=0.000001, lr=0.000003
Train batch 2400
Avg. loss per last 100 batches: 0.016263
Train batch 2400
Avg. loss per last 100 batches: 0.016263
Epoch: 34: Step: 2401/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 2401/4907, loss=0.000000, lr=0.000003
Train batch 2500
Avg. loss per last 100 batches: 0.001636
Train batch 2500
Avg. loss per last 100 batches: 0.001636
Epoch: 34: Step: 2501/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 2501/4907, loss=0.000000, lr=0.000003
Train batch 2600
Avg. loss per last 100 batches: 0.007907
Train batch 2600
Avg. loss per last 100 batches: 0.007907
Epoch: 34: Step: 2601/4907, loss=0.000002, lr=0.000003
Epoch: 34: Step: 2601/4907, loss=0.000002, lr=0.000003
Train batch 2700
Avg. loss per last 100 batches: 0.022678
Train batch 2700
Avg. loss per last 100 batches: 0.022678
Epoch: 34: Step: 2701/4907, loss=0.000001, lr=0.000003
Epoch: 34: Step: 2701/4907, loss=0.000001, lr=0.000003
Train batch 2800
Avg. loss per last 100 batches: 0.006657
Train batch 2800
Avg. loss per last 100 batches: 0.006657
Epoch: 34: Step: 2801/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 2801/4907, loss=0.000000, lr=0.000003
Train batch 2900
Avg. loss per last 100 batches: 0.016871
Train batch 2900
Avg. loss per last 100 batches: 0.016871
Epoch: 34: Step: 2901/4907, loss=0.000004, lr=0.000003
Epoch: 34: Step: 2901/4907, loss=0.000004, lr=0.000003
Train batch 3000
Avg. loss per last 100 batches: 0.018568
Train batch 3000
Avg. loss per last 100 batches: 0.018568
Epoch: 34: Step: 3001/4907, loss=0.000070, lr=0.000003
Epoch: 34: Step: 3001/4907, loss=0.000070, lr=0.000003
Train batch 3100
Avg. loss per last 100 batches: 0.003549
Train batch 3100
Avg. loss per last 100 batches: 0.003549
Epoch: 34: Step: 3101/4907, loss=0.099119, lr=0.000003
Epoch: 34: Step: 3101/4907, loss=0.099119, lr=0.000003
Train batch 3200
Train batch 3200
Avg. loss per last 100 batches: 0.022497
Avg. loss per last 100 batches: 0.022497
Epoch: 34: Step: 3201/4907, loss=0.000530, lr=0.000003
Epoch: 34: Step: 3201/4907, loss=0.000530, lr=0.000003
Train batch 3300
Avg. loss per last 100 batches: 0.020956
Train batch 3300
Avg. loss per last 100 batches: 0.020956
Epoch: 34: Step: 3301/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 3301/4907, loss=0.000000, lr=0.000003
Train batch 3400
Avg. loss per last 100 batches: 0.005921
Train batch 3400
Avg. loss per last 100 batches: 0.005921
Epoch: 34: Step: 3401/4907, loss=0.008329, lr=0.000003
Epoch: 34: Step: 3401/4907, loss=0.008329, lr=0.000003
Train batch 3500
Avg. loss per last 100 batches: 0.020544
Train batch 3500
Avg. loss per last 100 batches: 0.020544
Epoch: 34: Step: 3501/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 3501/4907, loss=0.000000, lr=0.000003
Train batch 3600
Avg. loss per last 100 batches: 0.023971
Train batch 3600
Avg. loss per last 100 batches: 0.023971
Epoch: 34: Step: 3601/4907, loss=0.000001, lr=0.000003
Epoch: 34: Step: 3601/4907, loss=0.000001, lr=0.000003
Train batch 3700
Avg. loss per last 100 batches: 0.014190
Train batch 3700
Avg. loss per last 100 batches: 0.014190
Epoch: 34: Step: 3701/4907, loss=0.005408, lr=0.000003
Epoch: 34: Step: 3701/4907, loss=0.005408, lr=0.000003
Train batch 3800
Avg. loss per last 100 batches: 0.016336
Train batch 3800
Avg. loss per last 100 batches: 0.016336
Epoch: 34: Step: 3801/4907, loss=0.000039, lr=0.000003
Epoch: 34: Step: 3801/4907, loss=0.000039, lr=0.000003
Train batch 3900
Avg. loss per last 100 batches: 0.016940
Train batch 3900
Avg. loss per last 100 batches: 0.016940
Epoch: 34: Step: 3901/4907, loss=0.000011, lr=0.000003
Epoch: 34: Step: 3901/4907, loss=0.000011, lr=0.000003
Train batch 4000
Avg. loss per last 100 batches: 0.013800
Train batch 4000
Avg. loss per last 100 batches: 0.013800
Epoch: 34: Step: 4001/4907, loss=0.000050, lr=0.000003
Epoch: 34: Step: 4001/4907, loss=0.000050, lr=0.000003
Train batch 4100
Avg. loss per last 100 batches: 0.012857
Train batch 4100
Avg. loss per last 100 batches: 0.012857
Epoch: 34: Step: 4101/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 4101/4907, loss=0.000000, lr=0.000003
Train batch 4200
Avg. loss per last 100 batches: 0.013932
Train batch 4200
Avg. loss per last 100 batches: 0.013932
Epoch: 34: Step: 4201/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 4201/4907, loss=0.000000, lr=0.000003
Train batch 4300
Avg. loss per last 100 batches: 0.025476
Train batch 4300
Avg. loss per last 100 batches: 0.025476
Epoch: 34: Step: 4301/4907, loss=0.001296, lr=0.000003
Epoch: 34: Step: 4301/4907, loss=0.001296, lr=0.000003
Train batch 4400
Avg. loss per last 100 batches: 0.023512
Train batch 4400
Avg. loss per last 100 batches: 0.023512
Epoch: 34: Step: 4401/4907, loss=0.001672, lr=0.000003
Epoch: 34: Step: 4401/4907, loss=0.001672, lr=0.000003
Train batch 4500
Avg. loss per last 100 batches: 0.013668
Train batch 4500
Avg. loss per last 100 batches: 0.013668
Epoch: 34: Step: 4501/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 4501/4907, loss=0.000000, lr=0.000003
Train batch 4600
Avg. loss per last 100 batches: 0.007142
Train batch 4600
Avg. loss per last 100 batches: 0.007142
Epoch: 34: Step: 4601/4907, loss=0.004143, lr=0.000003
Epoch: 34: Step: 4601/4907, loss=0.004143, lr=0.000003
Train batch 4700
Avg. loss per last 100 batches: 0.025722
Train batch 4700
Avg. loss per last 100 batches: 0.025722
Epoch: 34: Step: 4701/4907, loss=0.509442, lr=0.000003
Epoch: 34: Step: 4701/4907, loss=0.509442, lr=0.000003
Train batch 4800
Avg. loss per last 100 batches: 0.016605
Train batch 4800
Avg. loss per last 100 batches: 0.016605
Epoch: 34: Step: 4801/4907, loss=0.000000, lr=0.000003
Epoch: 34: Step: 4801/4907, loss=0.000000, lr=0.000003
Train batch 4900
Avg. loss per last 100 batches: 0.006088
Train batch 4900
Avg. loss per last 100 batches: 0.006088
Epoch: 34: Step: 4901/4907, loss=0.000008, lr=0.000003
Epoch: 34: Step: 4901/4907, loss=0.000008, lr=0.000003
Validation: Epoch: 34 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 34 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 166.81752608962555, total questions=6516
Av.rank validation: average rank 166.81752608962555, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.34.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.34.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 166.81752608962555, total questions=6516
Av.rank validation: average rank 166.81752608962555, total questions=6516
Av Loss per epoch=0.016186
epoch total correct predictions=58646
***** Epoch 35 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.34.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.34.4907
Av Loss per epoch=0.016186
epoch total correct predictions=58646
***** Epoch 35 *****
Epoch: 35: Step: 1/4907, loss=0.000000, lr=0.000003
Epoch: 35: Step: 1/4907, loss=0.000000, lr=0.000003
Train batch 100
Avg. loss per last 100 batches: 0.005512
Train batch 100
Avg. loss per last 100 batches: 0.005512
Epoch: 35: Step: 101/4907, loss=0.000037, lr=0.000003
Epoch: 35: Step: 101/4907, loss=0.000037, lr=0.000003
Train batch 200
Avg. loss per last 100 batches: 0.028355
Train batch 200
Avg. loss per last 100 batches: 0.028355
Epoch: 35: Step: 201/4907, loss=0.001250, lr=0.000002
Epoch: 35: Step: 201/4907, loss=0.001250, lr=0.000002
Train batch 300
Avg. loss per last 100 batches: 0.009666
Train batch 300
Avg. loss per last 100 batches: 0.009666
Epoch: 35: Step: 301/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 301/4907, loss=0.000000, lr=0.000002
Train batch 400
Avg. loss per last 100 batches: 0.012867
Train batch 400
Avg. loss per last 100 batches: 0.012867
Epoch: 35: Step: 401/4907, loss=0.000031, lr=0.000002
Epoch: 35: Step: 401/4907, loss=0.000031, lr=0.000002
Train batch 500
Avg. loss per last 100 batches: 0.002826
Train batch 500
Avg. loss per last 100 batches: 0.002826
Epoch: 35: Step: 501/4907, loss=0.000001, lr=0.000002
Epoch: 35: Step: 501/4907, loss=0.000001, lr=0.000002
Train batch 600
Avg. loss per last 100 batches: 0.011646
Train batch 600
Avg. loss per last 100 batches: 0.011646
Epoch: 35: Step: 601/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 601/4907, loss=0.000000, lr=0.000002
Train batch 700
Avg. loss per last 100 batches: 0.008660
Train batch 700
Avg. loss per last 100 batches: 0.008660
Epoch: 35: Step: 701/4907, loss=0.000051, lr=0.000002
Epoch: 35: Step: 701/4907, loss=0.000051, lr=0.000002
Train batch 800
Avg. loss per last 100 batches: 0.035777
Train batch 800
Avg. loss per last 100 batches: 0.035777
Epoch: 35: Step: 801/4907, loss=0.000003, lr=0.000002
Epoch: 35: Step: 801/4907, loss=0.000003, lr=0.000002
Train batch 900
Avg. loss per last 100 batches: 0.008060
Train batch 900
Avg. loss per last 100 batches: 0.008060
Epoch: 35: Step: 901/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 901/4907, loss=0.000000, lr=0.000002
Train batch 1000
Avg. loss per last 100 batches: 0.013236
Train batch 1000
Avg. loss per last 100 batches: 0.013236
Epoch: 35: Step: 1001/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 1001/4907, loss=0.000000, lr=0.000002
Train batch 1100
Avg. loss per last 100 batches: 0.021585
Train batch 1100
Avg. loss per last 100 batches: 0.021585
Epoch: 35: Step: 1101/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 1101/4907, loss=0.000000, lr=0.000002
Train batch 1200
Avg. loss per last 100 batches: 0.015422
Train batch 1200
Avg. loss per last 100 batches: 0.015422
Epoch: 35: Step: 1201/4907, loss=0.000009, lr=0.000002
Epoch: 35: Step: 1201/4907, loss=0.000009, lr=0.000002
Train batch 1300
Avg. loss per last 100 batches: 0.001284
Train batch 1300
Avg. loss per last 100 batches: 0.001284
Epoch: 35: Step: 1301/4907, loss=0.000001, lr=0.000002
Epoch: 35: Step: 1301/4907, loss=0.000001, lr=0.000002
Train batch 1400
Avg. loss per last 100 batches: 0.006295
Train batch 1400
Avg. loss per last 100 batches: 0.006295
Epoch: 35: Step: 1401/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 1401/4907, loss=0.000000, lr=0.000002
Train batch 1500
Avg. loss per last 100 batches: 0.014627
Train batch 1500
Avg. loss per last 100 batches: 0.014627
Epoch: 35: Step: 1501/4907, loss=0.000002, lr=0.000002
Epoch: 35: Step: 1501/4907, loss=0.000002, lr=0.000002
Train batch 1600
Avg. loss per last 100 batches: 0.013834
Train batch 1600
Avg. loss per last 100 batches: 0.013834
Epoch: 35: Step: 1601/4907, loss=0.000035, lr=0.000002
Epoch: 35: Step: 1601/4907, loss=0.000035, lr=0.000002
Train batch 1700
Avg. loss per last 100 batches: 0.015492
Train batch 1700
Avg. loss per last 100 batches: 0.015492
Epoch: 35: Step: 1701/4907, loss=0.015565, lr=0.000002
Epoch: 35: Step: 1701/4907, loss=0.015565, lr=0.000002
Train batch 1800
Avg. loss per last 100 batches: 0.013598
Train batch 1800
Avg. loss per last 100 batches: 0.013598
Epoch: 35: Step: 1801/4907, loss=0.000052, lr=0.000002
Epoch: 35: Step: 1801/4907, loss=0.000052, lr=0.000002
Train batch 1900
Avg. loss per last 100 batches: 0.013745
Train batch 1900
Avg. loss per last 100 batches: 0.013745
Epoch: 35: Step: 1901/4907, loss=0.000002, lr=0.000002
Epoch: 35: Step: 1901/4907, loss=0.000002, lr=0.000002
Train batch 2000
Avg. loss per last 100 batches: 0.020093
Train batch 2000
Avg. loss per last 100 batches: 0.020093
Epoch: 35: Step: 2001/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 2001/4907, loss=0.000000, lr=0.000002
Train batch 2100
Avg. loss per last 100 batches: 0.044540
Train batch 2100
Avg. loss per last 100 batches: 0.044540
Epoch: 35: Step: 2101/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 2101/4907, loss=0.000000, lr=0.000002
Train batch 2200
Avg. loss per last 100 batches: 0.010139
Train batch 2200
Avg. loss per last 100 batches: 0.010139
Epoch: 35: Step: 2201/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 2201/4907, loss=0.000000, lr=0.000002
Train batch 2300
Avg. loss per last 100 batches: 0.003836
Train batch 2300
Avg. loss per last 100 batches: 0.003836
Epoch: 35: Step: 2301/4907, loss=0.000004, lr=0.000002
Epoch: 35: Step: 2301/4907, loss=0.000004, lr=0.000002
Train batch 2400
Avg. loss per last 100 batches: 0.022844
Train batch 2400
Avg. loss per last 100 batches: 0.022844
Epoch: 35: Step: 2401/4907, loss=0.472877, lr=0.000002
Epoch: 35: Step: 2401/4907, loss=0.472877, lr=0.000002
Train batch 2500
Avg. loss per last 100 batches: 0.017061
Train batch 2500
Avg. loss per last 100 batches: 0.017061
Epoch: 35: Step: 2501/4907, loss=0.000018, lr=0.000002
Epoch: 35: Step: 2501/4907, loss=0.000018, lr=0.000002
Train batch 2600
Avg. loss per last 100 batches: 0.007693
Train batch 2600
Avg. loss per last 100 batches: 0.007693
Epoch: 35: Step: 2601/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 2601/4907, loss=0.000000, lr=0.000002
Train batch 2700
Avg. loss per last 100 batches: 0.008635
Train batch 2700
Avg. loss per last 100 batches: 0.008635
Epoch: 35: Step: 2701/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 2701/4907, loss=0.000000, lr=0.000002
Train batch 2800
Train batch 2800
Avg. loss per last 100 batches: 0.032852
Avg. loss per last 100 batches: 0.032852
Epoch: 35: Step: 2801/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 2801/4907, loss=0.000000, lr=0.000002
Train batch 2900
Avg. loss per last 100 batches: 0.020266
Train batch 2900
Avg. loss per last 100 batches: 0.020266
Epoch: 35: Step: 2901/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 2901/4907, loss=0.000000, lr=0.000002
Train batch 3000
Avg. loss per last 100 batches: 0.011845
Train batch 3000
Avg. loss per last 100 batches: 0.011845
Epoch: 35: Step: 3001/4907, loss=0.000561, lr=0.000002
Epoch: 35: Step: 3001/4907, loss=0.000561, lr=0.000002
Train batch 3100
Avg. loss per last 100 batches: 0.008213
Train batch 3100
Avg. loss per last 100 batches: 0.008213
Epoch: 35: Step: 3101/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 3101/4907, loss=0.000000, lr=0.000002
Train batch 3200
Avg. loss per last 100 batches: 0.012118
Train batch 3200
Avg. loss per last 100 batches: 0.012118
Epoch: 35: Step: 3201/4907, loss=0.273360, lr=0.000002
Epoch: 35: Step: 3201/4907, loss=0.273360, lr=0.000002
Train batch 3300
Avg. loss per last 100 batches: 0.004146
Train batch 3300
Avg. loss per last 100 batches: 0.004146
Epoch: 35: Step: 3301/4907, loss=0.000001, lr=0.000002
Epoch: 35: Step: 3301/4907, loss=0.000001, lr=0.000002
Train batch 3400
Avg. loss per last 100 batches: 0.013270
Train batch 3400
Avg. loss per last 100 batches: 0.013270
Epoch: 35: Step: 3401/4907, loss=0.000038, lr=0.000002
Epoch: 35: Step: 3401/4907, loss=0.000038, lr=0.000002
Train batch 3500
Train batch 3500
Avg. loss per last 100 batches: 0.015936
Avg. loss per last 100 batches: 0.015936
Epoch: 35: Step: 3501/4907, loss=0.000002, lr=0.000002
Epoch: 35: Step: 3501/4907, loss=0.000002, lr=0.000002
Train batch 3600
Avg. loss per last 100 batches: 0.020577
Train batch 3600
Avg. loss per last 100 batches: 0.020577
Epoch: 35: Step: 3601/4907, loss=0.000007, lr=0.000002
Epoch: 35: Step: 3601/4907, loss=0.000007, lr=0.000002
Train batch 3700
Avg. loss per last 100 batches: 0.008094
Train batch 3700
Avg. loss per last 100 batches: 0.008094
Epoch: 35: Step: 3701/4907, loss=0.000001, lr=0.000002
Epoch: 35: Step: 3701/4907, loss=0.000001, lr=0.000002
Train batch 3800
Avg. loss per last 100 batches: 0.013527
Train batch 3800
Avg. loss per last 100 batches: 0.013527
Epoch: 35: Step: 3801/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 3801/4907, loss=0.000000, lr=0.000002
Train batch 3900
Avg. loss per last 100 batches: 0.015919
Train batch 3900
Avg. loss per last 100 batches: 0.015919
Epoch: 35: Step: 3901/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 3901/4907, loss=0.000000, lr=0.000002
Train batch 4000
Avg. loss per last 100 batches: 0.011760
Train batch 4000
Avg. loss per last 100 batches: 0.011760
Epoch: 35: Step: 4001/4907, loss=0.631049, lr=0.000002
Epoch: 35: Step: 4001/4907, loss=0.631049, lr=0.000002
Train batch 4100
Avg. loss per last 100 batches: 0.016743
Train batch 4100
Avg. loss per last 100 batches: 0.016743
Epoch: 35: Step: 4101/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 4101/4907, loss=0.000000, lr=0.000002
Train batch 4200
Avg. loss per last 100 batches: 0.016689
Train batch 4200
Avg. loss per last 100 batches: 0.016689
Epoch: 35: Step: 4201/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 4201/4907, loss=0.000000, lr=0.000002
Train batch 4300
Avg. loss per last 100 batches: 0.012361
Train batch 4300
Avg. loss per last 100 batches: 0.012361
Epoch: 35: Step: 4301/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 4301/4907, loss=0.000000, lr=0.000002
Train batch 4400
Avg. loss per last 100 batches: 0.018154
Train batch 4400
Avg. loss per last 100 batches: 0.018154
Epoch: 35: Step: 4401/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 4401/4907, loss=0.000000, lr=0.000002
Train batch 4500
Avg. loss per last 100 batches: 0.019257
Train batch 4500
Avg. loss per last 100 batches: 0.019257
Epoch: 35: Step: 4501/4907, loss=0.918420, lr=0.000002
Epoch: 35: Step: 4501/4907, loss=0.918420, lr=0.000002
Train batch 4600
Avg. loss per last 100 batches: 0.042894
Train batch 4600
Avg. loss per last 100 batches: 0.042894
Epoch: 35: Step: 4601/4907, loss=0.000000, lr=0.000002
Epoch: 35: Step: 4601/4907, loss=0.000000, lr=0.000002
Train batch 4700
Avg. loss per last 100 batches: 0.020051
Train batch 4700
Avg. loss per last 100 batches: 0.020051
Epoch: 35: Step: 4701/4907, loss=0.073687, lr=0.000002
Epoch: 35: Step: 4701/4907, loss=0.073687, lr=0.000002
Train batch 4800
Avg. loss per last 100 batches: 0.023278
Train batch 4800
Avg. loss per last 100 batches: 0.023278
Epoch: 35: Step: 4801/4907, loss=0.000001, lr=0.000002
Epoch: 35: Step: 4801/4907, loss=0.000001, lr=0.000002
Train batch 4900
Avg. loss per last 100 batches: 0.027036
Train batch 4900
Avg. loss per last 100 batches: 0.027036
Epoch: 35: Step: 4901/4907, loss=0.000073, lr=0.000002
Epoch: 35: Step: 4901/4907, loss=0.000073, lr=0.000002
Validation: Epoch: 35 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 35 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 154.25997544505833, total questions=6516
Av.rank validation: average rank 154.25997544505833, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.35.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.35.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.35.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 154.25997544505833, total questions=6516
Av.rank validation: average rank 154.25997544505833, total questions=6516
Av Loss per epoch=0.015740
epoch total correct predictions=58630
***** Epoch 36 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.35.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.35.4907
Av Loss per epoch=0.015740
epoch total correct predictions=58630
***** Epoch 36 *****
Epoch: 36: Step: 1/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 1/4907, loss=0.000000, lr=0.000002
Train batch 100
Avg. loss per last 100 batches: 0.013830
Train batch 100
Avg. loss per last 100 batches: 0.013830
Epoch: 36: Step: 101/4907, loss=0.000055, lr=0.000002
Epoch: 36: Step: 101/4907, loss=0.000055, lr=0.000002
Train batch 200
Avg. loss per last 100 batches: 0.022038
Train batch 200
Avg. loss per last 100 batches: 0.022038
Epoch: 36: Step: 201/4907, loss=0.000001, lr=0.000002
Epoch: 36: Step: 201/4907, loss=0.000001, lr=0.000002
Train batch 300
Avg. loss per last 100 batches: 0.015515
Train batch 300
Avg. loss per last 100 batches: 0.015515
Epoch: 36: Step: 301/4907, loss=0.000009, lr=0.000002
Epoch: 36: Step: 301/4907, loss=0.000009, lr=0.000002
Train batch 400
Avg. loss per last 100 batches: 0.010943
Train batch 400
Avg. loss per last 100 batches: 0.010943
Epoch: 36: Step: 401/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 401/4907, loss=0.000000, lr=0.000002
Train batch 500
Avg. loss per last 100 batches: 0.013229
Train batch 500
Avg. loss per last 100 batches: 0.013229
Epoch: 36: Step: 501/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 501/4907, loss=0.000000, lr=0.000002
Train batch 600
Avg. loss per last 100 batches: 0.027242
Train batch 600
Avg. loss per last 100 batches: 0.027242
Epoch: 36: Step: 601/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 601/4907, loss=0.000000, lr=0.000002
Train batch 700
Avg. loss per last 100 batches: 0.025497
Train batch 700
Avg. loss per last 100 batches: 0.025497
Epoch: 36: Step: 701/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 701/4907, loss=0.000000, lr=0.000002
Train batch 800
Avg. loss per last 100 batches: 0.020551
Train batch 800
Avg. loss per last 100 batches: 0.020551
Epoch: 36: Step: 801/4907, loss=0.003032, lr=0.000002
Epoch: 36: Step: 801/4907, loss=0.003032, lr=0.000002
Train batch 900
Avg. loss per last 100 batches: 0.006643
Train batch 900
Avg. loss per last 100 batches: 0.006643
Epoch: 36: Step: 901/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 901/4907, loss=0.000000, lr=0.000002
Train batch 1000
Avg. loss per last 100 batches: 0.004755
Train batch 1000
Avg. loss per last 100 batches: 0.004755
Epoch: 36: Step: 1001/4907, loss=0.000590, lr=0.000002
Epoch: 36: Step: 1001/4907, loss=0.000590, lr=0.000002
Train batch 1100
Avg. loss per last 100 batches: 0.006150
Train batch 1100
Avg. loss per last 100 batches: 0.006150
Epoch: 36: Step: 1101/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 1101/4907, loss=0.000000, lr=0.000002
Train batch 1200
Avg. loss per last 100 batches: 0.021579
Train batch 1200
Avg. loss per last 100 batches: 0.021579
Epoch: 36: Step: 1201/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 1201/4907, loss=0.000000, lr=0.000002
Train batch 1300
Avg. loss per last 100 batches: 0.017663
Train batch 1300
Avg. loss per last 100 batches: 0.017663
Epoch: 36: Step: 1301/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 1301/4907, loss=0.000000, lr=0.000002
Train batch 1400
Avg. loss per last 100 batches: 0.026439
Train batch 1400
Avg. loss per last 100 batches: 0.026439
Epoch: 36: Step: 1401/4907, loss=0.000086, lr=0.000002
Epoch: 36: Step: 1401/4907, loss=0.000086, lr=0.000002
Train batch 1500
Avg. loss per last 100 batches: 0.010347
Train batch 1500
Avg. loss per last 100 batches: 0.010347
Epoch: 36: Step: 1501/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 1501/4907, loss=0.000000, lr=0.000002
Train batch 1600
Avg. loss per last 100 batches: 0.006530
Train batch 1600
Avg. loss per last 100 batches: 0.006530
Epoch: 36: Step: 1601/4907, loss=0.007673, lr=0.000002
Epoch: 36: Step: 1601/4907, loss=0.007673, lr=0.000002
Train batch 1700
Avg. loss per last 100 batches: 0.033107
Train batch 1700
Avg. loss per last 100 batches: 0.033107
Epoch: 36: Step: 1701/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 1701/4907, loss=0.000000, lr=0.000002
Train batch 1800
Avg. loss per last 100 batches: 0.012414
Train batch 1800
Avg. loss per last 100 batches: 0.012414
Epoch: 36: Step: 1801/4907, loss=0.001046, lr=0.000002
Epoch: 36: Step: 1801/4907, loss=0.001046, lr=0.000002
Train batch 1900
Avg. loss per last 100 batches: 0.013001
Train batch 1900
Avg. loss per last 100 batches: 0.013001
Epoch: 36: Step: 1901/4907, loss=0.023968, lr=0.000002
Epoch: 36: Step: 1901/4907, loss=0.023968, lr=0.000002
Train batch 2000
Avg. loss per last 100 batches: 0.010523
Train batch 2000
Avg. loss per last 100 batches: 0.010523
Epoch: 36: Step: 2001/4907, loss=0.003116, lr=0.000002
Epoch: 36: Step: 2001/4907, loss=0.003116, lr=0.000002
Train batch 2100
Avg. loss per last 100 batches: 0.007412
Train batch 2100
Avg. loss per last 100 batches: 0.007412
Epoch: 36: Step: 2101/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 2101/4907, loss=0.000000, lr=0.000002
Train batch 2200
Avg. loss per last 100 batches: 0.005827
Train batch 2200
Avg. loss per last 100 batches: 0.005827
Epoch: 36: Step: 2201/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 2201/4907, loss=0.000000, lr=0.000002
Train batch 2300
Avg. loss per last 100 batches: 0.011407
Train batch 2300
Avg. loss per last 100 batches: 0.011407
Epoch: 36: Step: 2301/4907, loss=0.000002, lr=0.000002
Epoch: 36: Step: 2301/4907, loss=0.000002, lr=0.000002
Train batch 2400
Avg. loss per last 100 batches: 0.006586
Train batch 2400
Avg. loss per last 100 batches: 0.006586
Epoch: 36: Step: 2401/4907, loss=0.000027, lr=0.000002
Epoch: 36: Step: 2401/4907, loss=0.000027, lr=0.000002
Train batch 2500
Avg. loss per last 100 batches: 0.010966
Train batch 2500
Avg. loss per last 100 batches: 0.010966
Epoch: 36: Step: 2501/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 2501/4907, loss=0.000000, lr=0.000002
Train batch 2600
Avg. loss per last 100 batches: 0.018906
Train batch 2600
Avg. loss per last 100 batches: 0.018906
Epoch: 36: Step: 2601/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 2601/4907, loss=0.000000, lr=0.000002
Train batch 2700
Avg. loss per last 100 batches: 0.026601
Train batch 2700
Avg. loss per last 100 batches: 0.026601
Epoch: 36: Step: 2701/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 2701/4907, loss=0.000000, lr=0.000002
Train batch 2800
Avg. loss per last 100 batches: 0.040604
Train batch 2800
Avg. loss per last 100 batches: 0.040604
Epoch: 36: Step: 2801/4907, loss=0.000675, lr=0.000002
Epoch: 36: Step: 2801/4907, loss=0.000675, lr=0.000002
Train batch 2900
Avg. loss per last 100 batches: 0.017923
Train batch 2900
Avg. loss per last 100 batches: 0.017923
Epoch: 36: Step: 2901/4907, loss=0.004771, lr=0.000002
Epoch: 36: Step: 2901/4907, loss=0.004771, lr=0.000002
Train batch 3000
Avg. loss per last 100 batches: 0.016047
Train batch 3000
Avg. loss per last 100 batches: 0.016047
Epoch: 36: Step: 3001/4907, loss=0.000001, lr=0.000002
Epoch: 36: Step: 3001/4907, loss=0.000001, lr=0.000002
Train batch 3100
Avg. loss per last 100 batches: 0.009891
Train batch 3100
Avg. loss per last 100 batches: 0.009891
Epoch: 36: Step: 3101/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 3101/4907, loss=0.000000, lr=0.000002
Train batch 3200
Avg. loss per last 100 batches: 0.009421
Train batch 3200
Avg. loss per last 100 batches: 0.009421
Epoch: 36: Step: 3201/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 3201/4907, loss=0.000000, lr=0.000002
Train batch 3300
Avg. loss per last 100 batches: 0.010046
Train batch 3300
Avg. loss per last 100 batches: 0.010046
Epoch: 36: Step: 3301/4907, loss=0.057702, lr=0.000002
Epoch: 36: Step: 3301/4907, loss=0.057702, lr=0.000002
Train batch 3400
Avg. loss per last 100 batches: 0.014285
Train batch 3400
Avg. loss per last 100 batches: 0.014285
Epoch: 36: Step: 3401/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 3401/4907, loss=0.000000, lr=0.000002
Train batch 3500
Avg. loss per last 100 batches: 0.009080
Train batch 3500
Avg. loss per last 100 batches: 0.009080
Epoch: 36: Step: 3501/4907, loss=0.021977, lr=0.000002
Epoch: 36: Step: 3501/4907, loss=0.021977, lr=0.000002
Train batch 3600
Avg. loss per last 100 batches: 0.014212
Train batch 3600
Avg. loss per last 100 batches: 0.014212
Epoch: 36: Step: 3601/4907, loss=0.000004, lr=0.000002
Epoch: 36: Step: 3601/4907, loss=0.000004, lr=0.000002
Train batch 3700
Avg. loss per last 100 batches: 0.010987
Train batch 3700
Avg. loss per last 100 batches: 0.010987
Epoch: 36: Step: 3701/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 3701/4907, loss=0.000000, lr=0.000002
Train batch 3800
Avg. loss per last 100 batches: 0.004695
Train batch 3800
Avg. loss per last 100 batches: 0.004695
Epoch: 36: Step: 3801/4907, loss=0.000557, lr=0.000002
Epoch: 36: Step: 3801/4907, loss=0.000557, lr=0.000002
Train batch 3900
Avg. loss per last 100 batches: 0.007891
Train batch 3900
Avg. loss per last 100 batches: 0.007891
Epoch: 36: Step: 3901/4907, loss=0.005575, lr=0.000002
Epoch: 36: Step: 3901/4907, loss=0.005575, lr=0.000002
Train batch 4000
Avg. loss per last 100 batches: 0.011177
Train batch 4000
Avg. loss per last 100 batches: 0.011177
Epoch: 36: Step: 4001/4907, loss=0.000001, lr=0.000002
Epoch: 36: Step: 4001/4907, loss=0.000001, lr=0.000002
Train batch 4100
Avg. loss per last 100 batches: 0.017037
Train batch 4100
Avg. loss per last 100 batches: 0.017037
Epoch: 36: Step: 4101/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 4101/4907, loss=0.000000, lr=0.000002
Train batch 4200
Avg. loss per last 100 batches: 0.009311
Train batch 4200
Avg. loss per last 100 batches: 0.009311
Epoch: 36: Step: 4201/4907, loss=0.000002, lr=0.000002
Epoch: 36: Step: 4201/4907, loss=0.000002, lr=0.000002
Train batch 4300
Avg. loss per last 100 batches: 0.017191
Train batch 4300
Avg. loss per last 100 batches: 0.017191
Epoch: 36: Step: 4301/4907, loss=0.000385, lr=0.000002
Epoch: 36: Step: 4301/4907, loss=0.000385, lr=0.000002
Train batch 4400
Avg. loss per last 100 batches: 0.014569
Train batch 4400
Avg. loss per last 100 batches: 0.014569
Epoch: 36: Step: 4401/4907, loss=0.003059, lr=0.000002
Epoch: 36: Step: 4401/4907, loss=0.003059, lr=0.000002
Train batch 4500
Avg. loss per last 100 batches: 0.017672
Train batch 4500
Avg. loss per last 100 batches: 0.017672
Epoch: 36: Step: 4501/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 4501/4907, loss=0.000000, lr=0.000002
Train batch 4600
Avg. loss per last 100 batches: 0.020281
Train batch 4600
Avg. loss per last 100 batches: 0.020281
Epoch: 36: Step: 4601/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 4601/4907, loss=0.000000, lr=0.000002
Train batch 4700
Avg. loss per last 100 batches: 0.016649
Train batch 4700
Avg. loss per last 100 batches: 0.016649
Epoch: 36: Step: 4701/4907, loss=0.000001, lr=0.000002
Epoch: 36: Step: 4701/4907, loss=0.000001, lr=0.000002
Train batch 4800
Avg. loss per last 100 batches: 0.006265
Train batch 4800
Avg. loss per last 100 batches: 0.006265
Epoch: 36: Step: 4801/4907, loss=0.253082, lr=0.000002
Epoch: 36: Step: 4801/4907, loss=0.253082, lr=0.000002
Train batch 4900
Avg. loss per last 100 batches: 0.021412
Train batch 4900
Avg. loss per last 100 batches: 0.021412
Epoch: 36: Step: 4901/4907, loss=0.000000, lr=0.000002
Epoch: 36: Step: 4901/4907, loss=0.000000, lr=0.000002
Validation: Epoch: 36 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 36 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 156.67081031307552, total questions=6516
Av.rank validation: average rank 156.67081031307552, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.36.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.36.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 156.67081031307552, total questions=6516
Av.rank validation: average rank 156.67081031307552, total questions=6516
Av Loss per epoch=0.014721
epoch total correct predictions=58643
***** Epoch 37 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.36.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.36.4907
Av Loss per epoch=0.014721
epoch total correct predictions=58643
***** Epoch 37 *****
Epoch: 37: Step: 1/4907, loss=0.000000, lr=0.000002
Epoch: 37: Step: 1/4907, loss=0.000000, lr=0.000002
Train batch 100
Avg. loss per last 100 batches: 0.017935
Train batch 100
Avg. loss per last 100 batches: 0.017935
Epoch: 37: Step: 101/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 101/4907, loss=0.000000, lr=0.000001
Train batch 200
Avg. loss per last 100 batches: 0.015897
Train batch 200
Avg. loss per last 100 batches: 0.015897
Epoch: 37: Step: 201/4907, loss=0.416064, lr=0.000001
Epoch: 37: Step: 201/4907, loss=0.416064, lr=0.000001
Train batch 300
Avg. loss per last 100 batches: 0.014334
Train batch 300
Avg. loss per last 100 batches: 0.014334
Epoch: 37: Step: 301/4907, loss=0.000019, lr=0.000001
Epoch: 37: Step: 301/4907, loss=0.000019, lr=0.000001
Train batch 400
Avg. loss per last 100 batches: 0.019306
Train batch 400
Avg. loss per last 100 batches: 0.019306
Epoch: 37: Step: 401/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 401/4907, loss=0.000000, lr=0.000001
Train batch 500
Avg. loss per last 100 batches: 0.015810
Train batch 500
Avg. loss per last 100 batches: 0.015810
Epoch: 37: Step: 501/4907, loss=0.003113, lr=0.000001
Epoch: 37: Step: 501/4907, loss=0.003113, lr=0.000001
Train batch 600
Avg. loss per last 100 batches: 0.015017
Train batch 600
Avg. loss per last 100 batches: 0.015017
Epoch: 37: Step: 601/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 601/4907, loss=0.000000, lr=0.000001
Train batch 700
Avg. loss per last 100 batches: 0.017974
Train batch 700
Avg. loss per last 100 batches: 0.017974
Epoch: 37: Step: 701/4907, loss=0.000009, lr=0.000001
Epoch: 37: Step: 701/4907, loss=0.000009, lr=0.000001
Train batch 800
Avg. loss per last 100 batches: 0.015085
Train batch 800
Avg. loss per last 100 batches: 0.015085
Epoch: 37: Step: 801/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 801/4907, loss=0.000000, lr=0.000001
Train batch 900
Avg. loss per last 100 batches: 0.027819
Train batch 900
Avg. loss per last 100 batches: 0.027819
Epoch: 37: Step: 901/4907, loss=0.000042, lr=0.000001
Epoch: 37: Step: 901/4907, loss=0.000042, lr=0.000001
Train batch 1000
Avg. loss per last 100 batches: 0.033454
Train batch 1000
Avg. loss per last 100 batches: 0.033454
Epoch: 37: Step: 1001/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 1001/4907, loss=0.000000, lr=0.000001
Train batch 1100
Avg. loss per last 100 batches: 0.005508
Train batch 1100
Avg. loss per last 100 batches: 0.005508
Epoch: 37: Step: 1101/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 1101/4907, loss=0.000000, lr=0.000001
Train batch 1200
Avg. loss per last 100 batches: 0.014515
Train batch 1200
Avg. loss per last 100 batches: 0.014515
Epoch: 37: Step: 1201/4907, loss=0.001656, lr=0.000001
Epoch: 37: Step: 1201/4907, loss=0.001656, lr=0.000001
Train batch 1300
Avg. loss per last 100 batches: 0.008343
Train batch 1300
Avg. loss per last 100 batches: 0.008343
Epoch: 37: Step: 1301/4907, loss=0.000006, lr=0.000001
Epoch: 37: Step: 1301/4907, loss=0.000006, lr=0.000001
Train batch 1400
Avg. loss per last 100 batches: 0.011871
Train batch 1400
Avg. loss per last 100 batches: 0.011871
Epoch: 37: Step: 1401/4907, loss=0.000021, lr=0.000001
Epoch: 37: Step: 1401/4907, loss=0.000021, lr=0.000001
Train batch 1500
Avg. loss per last 100 batches: 0.018034
Train batch 1500
Avg. loss per last 100 batches: 0.018034
Epoch: 37: Step: 1501/4907, loss=0.000056, lr=0.000001
Epoch: 37: Step: 1501/4907, loss=0.000056, lr=0.000001
Train batch 1600
Avg. loss per last 100 batches: 0.002637
Train batch 1600
Avg. loss per last 100 batches: 0.002637
Epoch: 37: Step: 1601/4907, loss=0.000144, lr=0.000001
Epoch: 37: Step: 1601/4907, loss=0.000144, lr=0.000001
Train batch 1700
Avg. loss per last 100 batches: 0.015653
Train batch 1700
Avg. loss per last 100 batches: 0.015653
Epoch: 37: Step: 1701/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 1701/4907, loss=0.000000, lr=0.000001
Train batch 1800
Avg. loss per last 100 batches: 0.008314
Train batch 1800
Avg. loss per last 100 batches: 0.008314
Epoch: 37: Step: 1801/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 1801/4907, loss=0.000000, lr=0.000001
Train batch 1900
Avg. loss per last 100 batches: 0.006330
Train batch 1900
Avg. loss per last 100 batches: 0.006330
Epoch: 37: Step: 1901/4907, loss=0.000259, lr=0.000001
Epoch: 37: Step: 1901/4907, loss=0.000259, lr=0.000001
Train batch 2000
Avg. loss per last 100 batches: 0.020622
Train batch 2000
Avg. loss per last 100 batches: 0.020622
Epoch: 37: Step: 2001/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 2001/4907, loss=0.000000, lr=0.000001
Train batch 2100
Avg. loss per last 100 batches: 0.005536
Train batch 2100
Avg. loss per last 100 batches: 0.005536
Epoch: 37: Step: 2101/4907, loss=0.000112, lr=0.000001
Epoch: 37: Step: 2101/4907, loss=0.000112, lr=0.000001
Train batch 2200
Avg. loss per last 100 batches: 0.004498
Train batch 2200
Avg. loss per last 100 batches: 0.004498
Epoch: 37: Step: 2201/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 2201/4907, loss=0.000000, lr=0.000001
Train batch 2300
Avg. loss per last 100 batches: 0.015580
Train batch 2300
Avg. loss per last 100 batches: 0.015580
Epoch: 37: Step: 2301/4907, loss=0.000075, lr=0.000001
Epoch: 37: Step: 2301/4907, loss=0.000075, lr=0.000001
Train batch 2400
Avg. loss per last 100 batches: 0.011065
Train batch 2400
Avg. loss per last 100 batches: 0.011065
Epoch: 37: Step: 2401/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 2401/4907, loss=0.000000, lr=0.000001
Train batch 2500
Avg. loss per last 100 batches: 0.023631
Train batch 2500
Avg. loss per last 100 batches: 0.023631
Epoch: 37: Step: 2501/4907, loss=0.109236, lr=0.000001
Epoch: 37: Step: 2501/4907, loss=0.109236, lr=0.000001
Train batch 2600
Avg. loss per last 100 batches: 0.018458
Train batch 2600
Avg. loss per last 100 batches: 0.018458
Epoch: 37: Step: 2601/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 2601/4907, loss=0.000000, lr=0.000001
Train batch 2700
Avg. loss per last 100 batches: 0.015814
Train batch 2700
Avg. loss per last 100 batches: 0.015814
Epoch: 37: Step: 2701/4907, loss=0.000007, lr=0.000001
Epoch: 37: Step: 2701/4907, loss=0.000007, lr=0.000001
Train batch 2800
Avg. loss per last 100 batches: 0.021576
Train batch 2800
Avg. loss per last 100 batches: 0.021576
Epoch: 37: Step: 2801/4907, loss=0.201129, lr=0.000001
Epoch: 37: Step: 2801/4907, loss=0.201129, lr=0.000001
Train batch 2900
Avg. loss per last 100 batches: 0.018219
Train batch 2900
Avg. loss per last 100 batches: 0.018219
Epoch: 37: Step: 2901/4907, loss=0.004649, lr=0.000001
Epoch: 37: Step: 2901/4907, loss=0.004649, lr=0.000001
Train batch 3000
Avg. loss per last 100 batches: 0.032232
Train batch 3000
Avg. loss per last 100 batches: 0.032232
Epoch: 37: Step: 3001/4907, loss=0.757799, lr=0.000001
Epoch: 37: Step: 3001/4907, loss=0.757799, lr=0.000001
Train batch 3100
Avg. loss per last 100 batches: 0.019910
Train batch 3100
Avg. loss per last 100 batches: 0.019910
Epoch: 37: Step: 3101/4907, loss=0.003086, lr=0.000001
Epoch: 37: Step: 3101/4907, loss=0.003086, lr=0.000001
Train batch 3200
Avg. loss per last 100 batches: 0.020345
Train batch 3200
Avg. loss per last 100 batches: 0.020345
Epoch: 37: Step: 3201/4907, loss=0.000099, lr=0.000001
Epoch: 37: Step: 3201/4907, loss=0.000099, lr=0.000001
Train batch 3300
Avg. loss per last 100 batches: 0.012930
Train batch 3300
Avg. loss per last 100 batches: 0.012930
Epoch: 37: Step: 3301/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 3301/4907, loss=0.000000, lr=0.000001
Train batch 3400
Avg. loss per last 100 batches: 0.001931
Train batch 3400
Avg. loss per last 100 batches: 0.001931
Epoch: 37: Step: 3401/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 3401/4907, loss=0.000000, lr=0.000001
Train batch 3500
Avg. loss per last 100 batches: 0.012140
Train batch 3500
Avg. loss per last 100 batches: 0.012140
Epoch: 37: Step: 3501/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 3501/4907, loss=0.000000, lr=0.000001
Train batch 3600
Avg. loss per last 100 batches: 0.008672
Train batch 3600
Avg. loss per last 100 batches: 0.008672
Epoch: 37: Step: 3601/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 3601/4907, loss=0.000000, lr=0.000001
Train batch 3700
Avg. loss per last 100 batches: 0.015598
Train batch 3700
Avg. loss per last 100 batches: 0.015598
Epoch: 37: Step: 3701/4907, loss=0.000004, lr=0.000001
Epoch: 37: Step: 3701/4907, loss=0.000004, lr=0.000001
Train batch 3800
Avg. loss per last 100 batches: 0.002941
Train batch 3800
Avg. loss per last 100 batches: 0.002941
Epoch: 37: Step: 3801/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 3801/4907, loss=0.000000, lr=0.000001
Train batch 3900
Avg. loss per last 100 batches: 0.034446
Train batch 3900
Avg. loss per last 100 batches: 0.034446
Epoch: 37: Step: 3901/4907, loss=0.008447, lr=0.000001
Epoch: 37: Step: 3901/4907, loss=0.008447, lr=0.000001
Train batch 4000
Avg. loss per last 100 batches: 0.017707
Train batch 4000
Avg. loss per last 100 batches: 0.017707
Epoch: 37: Step: 4001/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 4001/4907, loss=0.000000, lr=0.000001
Train batch 4100
Avg. loss per last 100 batches: 0.028290
Train batch 4100
Avg. loss per last 100 batches: 0.028290
Epoch: 37: Step: 4101/4907, loss=0.000184, lr=0.000001
Epoch: 37: Step: 4101/4907, loss=0.000184, lr=0.000001
Train batch 4200
Avg. loss per last 100 batches: 0.006570
Train batch 4200
Avg. loss per last 100 batches: 0.006570
Epoch: 37: Step: 4201/4907, loss=0.000001, lr=0.000001
Epoch: 37: Step: 4201/4907, loss=0.000001, lr=0.000001
Train batch 4300
Avg. loss per last 100 batches: 0.014545
Train batch 4300
Avg. loss per last 100 batches: 0.014545
Epoch: 37: Step: 4301/4907, loss=0.155994, lr=0.000001
Epoch: 37: Step: 4301/4907, loss=0.155994, lr=0.000001
Train batch 4400
Avg. loss per last 100 batches: 0.023476
Train batch 4400
Avg. loss per last 100 batches: 0.023476
Epoch: 37: Step: 4401/4907, loss=0.000011, lr=0.000001
Epoch: 37: Step: 4401/4907, loss=0.000011, lr=0.000001
Train batch 4500
Avg. loss per last 100 batches: 0.011351
Train batch 4500
Avg. loss per last 100 batches: 0.011351
Epoch: 37: Step: 4501/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 4501/4907, loss=0.000000, lr=0.000001
Train batch 4600
Avg. loss per last 100 batches: 0.005245
Train batch 4600
Avg. loss per last 100 batches: 0.005245
Epoch: 37: Step: 4601/4907, loss=0.001630, lr=0.000001
Epoch: 37: Step: 4601/4907, loss=0.001630, lr=0.000001
Train batch 4700
Avg. loss per last 100 batches: 0.025148
Train batch 4700
Avg. loss per last 100 batches: 0.025148
Epoch: 37: Step: 4701/4907, loss=0.000000, lr=0.000001
Epoch: 37: Step: 4701/4907, loss=0.000000, lr=0.000001
Train batch 4800
Avg. loss per last 100 batches: 0.002561
Train batch 4800
Avg. loss per last 100 batches: 0.002561
Epoch: 37: Step: 4801/4907, loss=0.000257, lr=0.000001
Epoch: 37: Step: 4801/4907, loss=0.000257, lr=0.000001
Train batch 4900
Avg. loss per last 100 batches: 0.014700
Train batch 4900
Avg. loss per last 100 batches: 0.014700
Epoch: 37: Step: 4901/4907, loss=0.000013, lr=0.000001
Epoch: 37: Step: 4901/4907, loss=0.000013, lr=0.000001
Validation: Epoch: 37 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 37 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 153.75322283609577, total questions=6516
Av.rank validation: average rank 153.75322283609577, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.37.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.37.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.37.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 153.75322283609577, total questions=6516
Av.rank validation: average rank 153.75322283609577, total questions=6516
Av Loss per epoch=0.015276
epoch total correct predictions=58636
***** Epoch 38 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.37.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.37.4907
Av Loss per epoch=0.015276
epoch total correct predictions=58636
***** Epoch 38 *****
Epoch: 38: Step: 1/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 1/4907, loss=0.000000, lr=0.000001
Train batch 100
Avg. loss per last 100 batches: 0.005832
Train batch 100
Avg. loss per last 100 batches: 0.005832
Epoch: 38: Step: 101/4907, loss=0.000009, lr=0.000001
Epoch: 38: Step: 101/4907, loss=0.000009, lr=0.000001
Train batch 200
Avg. loss per last 100 batches: 0.005677
Train batch 200
Avg. loss per last 100 batches: 0.005677
Epoch: 38: Step: 201/4907, loss=0.000001, lr=0.000001
Epoch: 38: Step: 201/4907, loss=0.000001, lr=0.000001
Train batch 300
Avg. loss per last 100 batches: 0.021574
Train batch 300
Avg. loss per last 100 batches: 0.021574
Epoch: 38: Step: 301/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 301/4907, loss=0.000000, lr=0.000001
Train batch 400
Avg. loss per last 100 batches: 0.009344
Train batch 400
Avg. loss per last 100 batches: 0.009344
Epoch: 38: Step: 401/4907, loss=0.000001, lr=0.000001
Epoch: 38: Step: 401/4907, loss=0.000001, lr=0.000001
Train batch 500
Avg. loss per last 100 batches: 0.004230
Train batch 500
Avg. loss per last 100 batches: 0.004230
Epoch: 38: Step: 501/4907, loss=0.000003, lr=0.000001
Epoch: 38: Step: 501/4907, loss=0.000003, lr=0.000001
Train batch 600
Avg. loss per last 100 batches: 0.017975
Train batch 600
Avg. loss per last 100 batches: 0.017975
Epoch: 38: Step: 601/4907, loss=0.000002, lr=0.000001
Epoch: 38: Step: 601/4907, loss=0.000002, lr=0.000001
Train batch 700
Avg. loss per last 100 batches: 0.024100
Train batch 700
Avg. loss per last 100 batches: 0.024100
Epoch: 38: Step: 701/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 701/4907, loss=0.000000, lr=0.000001
Train batch 800
Avg. loss per last 100 batches: 0.011981
Train batch 800
Avg. loss per last 100 batches: 0.011981
Epoch: 38: Step: 801/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 801/4907, loss=0.000000, lr=0.000001
Train batch 900
Avg. loss per last 100 batches: 0.008761
Train batch 900
Avg. loss per last 100 batches: 0.008761
Epoch: 38: Step: 901/4907, loss=0.126991, lr=0.000001
Epoch: 38: Step: 901/4907, loss=0.126991, lr=0.000001
Train batch 1000
Avg. loss per last 100 batches: 0.025081
Train batch 1000
Avg. loss per last 100 batches: 0.025081
Epoch: 38: Step: 1001/4907, loss=0.399502, lr=0.000001
Epoch: 38: Step: 1001/4907, loss=0.399502, lr=0.000001
Train batch 1100
Avg. loss per last 100 batches: 0.008262
Train batch 1100
Avg. loss per last 100 batches: 0.008262
Epoch: 38: Step: 1101/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 1101/4907, loss=0.000000, lr=0.000001
Train batch 1200
Avg. loss per last 100 batches: 0.012785
Train batch 1200
Avg. loss per last 100 batches: 0.012785
Epoch: 38: Step: 1201/4907, loss=0.000300, lr=0.000001
Epoch: 38: Step: 1201/4907, loss=0.000300, lr=0.000001
Train batch 1300
Avg. loss per last 100 batches: 0.007518
Train batch 1300
Avg. loss per last 100 batches: 0.007518
Epoch: 38: Step: 1301/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 1301/4907, loss=0.000000, lr=0.000001
Train batch 1400
Avg. loss per last 100 batches: 0.008023
Train batch 1400
Avg. loss per last 100 batches: 0.008023
Epoch: 38: Step: 1401/4907, loss=0.010376, lr=0.000001
Epoch: 38: Step: 1401/4907, loss=0.010376, lr=0.000001
Train batch 1500
Avg. loss per last 100 batches: 0.010602
Train batch 1500
Avg. loss per last 100 batches: 0.010602
Epoch: 38: Step: 1501/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 1501/4907, loss=0.000000, lr=0.000001
Train batch 1600
Avg. loss per last 100 batches: 0.014760
Train batch 1600
Avg. loss per last 100 batches: 0.014760
Epoch: 38: Step: 1601/4907, loss=0.000001, lr=0.000001
Epoch: 38: Step: 1601/4907, loss=0.000001, lr=0.000001
Train batch 1700
Avg. loss per last 100 batches: 0.017416
Train batch 1700
Avg. loss per last 100 batches: 0.017416
Epoch: 38: Step: 1701/4907, loss=0.000020, lr=0.000001
Epoch: 38: Step: 1701/4907, loss=0.000020, lr=0.000001
Train batch 1800
Avg. loss per last 100 batches: 0.009367
Train batch 1800
Avg. loss per last 100 batches: 0.009367
Epoch: 38: Step: 1801/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 1801/4907, loss=0.000000, lr=0.000001
Train batch 1900
Avg. loss per last 100 batches: 0.020421
Train batch 1900
Avg. loss per last 100 batches: 0.020421
Epoch: 38: Step: 1901/4907, loss=0.011139, lr=0.000001
Epoch: 38: Step: 1901/4907, loss=0.011139, lr=0.000001
Train batch 2000
Avg. loss per last 100 batches: 0.019399
Train batch 2000
Avg. loss per last 100 batches: 0.019399
Epoch: 38: Step: 2001/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 2001/4907, loss=0.000000, lr=0.000001
Train batch 2100
Avg. loss per last 100 batches: 0.041675
Train batch 2100
Avg. loss per last 100 batches: 0.041675
Epoch: 38: Step: 2101/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 2101/4907, loss=0.000000, lr=0.000001
Train batch 2200
Avg. loss per last 100 batches: 0.012491
Train batch 2200
Avg. loss per last 100 batches: 0.012491
Epoch: 38: Step: 2201/4907, loss=0.000014, lr=0.000001
Epoch: 38: Step: 2201/4907, loss=0.000014, lr=0.000001
Train batch 2300
Avg. loss per last 100 batches: 0.016526
Train batch 2300
Avg. loss per last 100 batches: 0.016526
Epoch: 38: Step: 2301/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 2301/4907, loss=0.000000, lr=0.000001
Train batch 2400
Avg. loss per last 100 batches: 0.018498
Train batch 2400
Avg. loss per last 100 batches: 0.018498
Epoch: 38: Step: 2401/4907, loss=0.000001, lr=0.000001
Epoch: 38: Step: 2401/4907, loss=0.000001, lr=0.000001
Train batch 2500
Avg. loss per last 100 batches: 0.012870
Train batch 2500
Avg. loss per last 100 batches: 0.012870
Epoch: 38: Step: 2501/4907, loss=0.000087, lr=0.000001
Epoch: 38: Step: 2501/4907, loss=0.000087, lr=0.000001
Train batch 2600
Avg. loss per last 100 batches: 0.009728
Train batch 2600
Avg. loss per last 100 batches: 0.009728
Epoch: 38: Step: 2601/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 2601/4907, loss=0.000000, lr=0.000001
Train batch 2700
Avg. loss per last 100 batches: 0.010293
Train batch 2700
Avg. loss per last 100 batches: 0.010293
Epoch: 38: Step: 2701/4907, loss=0.001897, lr=0.000001
Epoch: 38: Step: 2701/4907, loss=0.001897, lr=0.000001
Train batch 2800
Avg. loss per last 100 batches: 0.039046
Train batch 2800
Avg. loss per last 100 batches: 0.039046
Epoch: 38: Step: 2801/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 2801/4907, loss=0.000000, lr=0.000001
Train batch 2900
Avg. loss per last 100 batches: 0.009017
Train batch 2900
Avg. loss per last 100 batches: 0.009017
Epoch: 38: Step: 2901/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 2901/4907, loss=0.000000, lr=0.000001
Train batch 3000
Avg. loss per last 100 batches: 0.019868
Train batch 3000
Avg. loss per last 100 batches: 0.019868
Epoch: 38: Step: 3001/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 3001/4907, loss=0.000000, lr=0.000001
Train batch 3100
Avg. loss per last 100 batches: 0.012849
Train batch 3100
Avg. loss per last 100 batches: 0.012849
Epoch: 38: Step: 3101/4907, loss=0.069983, lr=0.000001
Epoch: 38: Step: 3101/4907, loss=0.069983, lr=0.000001
Train batch 3200
Avg. loss per last 100 batches: 0.015072
Train batch 3200
Avg. loss per last 100 batches: 0.015072
Epoch: 38: Step: 3201/4907, loss=0.000049, lr=0.000001
Epoch: 38: Step: 3201/4907, loss=0.000049, lr=0.000001
Train batch 3300
Avg. loss per last 100 batches: 0.014959
Train batch 3300
Avg. loss per last 100 batches: 0.014959
Epoch: 38: Step: 3301/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 3301/4907, loss=0.000000, lr=0.000001
Train batch 3400
Avg. loss per last 100 batches: 0.009116
Train batch 3400
Avg. loss per last 100 batches: 0.009116
Epoch: 38: Step: 3401/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 3401/4907, loss=0.000000, lr=0.000001
Train batch 3500
Avg. loss per last 100 batches: 0.011016
Train batch 3500
Avg. loss per last 100 batches: 0.011016
Epoch: 38: Step: 3501/4907, loss=0.000002, lr=0.000001
Epoch: 38: Step: 3501/4907, loss=0.000002, lr=0.000001
Train batch 3600
Avg. loss per last 100 batches: 0.016035
Train batch 3600
Avg. loss per last 100 batches: 0.016035
Epoch: 38: Step: 3601/4907, loss=0.000154, lr=0.000001
Epoch: 38: Step: 3601/4907, loss=0.000154, lr=0.000001
Train batch 3700
Avg. loss per last 100 batches: 0.023595
Train batch 3700
Avg. loss per last 100 batches: 0.023595
Epoch: 38: Step: 3701/4907, loss=0.000001, lr=0.000001
Epoch: 38: Step: 3701/4907, loss=0.000001, lr=0.000001
Train batch 3800
Avg. loss per last 100 batches: 0.025258
Train batch 3800
Avg. loss per last 100 batches: 0.025258
Epoch: 38: Step: 3801/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 3801/4907, loss=0.000000, lr=0.000001
Train batch 3900
Avg. loss per last 100 batches: 0.006232
Train batch 3900
Avg. loss per last 100 batches: 0.006232
Epoch: 38: Step: 3901/4907, loss=0.000001, lr=0.000001
Epoch: 38: Step: 3901/4907, loss=0.000001, lr=0.000001
Train batch 4000
Avg. loss per last 100 batches: 0.010767
Train batch 4000
Avg. loss per last 100 batches: 0.010767
Epoch: 38: Step: 4001/4907, loss=0.000035, lr=0.000001
Epoch: 38: Step: 4001/4907, loss=0.000035, lr=0.000001
Train batch 4100
Avg. loss per last 100 batches: 0.012495
Train batch 4100
Avg. loss per last 100 batches: 0.012495
Epoch: 38: Step: 4101/4907, loss=0.000760, lr=0.000001
Epoch: 38: Step: 4101/4907, loss=0.000760, lr=0.000001
Train batch 4200
Avg. loss per last 100 batches: 0.004072
Train batch 4200
Avg. loss per last 100 batches: 0.004072
Epoch: 38: Step: 4201/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 4201/4907, loss=0.000000, lr=0.000001
Train batch 4300
Avg. loss per last 100 batches: 0.002527
Train batch 4300
Avg. loss per last 100 batches: 0.002527
Epoch: 38: Step: 4301/4907, loss=0.000026, lr=0.000001
Epoch: 38: Step: 4301/4907, loss=0.000026, lr=0.000001
Train batch 4400
Avg. loss per last 100 batches: 0.017717
Train batch 4400
Avg. loss per last 100 batches: 0.017717
Epoch: 38: Step: 4401/4907, loss=0.000001, lr=0.000001
Epoch: 38: Step: 4401/4907, loss=0.000001, lr=0.000001
Train batch 4500
Avg. loss per last 100 batches: 0.005720
Train batch 4500
Avg. loss per last 100 batches: 0.005720
Epoch: 38: Step: 4501/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 4501/4907, loss=0.000000, lr=0.000001
Train batch 4600
Avg. loss per last 100 batches: 0.012409
Train batch 4600
Avg. loss per last 100 batches: 0.012409
Epoch: 38: Step: 4601/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 4601/4907, loss=0.000000, lr=0.000001
Train batch 4700
Avg. loss per last 100 batches: 0.008438
Train batch 4700
Avg. loss per last 100 batches: 0.008438
Epoch: 38: Step: 4701/4907, loss=0.000000, lr=0.000001
Epoch: 38: Step: 4701/4907, loss=0.000000, lr=0.000001
Train batch 4800
Avg. loss per last 100 batches: 0.016655
Train batch 4800
Avg. loss per last 100 batches: 0.016655
Epoch: 38: Step: 4801/4907, loss=0.286739, lr=0.000001
Epoch: 38: Step: 4801/4907, loss=0.286739, lr=0.000001
Train batch 4900
Avg. loss per last 100 batches: 0.014713
Train batch 4900
Avg. loss per last 100 batches: 0.014713
Epoch: 38: Step: 4901/4907, loss=0.021766, lr=0.000001
Epoch: 38: Step: 4901/4907, loss=0.021766, lr=0.000001
Validation: Epoch: 38 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 38 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 146.30586249232658, total questions=6516
Av.rank validation: average rank 146.30586249232658, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.38.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.38.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.38.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 146.30586249232658, total questions=6516
Av.rank validation: average rank 146.30586249232658, total questions=6516
Av Loss per epoch=0.014143
epoch total correct predictions=58669
***** Epoch 39 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.38.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.38.4907
Av Loss per epoch=0.014143
epoch total correct predictions=58669
***** Epoch 39 *****
Epoch: 39: Step: 1/4907, loss=0.000001, lr=0.000001
Epoch: 39: Step: 1/4907, loss=0.000001, lr=0.000001
Train batch 100
Avg. loss per last 100 batches: 0.005790
Train batch 100
Avg. loss per last 100 batches: 0.005790
Epoch: 39: Step: 101/4907, loss=0.000001, lr=0.000000
Epoch: 39: Step: 101/4907, loss=0.000001, lr=0.000000
Train batch 200
Avg. loss per last 100 batches: 0.014780
Train batch 200
Avg. loss per last 100 batches: 0.014780
Epoch: 39: Step: 201/4907, loss=0.000074, lr=0.000000
Epoch: 39: Step: 201/4907, loss=0.000074, lr=0.000000
Train batch 300
Avg. loss per last 100 batches: 0.014649
Train batch 300
Avg. loss per last 100 batches: 0.014649
Epoch: 39: Step: 301/4907, loss=0.005208, lr=0.000000
Epoch: 39: Step: 301/4907, loss=0.005208, lr=0.000000
Train batch 400
Avg. loss per last 100 batches: 0.020016
Train batch 400
Avg. loss per last 100 batches: 0.020016
Epoch: 39: Step: 401/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 401/4907, loss=0.000000, lr=0.000000
Train batch 500
Avg. loss per last 100 batches: 0.009192
Train batch 500
Avg. loss per last 100 batches: 0.009192
Epoch: 39: Step: 501/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 501/4907, loss=0.000000, lr=0.000000
Train batch 600
Avg. loss per last 100 batches: 0.005003
Train batch 600
Avg. loss per last 100 batches: 0.005003
Epoch: 39: Step: 601/4907, loss=0.328172, lr=0.000000
Epoch: 39: Step: 601/4907, loss=0.328172, lr=0.000000
Train batch 700
Avg. loss per last 100 batches: 0.022227
Train batch 700
Avg. loss per last 100 batches: 0.022227
Epoch: 39: Step: 701/4907, loss=0.000013, lr=0.000000
Epoch: 39: Step: 701/4907, loss=0.000013, lr=0.000000
Train batch 800
Avg. loss per last 100 batches: 0.022725
Train batch 800
Avg. loss per last 100 batches: 0.022725
Epoch: 39: Step: 801/4907, loss=0.000168, lr=0.000000
Epoch: 39: Step: 801/4907, loss=0.000168, lr=0.000000
Train batch 900
Avg. loss per last 100 batches: 0.016572
Train batch 900
Avg. loss per last 100 batches: 0.016572
Epoch: 39: Step: 901/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 901/4907, loss=0.000000, lr=0.000000
Train batch 1000
Avg. loss per last 100 batches: 0.021178
Train batch 1000
Avg. loss per last 100 batches: 0.021178
Epoch: 39: Step: 1001/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 1001/4907, loss=0.000000, lr=0.000000
Train batch 1100
Avg. loss per last 100 batches: 0.027943
Train batch 1100
Avg. loss per last 100 batches: 0.027943
Epoch: 39: Step: 1101/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 1101/4907, loss=0.000000, lr=0.000000
Train batch 1200
Avg. loss per last 100 batches: 0.011512
Train batch 1200
Avg. loss per last 100 batches: 0.011512
Epoch: 39: Step: 1201/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 1201/4907, loss=0.000000, lr=0.000000
Train batch 1300
Avg. loss per last 100 batches: 0.007695
Train batch 1300
Avg. loss per last 100 batches: 0.007695
Epoch: 39: Step: 1301/4907, loss=0.000005, lr=0.000000
Epoch: 39: Step: 1301/4907, loss=0.000005, lr=0.000000
Train batch 1400
Avg. loss per last 100 batches: 0.017838
Train batch 1400
Avg. loss per last 100 batches: 0.017838
Epoch: 39: Step: 1401/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 1401/4907, loss=0.000000, lr=0.000000
Train batch 1500
Avg. loss per last 100 batches: 0.008082
Train batch 1500
Avg. loss per last 100 batches: 0.008082
Epoch: 39: Step: 1501/4907, loss=0.000001, lr=0.000000
Epoch: 39: Step: 1501/4907, loss=0.000001, lr=0.000000
Train batch 1600
Avg. loss per last 100 batches: 0.016354
Train batch 1600
Avg. loss per last 100 batches: 0.016354
Epoch: 39: Step: 1601/4907, loss=0.000138, lr=0.000000
Epoch: 39: Step: 1601/4907, loss=0.000138, lr=0.000000
Train batch 1700
Avg. loss per last 100 batches: 0.015817
Train batch 1700
Avg. loss per last 100 batches: 0.015817
Epoch: 39: Step: 1701/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 1701/4907, loss=0.000000, lr=0.000000
Train batch 1800
Avg. loss per last 100 batches: 0.006609
Train batch 1800
Avg. loss per last 100 batches: 0.006609
Epoch: 39: Step: 1801/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 1801/4907, loss=0.000000, lr=0.000000
Train batch 1900
Avg. loss per last 100 batches: 0.022666
Train batch 1900
Avg. loss per last 100 batches: 0.022666
Epoch: 39: Step: 1901/4907, loss=0.000186, lr=0.000000
Epoch: 39: Step: 1901/4907, loss=0.000186, lr=0.000000
Train batch 2000
Avg. loss per last 100 batches: 0.009668
Train batch 2000
Avg. loss per last 100 batches: 0.009668
Epoch: 39: Step: 2001/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 2001/4907, loss=0.000000, lr=0.000000
Train batch 2100
Avg. loss per last 100 batches: 0.016785
Train batch 2100
Avg. loss per last 100 batches: 0.016785
Epoch: 39: Step: 2101/4907, loss=0.000049, lr=0.000000
Epoch: 39: Step: 2101/4907, loss=0.000049, lr=0.000000
Train batch 2200
Avg. loss per last 100 batches: 0.023273
Train batch 2200
Avg. loss per last 100 batches: 0.023273
Epoch: 39: Step: 2201/4907, loss=0.000196, lr=0.000000
Epoch: 39: Step: 2201/4907, loss=0.000196, lr=0.000000
Train batch 2300
Avg. loss per last 100 batches: 0.003677
Train batch 2300
Avg. loss per last 100 batches: 0.003677
Epoch: 39: Step: 2301/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 2301/4907, loss=0.000000, lr=0.000000
Train batch 2400
Avg. loss per last 100 batches: 0.018710
Train batch 2400
Avg. loss per last 100 batches: 0.018710
Epoch: 39: Step: 2401/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 2401/4907, loss=0.000000, lr=0.000000
Train batch 2500
Avg. loss per last 100 batches: 0.001751
Train batch 2500
Avg. loss per last 100 batches: 0.001751
Epoch: 39: Step: 2501/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 2501/4907, loss=0.000000, lr=0.000000
Train batch 2600
Avg. loss per last 100 batches: 0.006130
Train batch 2600
Avg. loss per last 100 batches: 0.006130
Epoch: 39: Step: 2601/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 2601/4907, loss=0.000000, lr=0.000000
Train batch 2700
Avg. loss per last 100 batches: 0.013663
Train batch 2700
Avg. loss per last 100 batches: 0.013663
Epoch: 39: Step: 2701/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 2701/4907, loss=0.000000, lr=0.000000
Train batch 2800
Avg. loss per last 100 batches: 0.024874
Train batch 2800
Avg. loss per last 100 batches: 0.024874
Epoch: 39: Step: 2801/4907, loss=0.000001, lr=0.000000
Epoch: 39: Step: 2801/4907, loss=0.000001, lr=0.000000
Train batch 2900
Avg. loss per last 100 batches: 0.020107
Train batch 2900
Avg. loss per last 100 batches: 0.020107
Epoch: 39: Step: 2901/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 2901/4907, loss=0.000000, lr=0.000000
Train batch 3000
Avg. loss per last 100 batches: 0.010312
Train batch 3000
Avg. loss per last 100 batches: 0.010312
Epoch: 39: Step: 3001/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 3001/4907, loss=0.000000, lr=0.000000
Train batch 3100
Avg. loss per last 100 batches: 0.003151
Train batch 3100
Avg. loss per last 100 batches: 0.003151
Epoch: 39: Step: 3101/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 3101/4907, loss=0.000000, lr=0.000000
Train batch 3200
Avg. loss per last 100 batches: 0.003037
Train batch 3200
Avg. loss per last 100 batches: 0.003037
Epoch: 39: Step: 3201/4907, loss=0.000003, lr=0.000000
Epoch: 39: Step: 3201/4907, loss=0.000003, lr=0.000000
Train batch 3300
Avg. loss per last 100 batches: 0.022123
Train batch 3300
Avg. loss per last 100 batches: 0.022123
Epoch: 39: Step: 3301/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 3301/4907, loss=0.000000, lr=0.000000
Train batch 3400
Avg. loss per last 100 batches: 0.008364
Train batch 3400
Avg. loss per last 100 batches: 0.008364
Epoch: 39: Step: 3401/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 3401/4907, loss=0.000000, lr=0.000000
Train batch 3500
Avg. loss per last 100 batches: 0.012791
Train batch 3500
Avg. loss per last 100 batches: 0.012791
Epoch: 39: Step: 3501/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 3501/4907, loss=0.000000, lr=0.000000
Train batch 3600
Avg. loss per last 100 batches: 0.015596
Train batch 3600
Avg. loss per last 100 batches: 0.015596
Epoch: 39: Step: 3601/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 3601/4907, loss=0.000000, lr=0.000000
Train batch 3700
Avg. loss per last 100 batches: 0.023644
Train batch 3700
Avg. loss per last 100 batches: 0.023644
Epoch: 39: Step: 3701/4907, loss=0.000003, lr=0.000000
Epoch: 39: Step: 3701/4907, loss=0.000003, lr=0.000000
Train batch 3800
Avg. loss per last 100 batches: 0.007832
Train batch 3800
Avg. loss per last 100 batches: 0.007832
Epoch: 39: Step: 3801/4907, loss=0.000287, lr=0.000000
Epoch: 39: Step: 3801/4907, loss=0.000287, lr=0.000000
Train batch 3900
Avg. loss per last 100 batches: 0.009540
Train batch 3900
Avg. loss per last 100 batches: 0.009540
Epoch: 39: Step: 3901/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 3901/4907, loss=0.000000, lr=0.000000
Train batch 4000
Avg. loss per last 100 batches: 0.015190
Train batch 4000
Avg. loss per last 100 batches: 0.015190
Epoch: 39: Step: 4001/4907, loss=0.000029, lr=0.000000
Epoch: 39: Step: 4001/4907, loss=0.000029, lr=0.000000
Train batch 4100
Avg. loss per last 100 batches: 0.000679
Train batch 4100
Avg. loss per last 100 batches: 0.000679
Epoch: 39: Step: 4101/4907, loss=0.005410, lr=0.000000
Epoch: 39: Step: 4101/4907, loss=0.005410, lr=0.000000
Train batch 4200
Avg. loss per last 100 batches: 0.018549
Train batch 4200
Avg. loss per last 100 batches: 0.018549
Epoch: 39: Step: 4201/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 4201/4907, loss=0.000000, lr=0.000000
Train batch 4300
Avg. loss per last 100 batches: 0.022441
Train batch 4300
Avg. loss per last 100 batches: 0.022441
Epoch: 39: Step: 4301/4907, loss=0.000024, lr=0.000000
Epoch: 39: Step: 4301/4907, loss=0.000024, lr=0.000000
Train batch 4400
Avg. loss per last 100 batches: 0.013296
Train batch 4400
Avg. loss per last 100 batches: 0.013296
Epoch: 39: Step: 4401/4907, loss=0.000101, lr=0.000000
Epoch: 39: Step: 4401/4907, loss=0.000101, lr=0.000000
Train batch 4500
Avg. loss per last 100 batches: 0.016969
Train batch 4500
Avg. loss per last 100 batches: 0.016969
Epoch: 39: Step: 4501/4907, loss=0.000086, lr=0.000000
Epoch: 39: Step: 4501/4907, loss=0.000086, lr=0.000000
Train batch 4600
Avg. loss per last 100 batches: 0.035776
Train batch 4600
Avg. loss per last 100 batches: 0.035776
Epoch: 39: Step: 4601/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 4601/4907, loss=0.000000, lr=0.000000
Train batch 4700
Avg. loss per last 100 batches: 0.022167
Train batch 4700
Avg. loss per last 100 batches: 0.022167
Epoch: 39: Step: 4701/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 4701/4907, loss=0.000000, lr=0.000000
Train batch 4800
Avg. loss per last 100 batches: 0.006161
Train batch 4800
Avg. loss per last 100 batches: 0.006161
Epoch: 39: Step: 4801/4907, loss=0.000000, lr=0.000000
Epoch: 39: Step: 4801/4907, loss=0.000000, lr=0.000000
Train batch 4900
Avg. loss per last 100 batches: 0.008397
Train batch 4900
Avg. loss per last 100 batches: 0.008397
Epoch: 39: Step: 4901/4907, loss=0.000009, lr=0.000000
Epoch: 39: Step: 4901/4907, loss=0.000009, lr=0.000000
Validation: Epoch: 39 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 39 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 142.64426028238182, total questions=6516
Av.rank validation: average rank 142.64426028238182, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.39.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.39.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.39.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 142.64426028238182, total questions=6516
Av.rank validation: average rank 142.64426028238182, total questions=6516
Av Loss per epoch=0.014315
epoch total correct predictions=58665
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.39.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.39.4907
Av Loss per epoch=0.014315
epoch total correct predictions=58665
Training finished. Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.39.4907
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
