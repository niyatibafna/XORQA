/usr/bin/python3: Error while finding spec for 'torch.distributed.launch' (ImportError: No module named 'torch')
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 1; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 256, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/CE/nbafna/anaconda3/envs/story_recom/bin/python3', '-u', '../baselines/DPR/train_dense_encoder.py', '--local_rank=1', '--max_grad_norm', '2.0', '--encoder_model_type', 'hf_bert', '--pretrained_model_cfg', 'bert-base-multilingual-uncased', '--model_file', '../biencoders/pretrained/dpr_biencoder_B', '--seed', '12345', '--sequence_length', '256', '--warmup_steps', '1237', '--batch_size', '12', '--do_lower_case', '--train_file', '../data/NQ/biencoder-nq-train.json', '--dev_file', '../data/NQ/biencoder-nq-dev.json', '--output_dir', '../biencoders/trained/B+A/', '--learning_rate', '2e-05', '--num_train_epochs', '40', '--dev_batch_size', '12', '--val_av_rank_start_epoch', '10', '--restart']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 1; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 256, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/CE/nbafna/anaconda3/envs/story_recom/bin/python3', '-u', '../baselines/DPR/train_dense_encoder.py', '--local_rank=1', '--max_grad_norm', '2.0', '--encoder_model_type', 'hf_bert', '--pretrained_model_cfg', 'bert-base-multilingual-uncased', '--model_file', '../biencoders/pretrained/dpr_biencoder_B', '--seed', '12345', '--sequence_length', '256', '--warmup_steps', '1237', '--batch_size', '12', '--do_lower_case', '--train_file', '../data/NQ/biencoder-nq-train.json', '--dev_file', '../data/NQ/biencoder-nq-dev.json', '--output_dir', '../biencoders/trained/B+A/', '--learning_rate', '2e-05', '--num_train_epochs', '40', '--dev_batch_size', '12', '--val_av_rank_start_epoch', '10', '--restart']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
16-bits training: False 
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
***** Initializing components for training *****
adam_eps                       -->   1e-08
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 1; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 256, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/CE/nbafna/anaconda3/envs/story_recom/bin/python3', '-u', '../baselines/DPR/train_dense_encoder.py', '--local_rank=1', '--max_grad_norm', '2.0', '--encoder_model_type', 'hf_bert', '--pretrained_model_cfg', 'bert-base-multilingual-uncased', '--model_file', '../biencoders/pretrained/dpr_biencoder_B', '--seed', '12345', '--sequence_length', '256', '--warmup_steps', '1237', '--batch_size', '12', '--do_lower_case', '--train_file', '../data/NQ/biencoder-nq-train.json', '--dev_file', '../data/NQ/biencoder-nq-dev.json', '--output_dir', '../biencoders/trained/B+A/', '--learning_rate', '2e-05', '--num_train_epochs', '40', '--dev_batch_size', '12', '--val_av_rank_start_epoch', '10', '--restart']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   12
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   12
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=2454
 Total updates=98160
  Eval step = 2454
***** Training *****
***** Epoch 0 *****
Traceback (most recent call last):
  File "../baselines/DPR/train_dense_encoder.py", line 568, in <module>
    main()
  File "../baselines/DPR/train_dense_encoder.py", line 558, in main
    trainer.run_train()
  File "../baselines/DPR/train_dense_encoder.py", line 130, in run_train
    self._train_epoch(scheduler, epoch, eval_step, train_iterator)
  File "../baselines/DPR/train_dense_encoder.py", line 326, in _train_epoch
    loss, correct_cnt = _do_biencoder_fwd_pass(self.biencoder, biencoder_batch, self.tensorizer, args)
  File "../baselines/DPR/train_dense_encoder.py", line 474, in _do_biencoder_fwd_pass
    model_out = model(input.question_ids, input.question_segments, q_attn_mask, input.context_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 511, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 93, in forward
    _ctx_seq, ctx_pooled_out, _ctx_hidden = self.get_representation(self.ctx_model, context_ids, ctx_segments,
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/biencoder.py", line 84, in get_representation
    sequence_output, pooled_output, hidden_states = sub_model(ids, segments, attn_mask)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/local/home/CE/nbafna/projects/XORQA_2/XORQA/baselines/DPR/dpr/models/hf_models.py", line 127, in forward
    sequence_output, pooled_output = super().forward(input_ids=input_ids, token_type_ids=token_type_ids,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 755, in forward
    encoder_outputs = self.encoder(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 433, in forward
    layer_outputs = layer_module(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 370, in forward
    self_attention_outputs = self.attention(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 314, in forward
    self_outputs = self.self(
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/transformers/modeling_bert.py", line 250, in forward
    attention_probs = self.dropout(attention_probs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/nn/functional.py", line 973, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 1; 11.93 GiB total capacity; 11.08 GiB already allocated; 68.12 MiB free; 11.29 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/distributed/launch.py", line 256, in main
    raise subprocess.CalledProcessError(returncode=process.returncode,
subprocess.CalledProcessError: Command '['/home/CE/nbafna/anaconda3/envs/story_recom/bin/python3', '-u', '../baselines/DPR/train_dense_encoder.py', '--local_rank=1', '--max_grad_norm', '2.0', '--encoder_model_type', 'hf_bert', '--pretrained_model_cfg', 'bert-base-multilingual-uncased', '--model_file', '../biencoders/pretrained/dpr_biencoder_B', '--seed', '12345', '--sequence_length', '256', '--warmup_steps', '1237', '--batch_size', '12', '--do_lower_case', '--train_file', '../data/NQ/biencoder-nq-train.json', '--dev_file', '../data/NQ/biencoder-nq-dev.json', '--output_dir', '../biencoders/trained/B+A/', '--learning_rate', '2e-05', '--num_train_epochs', '40', '--dev_batch_size', '12', '--val_av_rank_start_epoch', '10', '--restart']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Initialized host jones-6 as d.rank 1 on device=cuda:1, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   6
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   6
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:1
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   1
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
Initialized host jones-6 as d.rank 0 on device=cuda:0, n_gpu=1, world size=2
16-bits training: False 
 **************** CONFIGURATION **************** 
adam_betas                     -->   (0.9, 0.999)
adam_eps                       -->   1e-08
batch_size                     -->   6
checkpoint_file_name           -->   dpr_biencoder
dev_batch_size                 -->   6
dev_file                       -->   ../data/NQ/biencoder-nq-dev.json
device                         -->   cuda:0
distributed_world_size         -->   2
do_lower_case                  -->   True
dropout                        -->   0.1
encoder_model_type             -->   hf_bert
eval_per_epoch                 -->   1
fix_ctx_encoder                -->   False
fp16                           -->   False
fp16_opt_level                 -->   O1
global_loss_buf_sz             -->   150000
gradient_accumulation_steps    -->   1
hard_negatives                 -->   1
learning_rate                  -->   2e-05
local_rank                     -->   0
log_batch_step                 -->   100
max_grad_norm                  -->   2.0
model_file                     -->   ../biencoders/pretrained/dpr_biencoder_B
n_gpu                          -->   1
no_cuda                        -->   False
num_train_epochs               -->   40.0
other_negatives                -->   0
output_dir                     -->   ../biencoders/trained/B+A/
pretrained_file                -->   None
pretrained_model_cfg           -->   bert-base-multilingual-uncased
projection_dim                 -->   0
restart                        -->   True
seed                           -->   12345
sequence_length                -->   256
shuffle_positive_ctx           -->   False
train_file                     -->   ../data/NQ/biencoder-nq-train.json
train_files_upsample_rates     -->   None
train_rolling_loss_step        -->   100
val_av_rank_bsz                -->   128
val_av_rank_hard_neg           -->   30
val_av_rank_max_qs             -->   10000
val_av_rank_other_neg          -->   30
val_av_rank_start_epoch        -->   10
warmup_steps                   -->   1237
weight_decay                   -->   0.0
 **************** CONFIGURATION **************** 
***** Initializing components for training *****
Reading saved model from ../biencoders/pretrained/dpr_biencoder_B
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])
Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True
Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-multilingual-uncased
Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert
Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256
PyTorch version 1.6.0 available.
PyTorch version 1.6.0 available.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/CE/nbafna/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.fcb1786f49c279f0e0f158c9972b9bd9f6c0edb5d893dcb9b530d714d86f0edc
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "directionality": "bidi",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "pooler_fc_size": 768,
  "pooler_num_attention_heads": 12,
  "pooler_num_fc_layers": 3,
  "pooler_size_per_head": 128,
  "pooler_type": "first_token_transform",
  "type_vocab_size": 2,
  "vocab_size": 105879
}

loading weights file https://cdn.huggingface.co/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/CE/nbafna/.cache/torch/transformers/b72dd13aa8437628227c4928499f2476a84af4ab7ed75eb1365c5ae9fdbd7638.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
All model checkpoint weights were used when initializing HFBertEncoder.

All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-multilingual-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/CE/nbafna/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Loading checkpoint @ batch=81673 and epoch=4
Loading saved model state ...
Reading file ../data/NQ/biencoder-nq-train.json
Reading file ../data/NQ/biencoder-nq-train.json
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=4907
 Total updates=196280
  Eval step = 4907
***** Training *****
***** Epoch 0 *****
Aggregated data size: 58880
Total cleaned data size: 58880
  Total iterations per epoch=4907
 Total updates=196280
  Eval step = 4907
***** Training *****
***** Epoch 0 *****
Epoch: 0: Step: 1/4907, loss=4.017465, lr=0.000000
Epoch: 0: Step: 1/4907, loss=4.017465, lr=0.000000
Train batch 100
Avg. loss per last 100 batches: 3.069843
Train batch 100
Avg. loss per last 100 batches: 3.069843
Epoch: 0: Step: 101/4907, loss=2.570882, lr=0.000002
Epoch: 0: Step: 101/4907, loss=2.570882, lr=0.000002
Train batch 200
Avg. loss per last 100 batches: 1.891513
Train batch 200
Avg. loss per last 100 batches: 1.891513
Epoch: 0: Step: 201/4907, loss=3.002283, lr=0.000003
Epoch: 0: Step: 201/4907, loss=3.002283, lr=0.000003
Train batch 300
Avg. loss per last 100 batches: 1.091184
Train batch 300
Avg. loss per last 100 batches: 1.091184
Epoch: 0: Step: 301/4907, loss=1.667616, lr=0.000005
Epoch: 0: Step: 301/4907, loss=1.667616, lr=0.000005
Train batch 400
Avg. loss per last 100 batches: 0.762433
Train batch 400
Avg. loss per last 100 batches: 0.762433
Epoch: 0: Step: 401/4907, loss=0.057895, lr=0.000006
Epoch: 0: Step: 401/4907, loss=0.057895, lr=0.000006
Train batch 500
Avg. loss per last 100 batches: 0.670715
Train batch 500
Avg. loss per last 100 batches: 0.670715
Epoch: 0: Step: 501/4907, loss=0.748637, lr=0.000008
Epoch: 0: Step: 501/4907, loss=0.748637, lr=0.000008
Train batch 600
Avg. loss per last 100 batches: 0.561317
Train batch 600
Avg. loss per last 100 batches: 0.561317
Epoch: 0: Step: 601/4907, loss=0.506405, lr=0.000010
Epoch: 0: Step: 601/4907, loss=0.506405, lr=0.000010
Train batch 700
Avg. loss per last 100 batches: 0.518331
Train batch 700
Avg. loss per last 100 batches: 0.518331
Epoch: 0: Step: 701/4907, loss=0.313789, lr=0.000011
Epoch: 0: Step: 701/4907, loss=0.313789, lr=0.000011
Train batch 800
Avg. loss per last 100 batches: 0.438257
Train batch 800
Avg. loss per last 100 batches: 0.438257
Epoch: 0: Step: 801/4907, loss=0.160320, lr=0.000013
Epoch: 0: Step: 801/4907, loss=0.160320, lr=0.000013
Train batch 900
Avg. loss per last 100 batches: 0.392031
Train batch 900
Avg. loss per last 100 batches: 0.392031
Epoch: 0: Step: 901/4907, loss=0.186262, lr=0.000015
Epoch: 0: Step: 901/4907, loss=0.186262, lr=0.000015
Train batch 1000
Avg. loss per last 100 batches: 0.407528
Train batch 1000
Avg. loss per last 100 batches: 0.407528
Epoch: 0: Step: 1001/4907, loss=0.228110, lr=0.000016
Epoch: 0: Step: 1001/4907, loss=0.228110, lr=0.000016
Train batch 1100
Avg. loss per last 100 batches: 0.371221
Train batch 1100
Avg. loss per last 100 batches: 0.371221
Epoch: 0: Step: 1101/4907, loss=0.075700, lr=0.000018
Epoch: 0: Step: 1101/4907, loss=0.075700, lr=0.000018
Train batch 1200
Avg. loss per last 100 batches: 0.327214
Train batch 1200
Avg. loss per last 100 batches: 0.327214
Epoch: 0: Step: 1201/4907, loss=0.375097, lr=0.000019
Epoch: 0: Step: 1201/4907, loss=0.375097, lr=0.000019
Train batch 1300
Avg. loss per last 100 batches: 0.360195
Train batch 1300
Avg. loss per last 100 batches: 0.360195
Epoch: 0: Step: 1301/4907, loss=1.685537, lr=0.000020
Epoch: 0: Step: 1301/4907, loss=1.685537, lr=0.000020
Train batch 1400
Avg. loss per last 100 batches: 0.370966
Train batch 1400
Avg. loss per last 100 batches: 0.370966
Epoch: 0: Step: 1401/4907, loss=0.022527, lr=0.000020
Epoch: 0: Step: 1401/4907, loss=0.022527, lr=0.000020
Train batch 1500
Avg. loss per last 100 batches: 0.309958
Train batch 1500
Avg. loss per last 100 batches: 0.309958
Epoch: 0: Step: 1501/4907, loss=0.430437, lr=0.000020
Epoch: 0: Step: 1501/4907, loss=0.430437, lr=0.000020
Train batch 1600
Avg. loss per last 100 batches: 0.391048
Train batch 1600
Avg. loss per last 100 batches: 0.391048
Epoch: 0: Step: 1601/4907, loss=0.556172, lr=0.000020
Epoch: 0: Step: 1601/4907, loss=0.556172, lr=0.000020
Train batch 1700
Avg. loss per last 100 batches: 0.357360
Train batch 1700
Avg. loss per last 100 batches: 0.357360
Epoch: 0: Step: 1701/4907, loss=0.567616, lr=0.000020
Epoch: 0: Step: 1701/4907, loss=0.567616, lr=0.000020
Train batch 1800
Avg. loss per last 100 batches: 0.332552
Train batch 1800
Avg. loss per last 100 batches: 0.332552
Epoch: 0: Step: 1801/4907, loss=0.077064, lr=0.000020
Epoch: 0: Step: 1801/4907, loss=0.077064, lr=0.000020
Train batch 1900
Avg. loss per last 100 batches: 0.323447
Train batch 1900
Avg. loss per last 100 batches: 0.323447
Epoch: 0: Step: 1901/4907, loss=0.194578, lr=0.000020
Epoch: 0: Step: 1901/4907, loss=0.194578, lr=0.000020
Train batch 2000
Avg. loss per last 100 batches: 0.302242
Train batch 2000
Avg. loss per last 100 batches: 0.302242
Epoch: 0: Step: 2001/4907, loss=0.318219, lr=0.000020
Epoch: 0: Step: 2001/4907, loss=0.318219, lr=0.000020
Train batch 2100
Avg. loss per last 100 batches: 0.300839
Train batch 2100
Avg. loss per last 100 batches: 0.300839
Epoch: 0: Step: 2101/4907, loss=0.251361, lr=0.000020
Epoch: 0: Step: 2101/4907, loss=0.251361, lr=0.000020
Train batch 2200
Avg. loss per last 100 batches: 0.272917
Train batch 2200
Avg. loss per last 100 batches: 0.272917
Epoch: 0: Step: 2201/4907, loss=0.256415, lr=0.000020
Epoch: 0: Step: 2201/4907, loss=0.256415, lr=0.000020
Train batch 2300
Avg. loss per last 100 batches: 0.290550
Train batch 2300
Avg. loss per last 100 batches: 0.290550
Epoch: 0: Step: 2301/4907, loss=0.183895, lr=0.000020
Epoch: 0: Step: 2301/4907, loss=0.183895, lr=0.000020
Train batch 2400
Avg. loss per last 100 batches: 0.295779
Train batch 2400
Avg. loss per last 100 batches: 0.295779
Epoch: 0: Step: 2401/4907, loss=0.358316, lr=0.000020
Epoch: 0: Step: 2401/4907, loss=0.358316, lr=0.000020
Train batch 2500
Avg. loss per last 100 batches: 0.276079
Train batch 2500
Avg. loss per last 100 batches: 0.276079
Epoch: 0: Step: 2501/4907, loss=0.020416, lr=0.000020
Epoch: 0: Step: 2501/4907, loss=0.020416, lr=0.000020
Train batch 2600
Avg. loss per last 100 batches: 0.265472
Train batch 2600
Avg. loss per last 100 batches: 0.265472
Epoch: 0: Step: 2601/4907, loss=0.395620, lr=0.000020
Epoch: 0: Step: 2601/4907, loss=0.395620, lr=0.000020
Train batch 2700
Avg. loss per last 100 batches: 0.331890
Train batch 2700
Avg. loss per last 100 batches: 0.331890
Epoch: 0: Step: 2701/4907, loss=0.070595, lr=0.000020
Epoch: 0: Step: 2701/4907, loss=0.070595, lr=0.000020
Train batch 2800
Avg. loss per last 100 batches: 0.270311
Train batch 2800
Avg. loss per last 100 batches: 0.270311
Epoch: 0: Step: 2801/4907, loss=0.271623, lr=0.000020
Epoch: 0: Step: 2801/4907, loss=0.271623, lr=0.000020
Train batch 2900
Avg. loss per last 100 batches: 0.287199
Train batch 2900
Avg. loss per last 100 batches: 0.287199
Epoch: 0: Step: 2901/4907, loss=1.242602, lr=0.000020
Epoch: 0: Step: 2901/4907, loss=1.242602, lr=0.000020
Train batch 3000
Avg. loss per last 100 batches: 0.248387
Train batch 3000
Avg. loss per last 100 batches: 0.248387
Epoch: 0: Step: 3001/4907, loss=0.696428, lr=0.000020
Epoch: 0: Step: 3001/4907, loss=0.696428, lr=0.000020
Train batch 3100
Avg. loss per last 100 batches: 0.259377
Train batch 3100
Avg. loss per last 100 batches: 0.259377
Epoch: 0: Step: 3101/4907, loss=0.037212, lr=0.000020
Epoch: 0: Step: 3101/4907, loss=0.037212, lr=0.000020
Train batch 3200
Avg. loss per last 100 batches: 0.260392
Train batch 3200
Avg. loss per last 100 batches: 0.260392
Epoch: 0: Step: 3201/4907, loss=0.031090, lr=0.000020
Epoch: 0: Step: 3201/4907, loss=0.031090, lr=0.000020
Train batch 3300
Avg. loss per last 100 batches: 0.252486
Train batch 3300
Avg. loss per last 100 batches: 0.252486
Epoch: 0: Step: 3301/4907, loss=0.574335, lr=0.000020
Epoch: 0: Step: 3301/4907, loss=0.574335, lr=0.000020
Train batch 3400
Avg. loss per last 100 batches: 0.272031
Train batch 3400
Avg. loss per last 100 batches: 0.272031
Epoch: 0: Step: 3401/4907, loss=0.304338, lr=0.000020
Epoch: 0: Step: 3401/4907, loss=0.304338, lr=0.000020
Train batch 3500
Avg. loss per last 100 batches: 0.246026
Train batch 3500
Avg. loss per last 100 batches: 0.246026
Epoch: 0: Step: 3501/4907, loss=0.126953, lr=0.000020
Epoch: 0: Step: 3501/4907, loss=0.126953, lr=0.000020
Train batch 3600
Avg. loss per last 100 batches: 0.230955
Train batch 3600
Avg. loss per last 100 batches: 0.230955
Epoch: 0: Step: 3601/4907, loss=0.253241, lr=0.000020
Epoch: 0: Step: 3601/4907, loss=0.253241, lr=0.000020
Train batch 3700
Avg. loss per last 100 batches: 0.269894
Train batch 3700
Avg. loss per last 100 batches: 0.269894
Epoch: 0: Step: 3701/4907, loss=0.625531, lr=0.000020
Epoch: 0: Step: 3701/4907, loss=0.625531, lr=0.000020
Train batch 3800
Train batch 3800
Avg. loss per last 100 batches: 0.258112
Avg. loss per last 100 batches: 0.258112
Epoch: 0: Step: 3801/4907, loss=0.008255, lr=0.000020
Epoch: 0: Step: 3801/4907, loss=0.008255, lr=0.000020
Train batch 3900
Avg. loss per last 100 batches: 0.269410
Train batch 3900
Avg. loss per last 100 batches: 0.269410
Epoch: 0: Step: 3901/4907, loss=0.162754, lr=0.000020
Epoch: 0: Step: 3901/4907, loss=0.162754, lr=0.000020
Train batch 4000
Avg. loss per last 100 batches: 0.270165
Train batch 4000
Avg. loss per last 100 batches: 0.270165
Epoch: 0: Step: 4001/4907, loss=0.414500, lr=0.000020
Epoch: 0: Step: 4001/4907, loss=0.414500, lr=0.000020
Train batch 4100
Avg. loss per last 100 batches: 0.274964
Train batch 4100
Avg. loss per last 100 batches: 0.274964
Epoch: 0: Step: 4101/4907, loss=0.030356, lr=0.000020
Epoch: 0: Step: 4101/4907, loss=0.030356, lr=0.000020
Train batch 4200
Avg. loss per last 100 batches: 0.219227
Train batch 4200
Avg. loss per last 100 batches: 0.219227
Epoch: 0: Step: 4201/4907, loss=0.061458, lr=0.000020
Epoch: 0: Step: 4201/4907, loss=0.061458, lr=0.000020
Train batch 4300
Avg. loss per last 100 batches: 0.264812
Train batch 4300
Avg. loss per last 100 batches: 0.264812
Epoch: 0: Step: 4301/4907, loss=0.342663, lr=0.000020
Epoch: 0: Step: 4301/4907, loss=0.342663, lr=0.000020
Train batch 4400
Avg. loss per last 100 batches: 0.205112
Train batch 4400
Avg. loss per last 100 batches: 0.205112
Epoch: 0: Step: 4401/4907, loss=0.049997, lr=0.000020
Epoch: 0: Step: 4401/4907, loss=0.049997, lr=0.000020
Train batch 4500
Avg. loss per last 100 batches: 0.292821
Train batch 4500
Avg. loss per last 100 batches: 0.292821
Epoch: 0: Step: 4501/4907, loss=0.194680, lr=0.000020
Epoch: 0: Step: 4501/4907, loss=0.194680, lr=0.000020
Train batch 4600
Avg. loss per last 100 batches: 0.238678
Train batch 4600
Avg. loss per last 100 batches: 0.238678
Epoch: 0: Step: 4601/4907, loss=0.343834, lr=0.000020
Epoch: 0: Step: 4601/4907, loss=0.343834, lr=0.000020
Train batch 4700
Avg. loss per last 100 batches: 0.239998
Train batch 4700
Avg. loss per last 100 batches: 0.239998
Epoch: 0: Step: 4701/4907, loss=0.055941, lr=0.000020
Epoch: 0: Step: 4701/4907, loss=0.055941, lr=0.000020
Train batch 4800
Avg. loss per last 100 batches: 0.280769
Train batch 4800
Avg. loss per last 100 batches: 0.280769
Epoch: 0: Step: 4801/4907, loss=0.031332, lr=0.000020
Epoch: 0: Step: 4801/4907, loss=0.031332, lr=0.000020
Train batch 4900
Avg. loss per last 100 batches: 0.231245
Train batch 4900
Avg. loss per last 100 batches: 0.231245
Epoch: 0: Step: 4901/4907, loss=0.142841, lr=0.000020
Epoch: 0: Step: 4901/4907, loss=0.142841, lr=0.000020
Validation: Epoch: 0 Step: 4907/4907
NLL validation ...
Validation: Epoch: 0 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.382221 sec., loss=0.136667 
Eval step: 99 , used_time=28.478279 sec., loss=0.136667 
Eval step: 199 , used_time=57.527405 sec., loss=0.107963 
Eval step: 199 , used_time=57.431280 sec., loss=0.107963 
Eval step: 299 , used_time=86.464754 sec., loss=0.710855 
Eval step: 299 , used_time=86.560935 sec., loss=0.710855 
Eval step: 399 , used_time=115.580472 sec., loss=0.194009 
Eval step: 399 , used_time=115.484415 sec., loss=0.194009 
Eval step: 499 , used_time=144.562332 sec., loss=0.030770 
Eval step: 499 , used_time=144.466206 sec., loss=0.030770 
NLL Validation: loss = 0.442775. correct prediction ratio  5534/6516 ~  0.849294
NLL Validation: loss = 0.442775. correct prediction ratio  5534/6516 ~  0.849294
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
/home/CE/nbafna/anaconda3/envs/story_recom/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.
  warnings.warn(SAVE_STATE_WARNING, UserWarning)
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.0.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.0.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.0.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=37.166745 sec., loss=0.136667 
Eval step: 99 , used_time=27.652208 sec., loss=0.136667 
Eval step: 199 , used_time=56.626096 sec., loss=0.107963 
Eval step: 199 , used_time=66.140613 sec., loss=0.107963 
Eval step: 299 , used_time=85.609547 sec., loss=0.710855 
Eval step: 299 , used_time=95.124134 sec., loss=0.710855 
Eval step: 399 , used_time=124.126125 sec., loss=0.194009 
Eval step: 399 , used_time=114.611578 sec., loss=0.194009 
Eval step: 499 , used_time=143.638715 sec., loss=0.030770 
Eval step: 499 , used_time=153.153268 sec., loss=0.030770 
NLL Validation: loss = 0.442775. correct prediction ratio  5534/6516 ~  0.849294
NLL Validation: loss = 0.442775. correct prediction ratio  5534/6516 ~  0.849294
Av Loss per epoch=0.426650
epoch total correct predictions=51744
***** Epoch 1 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.0.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.0.4907
Av Loss per epoch=0.426650
epoch total correct predictions=51744
***** Epoch 1 *****
Epoch: 1: Step: 1/4907, loss=0.001258, lr=0.000020
Epoch: 1: Step: 1/4907, loss=0.001258, lr=0.000020
Train batch 100
Train batch 100
Avg. loss per last 100 batches: 0.179459
Avg. loss per last 100 batches: 0.179459
Epoch: 1: Step: 101/4907, loss=0.000992, lr=0.000020
Epoch: 1: Step: 101/4907, loss=0.000992, lr=0.000020
Train batch 200
Avg. loss per last 100 batches: 0.159920
Train batch 200
Avg. loss per last 100 batches: 0.159920
Epoch: 1: Step: 201/4907, loss=0.027029, lr=0.000020
Epoch: 1: Step: 201/4907, loss=0.027029, lr=0.000020
Train batch 300
Avg. loss per last 100 batches: 0.190505
Train batch 300
Avg. loss per last 100 batches: 0.190505
Epoch: 1: Step: 301/4907, loss=0.008841, lr=0.000020
Epoch: 1: Step: 301/4907, loss=0.008841, lr=0.000020
Train batch 400
Avg. loss per last 100 batches: 0.194797
Train batch 400
Avg. loss per last 100 batches: 0.194797
Epoch: 1: Step: 401/4907, loss=0.404120, lr=0.000020
Epoch: 1: Step: 401/4907, loss=0.404120, lr=0.000020
Train batch 500
Avg. loss per last 100 batches: 0.168945
Train batch 500
Avg. loss per last 100 batches: 0.168945
Epoch: 1: Step: 501/4907, loss=0.289099, lr=0.000020
Epoch: 1: Step: 501/4907, loss=0.289099, lr=0.000020
Train batch 600
Avg. loss per last 100 batches: 0.197387
Train batch 600
Avg. loss per last 100 batches: 0.197387
Epoch: 1: Step: 601/4907, loss=0.193249, lr=0.000020
Epoch: 1: Step: 601/4907, loss=0.193249, lr=0.000020
Train batch 700
Avg. loss per last 100 batches: 0.223751
Train batch 700
Avg. loss per last 100 batches: 0.223751
Epoch: 1: Step: 701/4907, loss=0.401367, lr=0.000020
Epoch: 1: Step: 701/4907, loss=0.401367, lr=0.000020
Train batch 800
Avg. loss per last 100 batches: 0.176679
Train batch 800
Avg. loss per last 100 batches: 0.176679
Epoch: 1: Step: 801/4907, loss=0.041093, lr=0.000020
Epoch: 1: Step: 801/4907, loss=0.041093, lr=0.000020
Train batch 900
Avg. loss per last 100 batches: 0.205234
Train batch 900
Avg. loss per last 100 batches: 0.205234
Epoch: 1: Step: 901/4907, loss=0.020002, lr=0.000020
Epoch: 1: Step: 901/4907, loss=0.020002, lr=0.000020
Train batch 1000
Avg. loss per last 100 batches: 0.156478
Train batch 1000
Avg. loss per last 100 batches: 0.156478
Epoch: 1: Step: 1001/4907, loss=0.005093, lr=0.000020
Epoch: 1: Step: 1001/4907, loss=0.005093, lr=0.000020
Train batch 1100
Avg. loss per last 100 batches: 0.201711
Train batch 1100
Avg. loss per last 100 batches: 0.201711
Epoch: 1: Step: 1101/4907, loss=0.097390, lr=0.000020
Epoch: 1: Step: 1101/4907, loss=0.097390, lr=0.000020
Train batch 1200
Avg. loss per last 100 batches: 0.174939
Train batch 1200
Avg. loss per last 100 batches: 0.174939
Epoch: 1: Step: 1201/4907, loss=0.405383, lr=0.000020
Epoch: 1: Step: 1201/4907, loss=0.405383, lr=0.000020
Train batch 1300
Avg. loss per last 100 batches: 0.167789
Train batch 1300
Avg. loss per last 100 batches: 0.167789
Epoch: 1: Step: 1301/4907, loss=0.006983, lr=0.000019
Epoch: 1: Step: 1301/4907, loss=0.006983, lr=0.000019
Train batch 1400
Avg. loss per last 100 batches: 0.193839
Train batch 1400
Avg. loss per last 100 batches: 0.193839
Epoch: 1: Step: 1401/4907, loss=0.333481, lr=0.000019
Epoch: 1: Step: 1401/4907, loss=0.333481, lr=0.000019
Train batch 1500
Avg. loss per last 100 batches: 0.193488
Train batch 1500
Avg. loss per last 100 batches: 0.193488
Epoch: 1: Step: 1501/4907, loss=0.459582, lr=0.000019
Epoch: 1: Step: 1501/4907, loss=0.459582, lr=0.000019
Train batch 1600
Avg. loss per last 100 batches: 0.177727
Train batch 1600
Avg. loss per last 100 batches: 0.177727
Epoch: 1: Step: 1601/4907, loss=0.063317, lr=0.000019
Epoch: 1: Step: 1601/4907, loss=0.063317, lr=0.000019
Train batch 1700
Avg. loss per last 100 batches: 0.200063
Train batch 1700
Avg. loss per last 100 batches: 0.200063
Epoch: 1: Step: 1701/4907, loss=0.005366, lr=0.000019
Epoch: 1: Step: 1701/4907, loss=0.005366, lr=0.000019
Train batch 1800
Avg. loss per last 100 batches: 0.172874
Train batch 1800
Avg. loss per last 100 batches: 0.172874
Epoch: 1: Step: 1801/4907, loss=0.231990, lr=0.000019
Epoch: 1: Step: 1801/4907, loss=0.231990, lr=0.000019
Train batch 1900
Avg. loss per last 100 batches: 0.196664
Train batch 1900
Avg. loss per last 100 batches: 0.196664
Epoch: 1: Step: 1901/4907, loss=0.151664, lr=0.000019
Epoch: 1: Step: 1901/4907, loss=0.151664, lr=0.000019
Train batch 2000
Avg. loss per last 100 batches: 0.174421
Train batch 2000
Avg. loss per last 100 batches: 0.174421
Epoch: 1: Step: 2001/4907, loss=0.040840, lr=0.000019
Epoch: 1: Step: 2001/4907, loss=0.040840, lr=0.000019
Train batch 2100
Avg. loss per last 100 batches: 0.186479
Train batch 2100
Avg. loss per last 100 batches: 0.186479
Epoch: 1: Step: 2101/4907, loss=0.108022, lr=0.000019
Epoch: 1: Step: 2101/4907, loss=0.108022, lr=0.000019
Train batch 2200
Avg. loss per last 100 batches: 0.215635
Train batch 2200
Avg. loss per last 100 batches: 0.215635
Epoch: 1: Step: 2201/4907, loss=0.197594, lr=0.000019
Epoch: 1: Step: 2201/4907, loss=0.197594, lr=0.000019
Train batch 2300
Avg. loss per last 100 batches: 0.146376
Train batch 2300
Avg. loss per last 100 batches: 0.146376
Epoch: 1: Step: 2301/4907, loss=0.030966, lr=0.000019
Epoch: 1: Step: 2301/4907, loss=0.030966, lr=0.000019
Train batch 2400
Avg. loss per last 100 batches: 0.232441
Train batch 2400
Avg. loss per last 100 batches: 0.232441
Epoch: 1: Step: 2401/4907, loss=0.598759, lr=0.000019
Epoch: 1: Step: 2401/4907, loss=0.598759, lr=0.000019
Train batch 2500
Avg. loss per last 100 batches: 0.159647
Train batch 2500
Avg. loss per last 100 batches: 0.159647
Epoch: 1: Step: 2501/4907, loss=0.023987, lr=0.000019
Epoch: 1: Step: 2501/4907, loss=0.023987, lr=0.000019
Train batch 2600
Avg. loss per last 100 batches: 0.220390
Train batch 2600
Avg. loss per last 100 batches: 0.220390
Epoch: 1: Step: 2601/4907, loss=0.047156, lr=0.000019
Epoch: 1: Step: 2601/4907, loss=0.047156, lr=0.000019
Train batch 2700
Avg. loss per last 100 batches: 0.176452
Train batch 2700
Avg. loss per last 100 batches: 0.176452
Epoch: 1: Step: 2701/4907, loss=0.061378, lr=0.000019
Epoch: 1: Step: 2701/4907, loss=0.061378, lr=0.000019
Train batch 2800
Avg. loss per last 100 batches: 0.196884
Train batch 2800
Avg. loss per last 100 batches: 0.196884
Epoch: 1: Step: 2801/4907, loss=0.456783, lr=0.000019
Epoch: 1: Step: 2801/4907, loss=0.456783, lr=0.000019
Train batch 2900
Avg. loss per last 100 batches: 0.214743
Train batch 2900
Avg. loss per last 100 batches: 0.214743
Epoch: 1: Step: 2901/4907, loss=0.076379, lr=0.000019
Epoch: 1: Step: 2901/4907, loss=0.076379, lr=0.000019
Train batch 3000
Train batch 3000
Avg. loss per last 100 batches: 0.224168
Avg. loss per last 100 batches: 0.224168
Epoch: 1: Step: 3001/4907, loss=0.383368, lr=0.000019
Epoch: 1: Step: 3001/4907, loss=0.383368, lr=0.000019
Train batch 3100
Avg. loss per last 100 batches: 0.188676
Train batch 3100
Avg. loss per last 100 batches: 0.188676
Epoch: 1: Step: 3101/4907, loss=0.028601, lr=0.000019
Epoch: 1: Step: 3101/4907, loss=0.028601, lr=0.000019
Train batch 3200
Avg. loss per last 100 batches: 0.185303
Train batch 3200
Avg. loss per last 100 batches: 0.185303
Epoch: 1: Step: 3201/4907, loss=0.043866, lr=0.000019
Epoch: 1: Step: 3201/4907, loss=0.043866, lr=0.000019
Train batch 3300
Avg. loss per last 100 batches: 0.172942
Train batch 3300
Avg. loss per last 100 batches: 0.172942
Epoch: 1: Step: 3301/4907, loss=0.475438, lr=0.000019
Epoch: 1: Step: 3301/4907, loss=0.475438, lr=0.000019
Train batch 3400
Avg. loss per last 100 batches: 0.176894
Train batch 3400
Avg. loss per last 100 batches: 0.176894
Epoch: 1: Step: 3401/4907, loss=0.177587, lr=0.000019
Epoch: 1: Step: 3401/4907, loss=0.177587, lr=0.000019
Train batch 3500
Avg. loss per last 100 batches: 0.169071
Train batch 3500
Avg. loss per last 100 batches: 0.169071
Epoch: 1: Step: 3501/4907, loss=0.501168, lr=0.000019
Epoch: 1: Step: 3501/4907, loss=0.501168, lr=0.000019
Train batch 3600
Avg. loss per last 100 batches: 0.199180
Train batch 3600
Avg. loss per last 100 batches: 0.199180
Epoch: 1: Step: 3601/4907, loss=0.033487, lr=0.000019
Epoch: 1: Step: 3601/4907, loss=0.033487, lr=0.000019
Train batch 3700
Avg. loss per last 100 batches: 0.177761
Train batch 3700
Avg. loss per last 100 batches: 0.177761
Epoch: 1: Step: 3701/4907, loss=0.121475, lr=0.000019
Epoch: 1: Step: 3701/4907, loss=0.121475, lr=0.000019
Train batch 3800
Avg. loss per last 100 batches: 0.207319
Train batch 3800
Avg. loss per last 100 batches: 0.207319
Epoch: 1: Step: 3801/4907, loss=0.425595, lr=0.000019
Epoch: 1: Step: 3801/4907, loss=0.425595, lr=0.000019
Train batch 3900
Avg. loss per last 100 batches: 0.180728
Train batch 3900
Avg. loss per last 100 batches: 0.180728
Epoch: 1: Step: 3901/4907, loss=0.032794, lr=0.000019
Epoch: 1: Step: 3901/4907, loss=0.032794, lr=0.000019
Train batch 4000
Avg. loss per last 100 batches: 0.173951
Train batch 4000
Avg. loss per last 100 batches: 0.173951
Epoch: 1: Step: 4001/4907, loss=0.337232, lr=0.000019
Epoch: 1: Step: 4001/4907, loss=0.337232, lr=0.000019
Train batch 4100
Avg. loss per last 100 batches: 0.219733
Train batch 4100
Avg. loss per last 100 batches: 0.219733
Epoch: 1: Step: 4101/4907, loss=0.003933, lr=0.000019
Epoch: 1: Step: 4101/4907, loss=0.003933, lr=0.000019
Train batch 4200
Avg. loss per last 100 batches: 0.193375
Train batch 4200
Avg. loss per last 100 batches: 0.193375
Epoch: 1: Step: 4201/4907, loss=0.121112, lr=0.000019
Epoch: 1: Step: 4201/4907, loss=0.121112, lr=0.000019
Train batch 4300
Avg. loss per last 100 batches: 0.154189
Train batch 4300
Avg. loss per last 100 batches: 0.154189
Epoch: 1: Step: 4301/4907, loss=0.417406, lr=0.000019
Epoch: 1: Step: 4301/4907, loss=0.417406, lr=0.000019
Train batch 4400
Avg. loss per last 100 batches: 0.165795
Train batch 4400
Avg. loss per last 100 batches: 0.165795
Epoch: 1: Step: 4401/4907, loss=0.012326, lr=0.000019
Epoch: 1: Step: 4401/4907, loss=0.012326, lr=0.000019
Train batch 4500
Avg. loss per last 100 batches: 0.152986
Train batch 4500
Avg. loss per last 100 batches: 0.152986
Epoch: 1: Step: 4501/4907, loss=0.000347, lr=0.000019
Epoch: 1: Step: 4501/4907, loss=0.000347, lr=0.000019
Train batch 4600
Avg. loss per last 100 batches: 0.171363
Train batch 4600
Avg. loss per last 100 batches: 0.171363
Epoch: 1: Step: 4601/4907, loss=0.285035, lr=0.000019
Epoch: 1: Step: 4601/4907, loss=0.285035, lr=0.000019
Train batch 4700
Avg. loss per last 100 batches: 0.155556
Train batch 4700
Avg. loss per last 100 batches: 0.155556
Epoch: 1: Step: 4701/4907, loss=0.000043, lr=0.000019
Epoch: 1: Step: 4701/4907, loss=0.000043, lr=0.000019
Train batch 4800
Avg. loss per last 100 batches: 0.203998
Train batch 4800
Avg. loss per last 100 batches: 0.203998
Epoch: 1: Step: 4801/4907, loss=0.348933, lr=0.000019
Epoch: 1: Step: 4801/4907, loss=0.348933, lr=0.000019
Train batch 4900
Avg. loss per last 100 batches: 0.175091
Train batch 4900
Avg. loss per last 100 batches: 0.175091
Epoch: 1: Step: 4901/4907, loss=0.148549, lr=0.000019
Epoch: 1: Step: 4901/4907, loss=0.148549, lr=0.000019
Validation: Epoch: 1 Step: 4907/4907
NLL validation ...
Validation: Epoch: 1 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.605858 sec., loss=0.077826 
Eval step: 99 , used_time=28.699984 sec., loss=0.077826 
Eval step: 199 , used_time=57.754295 sec., loss=0.201883 
Eval step: 199 , used_time=57.660103 sec., loss=0.201883 
Eval step: 299 , used_time=86.670137 sec., loss=0.305515 
Eval step: 299 , used_time=86.764333 sec., loss=0.305515 
Eval step: 399 , used_time=115.788094 sec., loss=0.316863 
Eval step: 399 , used_time=115.693875 sec., loss=0.316863 
Eval step: 499 , used_time=144.806786 sec., loss=0.417448 
Eval step: 499 , used_time=144.712615 sec., loss=0.417448 
NLL Validation: loss = 0.451227. correct prediction ratio  5657/6516 ~  0.868171
NLL Validation: loss = 0.451227. correct prediction ratio  5657/6516 ~  0.868171
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.1.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.1.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.935342 sec., loss=0.077826 
Eval step: 99 , used_time=36.567600 sec., loss=0.077826 
Eval step: 199 , used_time=56.944395 sec., loss=0.201883 
Eval step: 199 , used_time=65.576672 sec., loss=0.201883 
Eval step: 299 , used_time=85.960128 sec., loss=0.305515 
Eval step: 299 , used_time=94.592399 sec., loss=0.305515 
Eval step: 399 , used_time=123.672327 sec., loss=0.316863 
Eval step: 399 , used_time=115.040100 sec., loss=0.316863 
Eval step: 499 , used_time=144.065049 sec., loss=0.417448 
Eval step: 499 , used_time=152.697338 sec., loss=0.417448 
NLL Validation: loss = 0.451227. correct prediction ratio  5657/6516 ~  0.868171
NLL Validation: loss = 0.451227. correct prediction ratio  5657/6516 ~  0.868171
Av Loss per epoch=0.185641
epoch total correct predictions=55291
***** Epoch 2 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.1.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.1.4907
Av Loss per epoch=0.185641
epoch total correct predictions=55291
***** Epoch 2 *****
Epoch: 2: Step: 1/4907, loss=0.000268, lr=0.000019
Epoch: 2: Step: 1/4907, loss=0.000268, lr=0.000019
Train batch 100
Avg. loss per last 100 batches: 0.130475
Train batch 100
Avg. loss per last 100 batches: 0.130475
Epoch: 2: Step: 101/4907, loss=0.010334, lr=0.000019
Epoch: 2: Step: 101/4907, loss=0.010334, lr=0.000019
Train batch 200
Avg. loss per last 100 batches: 0.127704
Train batch 200
Avg. loss per last 100 batches: 0.127704
Epoch: 2: Step: 201/4907, loss=0.000625, lr=0.000019
Epoch: 2: Step: 201/4907, loss=0.000625, lr=0.000019
Train batch 300
Avg. loss per last 100 batches: 0.151365
Train batch 300
Avg. loss per last 100 batches: 0.151365
Epoch: 2: Step: 301/4907, loss=0.043701, lr=0.000019
Epoch: 2: Step: 301/4907, loss=0.043701, lr=0.000019
Train batch 400
Avg. loss per last 100 batches: 0.101573
Train batch 400
Avg. loss per last 100 batches: 0.101573
Epoch: 2: Step: 401/4907, loss=0.147218, lr=0.000019
Epoch: 2: Step: 401/4907, loss=0.147218, lr=0.000019
Train batch 500
Avg. loss per last 100 batches: 0.119681
Train batch 500
Avg. loss per last 100 batches: 0.119681
Epoch: 2: Step: 501/4907, loss=0.148791, lr=0.000019
Epoch: 2: Step: 501/4907, loss=0.148791, lr=0.000019
Train batch 600
Avg. loss per last 100 batches: 0.133358
Train batch 600
Avg. loss per last 100 batches: 0.133358
Epoch: 2: Step: 601/4907, loss=0.001056, lr=0.000019
Epoch: 2: Step: 601/4907, loss=0.001056, lr=0.000019
Train batch 700
Avg. loss per last 100 batches: 0.168708
Train batch 700
Avg. loss per last 100 batches: 0.168708
Epoch: 2: Step: 701/4907, loss=0.029978, lr=0.000019
Epoch: 2: Step: 701/4907, loss=0.029978, lr=0.000019
Train batch 800
Avg. loss per last 100 batches: 0.163624
Train batch 800
Avg. loss per last 100 batches: 0.163624
Epoch: 2: Step: 801/4907, loss=0.153192, lr=0.000019
Epoch: 2: Step: 801/4907, loss=0.153192, lr=0.000019
Train batch 900
Avg. loss per last 100 batches: 0.169148
Train batch 900
Avg. loss per last 100 batches: 0.169148
Epoch: 2: Step: 901/4907, loss=0.033535, lr=0.000019
Epoch: 2: Step: 901/4907, loss=0.033535, lr=0.000019
Train batch 1000
Avg. loss per last 100 batches: 0.132535
Train batch 1000
Avg. loss per last 100 batches: 0.132535
Epoch: 2: Step: 1001/4907, loss=0.002488, lr=0.000019
Epoch: 2: Step: 1001/4907, loss=0.002488, lr=0.000019
Train batch 1100
Avg. loss per last 100 batches: 0.125052
Train batch 1100
Avg. loss per last 100 batches: 0.125052
Epoch: 2: Step: 1101/4907, loss=0.132047, lr=0.000019
Epoch: 2: Step: 1101/4907, loss=0.132047, lr=0.000019
Train batch 1200
Avg. loss per last 100 batches: 0.135744
Train batch 1200
Avg. loss per last 100 batches: 0.135744
Epoch: 2: Step: 1201/4907, loss=0.007177, lr=0.000019
Epoch: 2: Step: 1201/4907, loss=0.007177, lr=0.000019
Train batch 1300
Avg. loss per last 100 batches: 0.160538
Train batch 1300
Avg. loss per last 100 batches: 0.160538
Epoch: 2: Step: 1301/4907, loss=0.053374, lr=0.000019
Epoch: 2: Step: 1301/4907, loss=0.053374, lr=0.000019
Train batch 1400
Avg. loss per last 100 batches: 0.105453
Train batch 1400
Avg. loss per last 100 batches: 0.105453
Epoch: 2: Step: 1401/4907, loss=0.084464, lr=0.000019
Epoch: 2: Step: 1401/4907, loss=0.084464, lr=0.000019
Train batch 1500
Avg. loss per last 100 batches: 0.142406
Train batch 1500
Avg. loss per last 100 batches: 0.142406
Epoch: 2: Step: 1501/4907, loss=0.658203, lr=0.000019
Epoch: 2: Step: 1501/4907, loss=0.658203, lr=0.000019
Train batch 1600
Avg. loss per last 100 batches: 0.162034
Train batch 1600
Avg. loss per last 100 batches: 0.162034
Epoch: 2: Step: 1601/4907, loss=0.009362, lr=0.000019
Epoch: 2: Step: 1601/4907, loss=0.009362, lr=0.000019
Train batch 1700
Avg. loss per last 100 batches: 0.127299
Train batch 1700
Avg. loss per last 100 batches: 0.127299
Epoch: 2: Step: 1701/4907, loss=0.004520, lr=0.000019
Epoch: 2: Step: 1701/4907, loss=0.004520, lr=0.000019
Train batch 1800
Avg. loss per last 100 batches: 0.145849
Train batch 1800
Avg. loss per last 100 batches: 0.145849
Epoch: 2: Step: 1801/4907, loss=0.398162, lr=0.000019
Epoch: 2: Step: 1801/4907, loss=0.398162, lr=0.000019
Train batch 1900
Avg. loss per last 100 batches: 0.174043
Train batch 1900
Avg. loss per last 100 batches: 0.174043
Epoch: 2: Step: 1901/4907, loss=0.473691, lr=0.000019
Epoch: 2: Step: 1901/4907, loss=0.473691, lr=0.000019
Train batch 2000
Avg. loss per last 100 batches: 0.157203
Train batch 2000
Avg. loss per last 100 batches: 0.157203
Epoch: 2: Step: 2001/4907, loss=0.024653, lr=0.000019
Epoch: 2: Step: 2001/4907, loss=0.024653, lr=0.000019
Train batch 2100
Avg. loss per last 100 batches: 0.177066
Train batch 2100
Avg. loss per last 100 batches: 0.177066
Epoch: 2: Step: 2101/4907, loss=0.009065, lr=0.000019
Epoch: 2: Step: 2101/4907, loss=0.009065, lr=0.000019
Train batch 2200
Avg. loss per last 100 batches: 0.151937
Train batch 2200
Avg. loss per last 100 batches: 0.151937
Epoch: 2: Step: 2201/4907, loss=0.262000, lr=0.000019
Epoch: 2: Step: 2201/4907, loss=0.262000, lr=0.000019
Train batch 2300
Avg. loss per last 100 batches: 0.135962
Train batch 2300
Avg. loss per last 100 batches: 0.135962
Epoch: 2: Step: 2301/4907, loss=0.043347, lr=0.000019
Epoch: 2: Step: 2301/4907, loss=0.043347, lr=0.000019
Train batch 2400
Avg. loss per last 100 batches: 0.146038
Train batch 2400
Avg. loss per last 100 batches: 0.146038
Epoch: 2: Step: 2401/4907, loss=0.238134, lr=0.000019
Epoch: 2: Step: 2401/4907, loss=0.238134, lr=0.000019
Train batch 2500
Avg. loss per last 100 batches: 0.149174
Train batch 2500
Avg. loss per last 100 batches: 0.149174
Epoch: 2: Step: 2501/4907, loss=0.123742, lr=0.000019
Epoch: 2: Step: 2501/4907, loss=0.123742, lr=0.000019
Train batch 2600
Avg. loss per last 100 batches: 0.137475
Train batch 2600
Avg. loss per last 100 batches: 0.137475
Epoch: 2: Step: 2601/4907, loss=0.000033, lr=0.000019
Epoch: 2: Step: 2601/4907, loss=0.000033, lr=0.000019
Train batch 2700
Avg. loss per last 100 batches: 0.164814
Train batch 2700
Avg. loss per last 100 batches: 0.164814
Epoch: 2: Step: 2701/4907, loss=0.076965, lr=0.000019
Epoch: 2: Step: 2701/4907, loss=0.076965, lr=0.000019
Train batch 2800
Avg. loss per last 100 batches: 0.142691
Train batch 2800
Avg. loss per last 100 batches: 0.142691
Epoch: 2: Step: 2801/4907, loss=0.007685, lr=0.000019
Epoch: 2: Step: 2801/4907, loss=0.007685, lr=0.000019
Train batch 2900
Avg. loss per last 100 batches: 0.132201
Train batch 2900
Avg. loss per last 100 batches: 0.132201
Epoch: 2: Step: 2901/4907, loss=0.006690, lr=0.000019
Epoch: 2: Step: 2901/4907, loss=0.006690, lr=0.000019
Train batch 3000
Avg. loss per last 100 batches: 0.131705
Train batch 3000
Avg. loss per last 100 batches: 0.131705
Epoch: 2: Step: 3001/4907, loss=0.001988, lr=0.000019
Epoch: 2: Step: 3001/4907, loss=0.001988, lr=0.000019
Train batch 3100
Avg. loss per last 100 batches: 0.134688
Train batch 3100
Avg. loss per last 100 batches: 0.134688
Epoch: 2: Step: 3101/4907, loss=0.245513, lr=0.000019
Epoch: 2: Step: 3101/4907, loss=0.245513, lr=0.000019
Train batch 3200
Avg. loss per last 100 batches: 0.123418
Train batch 3200
Avg. loss per last 100 batches: 0.123418
Epoch: 2: Step: 3201/4907, loss=0.066106, lr=0.000019
Epoch: 2: Step: 3201/4907, loss=0.066106, lr=0.000019
Train batch 3300
Avg. loss per last 100 batches: 0.133523
Train batch 3300
Avg. loss per last 100 batches: 0.133523
Epoch: 2: Step: 3301/4907, loss=0.002442, lr=0.000019
Epoch: 2: Step: 3301/4907, loss=0.002442, lr=0.000019
Train batch 3400
Avg. loss per last 100 batches: 0.146097
Train batch 3400
Avg. loss per last 100 batches: 0.146097
Epoch: 2: Step: 3401/4907, loss=0.098527, lr=0.000019
Epoch: 2: Step: 3401/4907, loss=0.098527, lr=0.000019
Train batch 3500
Avg. loss per last 100 batches: 0.177713
Train batch 3500
Avg. loss per last 100 batches: 0.177713
Epoch: 2: Step: 3501/4907, loss=0.647211, lr=0.000019
Epoch: 2: Step: 3501/4907, loss=0.647211, lr=0.000019
Train batch 3600
Avg. loss per last 100 batches: 0.161815
Train batch 3600
Avg. loss per last 100 batches: 0.161815
Epoch: 2: Step: 3601/4907, loss=0.353080, lr=0.000019
Epoch: 2: Step: 3601/4907, loss=0.353080, lr=0.000019
Train batch 3700
Avg. loss per last 100 batches: 0.125213
Train batch 3700
Avg. loss per last 100 batches: 0.125213
Epoch: 2: Step: 3701/4907, loss=0.000555, lr=0.000019
Epoch: 2: Step: 3701/4907, loss=0.000555, lr=0.000019
Train batch 3800
Avg. loss per last 100 batches: 0.164101
Train batch 3800
Avg. loss per last 100 batches: 0.164101
Epoch: 2: Step: 3801/4907, loss=0.044440, lr=0.000019
Epoch: 2: Step: 3801/4907, loss=0.044440, lr=0.000019
Train batch 3900
Avg. loss per last 100 batches: 0.121745
Train batch 3900
Avg. loss per last 100 batches: 0.121745
Epoch: 2: Step: 3901/4907, loss=0.387297, lr=0.000019
Epoch: 2: Step: 3901/4907, loss=0.387297, lr=0.000019
Train batch 4000
Avg. loss per last 100 batches: 0.128386
Train batch 4000
Avg. loss per last 100 batches: 0.128386
Epoch: 2: Step: 4001/4907, loss=0.055780, lr=0.000019
Epoch: 2: Step: 4001/4907, loss=0.055780, lr=0.000019
Train batch 4100
Avg. loss per last 100 batches: 0.126807
Train batch 4100
Avg. loss per last 100 batches: 0.126807
Epoch: 2: Step: 4101/4907, loss=0.018521, lr=0.000019
Epoch: 2: Step: 4101/4907, loss=0.018521, lr=0.000019
Train batch 4200
Avg. loss per last 100 batches: 0.134794
Train batch 4200
Avg. loss per last 100 batches: 0.134794
Epoch: 2: Step: 4201/4907, loss=0.015150, lr=0.000019
Epoch: 2: Step: 4201/4907, loss=0.015150, lr=0.000019
Train batch 4300
Avg. loss per last 100 batches: 0.175592
Train batch 4300
Avg. loss per last 100 batches: 0.175592
Epoch: 2: Step: 4301/4907, loss=0.018430, lr=0.000019
Epoch: 2: Step: 4301/4907, loss=0.018430, lr=0.000019
Train batch 4400
Avg. loss per last 100 batches: 0.129347
Train batch 4400
Avg. loss per last 100 batches: 0.129347
Epoch: 2: Step: 4401/4907, loss=0.214990, lr=0.000019
Epoch: 2: Step: 4401/4907, loss=0.214990, lr=0.000019
Train batch 4500
Avg. loss per last 100 batches: 0.184668
Train batch 4500
Avg. loss per last 100 batches: 0.184668
Epoch: 2: Step: 4501/4907, loss=0.209033, lr=0.000019
Epoch: 2: Step: 4501/4907, loss=0.209033, lr=0.000019
Train batch 4600
Avg. loss per last 100 batches: 0.143202
Train batch 4600
Avg. loss per last 100 batches: 0.143202
Epoch: 2: Step: 4601/4907, loss=0.139282, lr=0.000019
Epoch: 2: Step: 4601/4907, loss=0.139282, lr=0.000019
Train batch 4700
Avg. loss per last 100 batches: 0.148346
Train batch 4700
Avg. loss per last 100 batches: 0.148346
Epoch: 2: Step: 4701/4907, loss=0.144561, lr=0.000019
Epoch: 2: Step: 4701/4907, loss=0.144561, lr=0.000019
Train batch 4800
Avg. loss per last 100 batches: 0.165761
Train batch 4800
Avg. loss per last 100 batches: 0.165761
Epoch: 2: Step: 4801/4907, loss=0.009912, lr=0.000019
Epoch: 2: Step: 4801/4907, loss=0.009912, lr=0.000019
Train batch 4900
Avg. loss per last 100 batches: 0.129788
Train batch 4900
Avg. loss per last 100 batches: 0.129788
Epoch: 2: Step: 4901/4907, loss=0.165693, lr=0.000019
Epoch: 2: Step: 4901/4907, loss=0.165693, lr=0.000019
Validation: Epoch: 2 Step: 4907/4907
NLL validation ...
Validation: Epoch: 2 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.335539 sec., loss=0.122542 
Eval step: 99 , used_time=28.230158 sec., loss=0.122542 
Eval step: 199 , used_time=57.362612 sec., loss=0.061032 
Eval step: 199 , used_time=57.257202 sec., loss=0.061032 
Eval step: 299 , used_time=86.297840 sec., loss=1.003866 
Eval step: 299 , used_time=86.403234 sec., loss=1.003866 
Eval step: 399 , used_time=115.336743 sec., loss=0.220949 
Eval step: 399 , used_time=115.442150 sec., loss=0.220949 
Eval step: 499 , used_time=144.530285 sec., loss=0.433617 
Eval step: 499 , used_time=144.424981 sec., loss=0.433617 
NLL Validation: loss = 0.462244. correct prediction ratio  5725/6516 ~  0.878607
NLL Validation: loss = 0.462244. correct prediction ratio  5725/6516 ~  0.878607
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.2.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.2.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.933107 sec., loss=0.122542 
Eval step: 99 , used_time=36.510980 sec., loss=0.122542 
Eval step: 199 , used_time=65.536556 sec., loss=0.061032 
Eval step: 199 , used_time=56.958689 sec., loss=0.061032 
Eval step: 299 , used_time=85.982447 sec., loss=1.003866 
Eval step: 299 , used_time=94.560378 sec., loss=1.003866 
Eval step: 399 , used_time=115.018286 sec., loss=0.220949 
Eval step: 399 , used_time=123.596190 sec., loss=0.220949 
Eval step: 499 , used_time=144.037237 sec., loss=0.433617 
Eval step: 499 , used_time=152.615137 sec., loss=0.433617 
NLL Validation: loss = 0.462244. correct prediction ratio  5725/6516 ~  0.878607
NLL Validation: loss = 0.462244. correct prediction ratio  5725/6516 ~  0.878607
Av Loss per epoch=0.143918
epoch total correct predictions=56122
***** Epoch 3 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.2.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.2.4907
Av Loss per epoch=0.143918
epoch total correct predictions=56122
***** Epoch 3 *****
Epoch: 3: Step: 1/4907, loss=0.004473, lr=0.000019
Epoch: 3: Step: 1/4907, loss=0.004473, lr=0.000019
Train batch 100
Avg. loss per last 100 batches: 0.119629
Train batch 100
Avg. loss per last 100 batches: 0.119629
Epoch: 3: Step: 101/4907, loss=0.093036, lr=0.000019
Epoch: 3: Step: 101/4907, loss=0.093036, lr=0.000019
Train batch 200
Avg. loss per last 100 batches: 0.102527
Train batch 200
Avg. loss per last 100 batches: 0.102527
Epoch: 3: Step: 201/4907, loss=0.040410, lr=0.000019
Epoch: 3: Step: 201/4907, loss=0.040410, lr=0.000019
Train batch 300
Avg. loss per last 100 batches: 0.111630
Train batch 300
Avg. loss per last 100 batches: 0.111630
Epoch: 3: Step: 301/4907, loss=0.189625, lr=0.000019
Epoch: 3: Step: 301/4907, loss=0.189625, lr=0.000019
Train batch 400
Avg. loss per last 100 batches: 0.066411
Train batch 400
Avg. loss per last 100 batches: 0.066411
Epoch: 3: Step: 401/4907, loss=0.055362, lr=0.000019
Epoch: 3: Step: 401/4907, loss=0.055362, lr=0.000019
Train batch 500
Avg. loss per last 100 batches: 0.146594
Train batch 500
Avg. loss per last 100 batches: 0.146594
Epoch: 3: Step: 501/4907, loss=0.003542, lr=0.000019
Epoch: 3: Step: 501/4907, loss=0.003542, lr=0.000019
Train batch 600
Avg. loss per last 100 batches: 0.106360
Train batch 600
Avg. loss per last 100 batches: 0.106360
Epoch: 3: Step: 601/4907, loss=0.158010, lr=0.000019
Epoch: 3: Step: 601/4907, loss=0.158010, lr=0.000019
Train batch 700
Avg. loss per last 100 batches: 0.117489
Train batch 700
Avg. loss per last 100 batches: 0.117489
Epoch: 3: Step: 701/4907, loss=0.004962, lr=0.000019
Epoch: 3: Step: 701/4907, loss=0.004962, lr=0.000019
Train batch 800
Avg. loss per last 100 batches: 0.123142
Train batch 800
Avg. loss per last 100 batches: 0.123142
Epoch: 3: Step: 801/4907, loss=0.001598, lr=0.000019
Epoch: 3: Step: 801/4907, loss=0.001598, lr=0.000019
Train batch 900
Avg. loss per last 100 batches: 0.096027
Train batch 900
Avg. loss per last 100 batches: 0.096027
Epoch: 3: Step: 901/4907, loss=0.084808, lr=0.000019
Epoch: 3: Step: 901/4907, loss=0.084808, lr=0.000019
Train batch 1000
Avg. loss per last 100 batches: 0.125055
Train batch 1000
Avg. loss per last 100 batches: 0.125055
Epoch: 3: Step: 1001/4907, loss=0.000018, lr=0.000019
Epoch: 3: Step: 1001/4907, loss=0.000018, lr=0.000019
Train batch 1100
Avg. loss per last 100 batches: 0.151193
Train batch 1100
Avg. loss per last 100 batches: 0.151193
Epoch: 3: Step: 1101/4907, loss=0.574116, lr=0.000019
Epoch: 3: Step: 1101/4907, loss=0.574116, lr=0.000019
Train batch 1200
Avg. loss per last 100 batches: 0.140426
Train batch 1200
Avg. loss per last 100 batches: 0.140426
Epoch: 3: Step: 1201/4907, loss=0.000392, lr=0.000018
Epoch: 3: Step: 1201/4907, loss=0.000392, lr=0.000018
Train batch 1300
Avg. loss per last 100 batches: 0.128039
Train batch 1300
Avg. loss per last 100 batches: 0.128039
Epoch: 3: Step: 1301/4907, loss=0.117565, lr=0.000018
Epoch: 3: Step: 1301/4907, loss=0.117565, lr=0.000018
Train batch 1400
Avg. loss per last 100 batches: 0.130077
Train batch 1400
Avg. loss per last 100 batches: 0.130077
Epoch: 3: Step: 1401/4907, loss=0.229384, lr=0.000018
Epoch: 3: Step: 1401/4907, loss=0.229384, lr=0.000018
Train batch 1500
Avg. loss per last 100 batches: 0.107464
Train batch 1500
Avg. loss per last 100 batches: 0.107464
Epoch: 3: Step: 1501/4907, loss=0.010459, lr=0.000018
Epoch: 3: Step: 1501/4907, loss=0.010459, lr=0.000018
Train batch 1600
Avg. loss per last 100 batches: 0.121274
Train batch 1600
Avg. loss per last 100 batches: 0.121274
Epoch: 3: Step: 1601/4907, loss=0.963592, lr=0.000018
Epoch: 3: Step: 1601/4907, loss=0.963592, lr=0.000018
Train batch 1700
Avg. loss per last 100 batches: 0.105798
Train batch 1700
Avg. loss per last 100 batches: 0.105798
Epoch: 3: Step: 1701/4907, loss=0.247258, lr=0.000018
Epoch: 3: Step: 1701/4907, loss=0.247258, lr=0.000018
Train batch 1800
Avg. loss per last 100 batches: 0.124541
Train batch 1800
Avg. loss per last 100 batches: 0.124541
Epoch: 3: Step: 1801/4907, loss=0.111277, lr=0.000018
Epoch: 3: Step: 1801/4907, loss=0.111277, lr=0.000018
Train batch 1900
Avg. loss per last 100 batches: 0.150231
Train batch 1900
Avg. loss per last 100 batches: 0.150231
Epoch: 3: Step: 1901/4907, loss=0.001750, lr=0.000018
Epoch: 3: Step: 1901/4907, loss=0.001750, lr=0.000018
Train batch 2000
Avg. loss per last 100 batches: 0.113310
Train batch 2000
Avg. loss per last 100 batches: 0.113310
Epoch: 3: Step: 2001/4907, loss=0.606853, lr=0.000018
Epoch: 3: Step: 2001/4907, loss=0.606853, lr=0.000018
Train batch 2100
Avg. loss per last 100 batches: 0.114196
Train batch 2100
Avg. loss per last 100 batches: 0.114196
Epoch: 3: Step: 2101/4907, loss=0.137080, lr=0.000018
Epoch: 3: Step: 2101/4907, loss=0.137080, lr=0.000018
Train batch 2200
Avg. loss per last 100 batches: 0.128158
Train batch 2200
Avg. loss per last 100 batches: 0.128158
Epoch: 3: Step: 2201/4907, loss=0.138897, lr=0.000018
Epoch: 3: Step: 2201/4907, loss=0.138897, lr=0.000018
Train batch 2300
Avg. loss per last 100 batches: 0.102113
Train batch 2300
Avg. loss per last 100 batches: 0.102113
Epoch: 3: Step: 2301/4907, loss=0.011787, lr=0.000018
Epoch: 3: Step: 2301/4907, loss=0.011787, lr=0.000018
Train batch 2400
Avg. loss per last 100 batches: 0.097110
Train batch 2400
Avg. loss per last 100 batches: 0.097110
Epoch: 3: Step: 2401/4907, loss=0.571753, lr=0.000018
Epoch: 3: Step: 2401/4907, loss=0.571753, lr=0.000018
Train batch 2500
Avg. loss per last 100 batches: 0.123071
Train batch 2500
Avg. loss per last 100 batches: 0.123071
Epoch: 3: Step: 2501/4907, loss=0.000196, lr=0.000018
Epoch: 3: Step: 2501/4907, loss=0.000196, lr=0.000018
Train batch 2600
Avg. loss per last 100 batches: 0.156281
Train batch 2600
Avg. loss per last 100 batches: 0.156281
Epoch: 3: Step: 2601/4907, loss=0.086756, lr=0.000018
Epoch: 3: Step: 2601/4907, loss=0.086756, lr=0.000018
Train batch 2700
Avg. loss per last 100 batches: 0.127812
Train batch 2700
Avg. loss per last 100 batches: 0.127812
Epoch: 3: Step: 2701/4907, loss=0.004001, lr=0.000018
Epoch: 3: Step: 2701/4907, loss=0.004001, lr=0.000018
Train batch 2800
Avg. loss per last 100 batches: 0.104981
Train batch 2800
Avg. loss per last 100 batches: 0.104981
Epoch: 3: Step: 2801/4907, loss=0.002765, lr=0.000018
Epoch: 3: Step: 2801/4907, loss=0.002765, lr=0.000018
Train batch 2900
Avg. loss per last 100 batches: 0.116319
Train batch 2900
Avg. loss per last 100 batches: 0.116319
Epoch: 3: Step: 2901/4907, loss=0.183596, lr=0.000018
Epoch: 3: Step: 2901/4907, loss=0.183596, lr=0.000018
Train batch 3000
Avg. loss per last 100 batches: 0.111848
Train batch 3000
Avg. loss per last 100 batches: 0.111848
Epoch: 3: Step: 3001/4907, loss=0.001847, lr=0.000018
Epoch: 3: Step: 3001/4907, loss=0.001847, lr=0.000018
Train batch 3100
Avg. loss per last 100 batches: 0.134781
Train batch 3100
Avg. loss per last 100 batches: 0.134781
Epoch: 3: Step: 3101/4907, loss=0.011978, lr=0.000018
Epoch: 3: Step: 3101/4907, loss=0.011978, lr=0.000018
Train batch 3200
Avg. loss per last 100 batches: 0.127810
Train batch 3200
Avg. loss per last 100 batches: 0.127810
Epoch: 3: Step: 3201/4907, loss=0.000007, lr=0.000018
Epoch: 3: Step: 3201/4907, loss=0.000007, lr=0.000018
Train batch 3300
Avg. loss per last 100 batches: 0.086554
Train batch 3300
Avg. loss per last 100 batches: 0.086554
Epoch: 3: Step: 3301/4907, loss=0.256230, lr=0.000018
Epoch: 3: Step: 3301/4907, loss=0.256230, lr=0.000018
Train batch 3400
Avg. loss per last 100 batches: 0.129516
Train batch 3400
Avg. loss per last 100 batches: 0.129516
Epoch: 3: Step: 3401/4907, loss=0.006425, lr=0.000018
Epoch: 3: Step: 3401/4907, loss=0.006425, lr=0.000018
Train batch 3500
Avg. loss per last 100 batches: 0.103982
Train batch 3500
Avg. loss per last 100 batches: 0.103982
Epoch: 3: Step: 3501/4907, loss=0.294638, lr=0.000018
Epoch: 3: Step: 3501/4907, loss=0.294638, lr=0.000018
Train batch 3600
Avg. loss per last 100 batches: 0.098667
Train batch 3600
Avg. loss per last 100 batches: 0.098667
Epoch: 3: Step: 3601/4907, loss=0.010094, lr=0.000018
Epoch: 3: Step: 3601/4907, loss=0.010094, lr=0.000018
Train batch 3700
Avg. loss per last 100 batches: 0.109604
Train batch 3700
Avg. loss per last 100 batches: 0.109604
Epoch: 3: Step: 3701/4907, loss=0.000188, lr=0.000018
Epoch: 3: Step: 3701/4907, loss=0.000188, lr=0.000018
Train batch 3800
Avg. loss per last 100 batches: 0.165877
Train batch 3800
Avg. loss per last 100 batches: 0.165877
Epoch: 3: Step: 3801/4907, loss=0.039779, lr=0.000018
Epoch: 3: Step: 3801/4907, loss=0.039779, lr=0.000018
Train batch 3900
Avg. loss per last 100 batches: 0.119423
Train batch 3900
Avg. loss per last 100 batches: 0.119423
Epoch: 3: Step: 3901/4907, loss=0.375582, lr=0.000018
Epoch: 3: Step: 3901/4907, loss=0.375582, lr=0.000018
Train batch 4000
Avg. loss per last 100 batches: 0.142770
Train batch 4000
Avg. loss per last 100 batches: 0.142770
Epoch: 3: Step: 4001/4907, loss=0.433221, lr=0.000018
Epoch: 3: Step: 4001/4907, loss=0.433221, lr=0.000018
Train batch 4100
Avg. loss per last 100 batches: 0.113488
Train batch 4100
Avg. loss per last 100 batches: 0.113488
Epoch: 3: Step: 4101/4907, loss=0.025105, lr=0.000018
Epoch: 3: Step: 4101/4907, loss=0.025105, lr=0.000018
Train batch 4200
Avg. loss per last 100 batches: 0.121757
Train batch 4200
Avg. loss per last 100 batches: 0.121757
Epoch: 3: Step: 4201/4907, loss=0.001382, lr=0.000018
Epoch: 3: Step: 4201/4907, loss=0.001382, lr=0.000018
Train batch 4300
Avg. loss per last 100 batches: 0.121412
Train batch 4300
Avg. loss per last 100 batches: 0.121412
Epoch: 3: Step: 4301/4907, loss=0.002254, lr=0.000018
Epoch: 3: Step: 4301/4907, loss=0.002254, lr=0.000018
Train batch 4400
Avg. loss per last 100 batches: 0.095577
Train batch 4400
Avg. loss per last 100 batches: 0.095577
Epoch: 3: Step: 4401/4907, loss=0.009432, lr=0.000018
Epoch: 3: Step: 4401/4907, loss=0.009432, lr=0.000018
Train batch 4500
Avg. loss per last 100 batches: 0.121860
Train batch 4500
Avg. loss per last 100 batches: 0.121860
Epoch: 3: Step: 4501/4907, loss=0.281079, lr=0.000018
Epoch: 3: Step: 4501/4907, loss=0.281079, lr=0.000018
Train batch 4600
Avg. loss per last 100 batches: 0.129762
Train batch 4600
Avg. loss per last 100 batches: 0.129762
Epoch: 3: Step: 4601/4907, loss=0.016645, lr=0.000018
Epoch: 3: Step: 4601/4907, loss=0.016645, lr=0.000018
Train batch 4700
Avg. loss per last 100 batches: 0.119798
Train batch 4700
Avg. loss per last 100 batches: 0.119798
Epoch: 3: Step: 4701/4907, loss=0.007714, lr=0.000018
Epoch: 3: Step: 4701/4907, loss=0.007714, lr=0.000018
Train batch 4800
Avg. loss per last 100 batches: 0.117641
Train batch 4800
Avg. loss per last 100 batches: 0.117641
Epoch: 3: Step: 4801/4907, loss=0.094617, lr=0.000018
Epoch: 3: Step: 4801/4907, loss=0.094617, lr=0.000018
Train batch 4900
Avg. loss per last 100 batches: 0.111497
Train batch 4900
Avg. loss per last 100 batches: 0.111497
Epoch: 3: Step: 4901/4907, loss=0.078763, lr=0.000018
Epoch: 3: Step: 4901/4907, loss=0.078763, lr=0.000018
Validation: Epoch: 3 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 3 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=29.266317 sec., loss=0.028750 
Eval step: 99 , used_time=28.294732 sec., loss=0.028750 
Eval step: 199 , used_time=57.293950 sec., loss=0.364822 
Eval step: 199 , used_time=58.265505 sec., loss=0.364822 
Eval step: 299 , used_time=87.258734 sec., loss=0.629369 
Eval step: 299 , used_time=86.287220 sec., loss=0.629369 
Eval step: 399 , used_time=115.286576 sec., loss=0.364496 
Eval step: 399 , used_time=116.258183 sec., loss=0.364496 
Eval step: 499 , used_time=145.256582 sec., loss=0.157763 
Eval step: 499 , used_time=144.285015 sec., loss=0.157763 
NLL Validation: loss = 0.500311. correct prediction ratio  5708/6516 ~  0.875998
NLL Validation: loss = 0.500311. correct prediction ratio  5708/6516 ~  0.875998
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.3.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.3.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=35.419217 sec., loss=0.028750 
Eval step: 99 , used_time=27.820044 sec., loss=0.028750 
Eval step: 199 , used_time=56.821861 sec., loss=0.364822 
Eval step: 199 , used_time=64.421064 sec., loss=0.364822 
Eval step: 299 , used_time=93.427894 sec., loss=0.629369 
Eval step: 299 , used_time=85.830885 sec., loss=0.629369 
Eval step: 399 , used_time=114.843012 sec., loss=0.364496 
Eval step: 399 , used_time=122.442214 sec., loss=0.364496 
Eval step: 499 , used_time=143.876518 sec., loss=0.157763 
Eval step: 499 , used_time=151.475744 sec., loss=0.157763 
NLL Validation: loss = 0.500311. correct prediction ratio  5708/6516 ~  0.875998
NLL Validation: loss = 0.500311. correct prediction ratio  5708/6516 ~  0.875998
Av Loss per epoch=0.119083
epoch total correct predictions=56624
***** Epoch 4 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.3.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.3.4907
Av Loss per epoch=0.119083
epoch total correct predictions=56624
***** Epoch 4 *****
Epoch: 4: Step: 1/4907, loss=0.079615, lr=0.000018
Epoch: 4: Step: 1/4907, loss=0.079615, lr=0.000018
Train batch 100
Avg. loss per last 100 batches: 0.079507
Train batch 100
Avg. loss per last 100 batches: 0.079507
Epoch: 4: Step: 101/4907, loss=0.000878, lr=0.000018
Epoch: 4: Step: 101/4907, loss=0.000878, lr=0.000018
Train batch 200
Avg. loss per last 100 batches: 0.128135
Train batch 200
Avg. loss per last 100 batches: 0.128135
Epoch: 4: Step: 201/4907, loss=0.423955, lr=0.000018
Epoch: 4: Step: 201/4907, loss=0.423955, lr=0.000018
Train batch 300
Avg. loss per last 100 batches: 0.100475
Train batch 300
Avg. loss per last 100 batches: 0.100475
Epoch: 4: Step: 301/4907, loss=0.608137, lr=0.000018
Epoch: 4: Step: 301/4907, loss=0.608137, lr=0.000018
Train batch 400
Avg. loss per last 100 batches: 0.115109
Train batch 400
Avg. loss per last 100 batches: 0.115109
Epoch: 4: Step: 401/4907, loss=0.000062, lr=0.000018
Epoch: 4: Step: 401/4907, loss=0.000062, lr=0.000018
Train batch 500
Avg. loss per last 100 batches: 0.103152
Train batch 500
Avg. loss per last 100 batches: 0.103152
Epoch: 4: Step: 501/4907, loss=0.141809, lr=0.000018
Epoch: 4: Step: 501/4907, loss=0.141809, lr=0.000018
Train batch 600
Avg. loss per last 100 batches: 0.116558
Train batch 600
Avg. loss per last 100 batches: 0.116558
Epoch: 4: Step: 601/4907, loss=0.009834, lr=0.000018
Epoch: 4: Step: 601/4907, loss=0.009834, lr=0.000018
Train batch 700
Avg. loss per last 100 batches: 0.099209
Train batch 700
Avg. loss per last 100 batches: 0.099209
Epoch: 4: Step: 701/4907, loss=0.089348, lr=0.000018
Epoch: 4: Step: 701/4907, loss=0.089348, lr=0.000018
Train batch 800
Avg. loss per last 100 batches: 0.154910
Train batch 800
Avg. loss per last 100 batches: 0.154910
Epoch: 4: Step: 801/4907, loss=0.295065, lr=0.000018
Epoch: 4: Step: 801/4907, loss=0.295065, lr=0.000018
Train batch 900
Avg. loss per last 100 batches: 0.086626
Train batch 900
Avg. loss per last 100 batches: 0.086626
Epoch: 4: Step: 901/4907, loss=0.007776, lr=0.000018
Epoch: 4: Step: 901/4907, loss=0.007776, lr=0.000018
Train batch 1000
Avg. loss per last 100 batches: 0.148632
Train batch 1000
Avg. loss per last 100 batches: 0.148632
Epoch: 4: Step: 1001/4907, loss=0.007279, lr=0.000018
Epoch: 4: Step: 1001/4907, loss=0.007279, lr=0.000018
Train batch 1100
Avg. loss per last 100 batches: 0.071484
Train batch 1100
Avg. loss per last 100 batches: 0.071484
Epoch: 4: Step: 1101/4907, loss=0.156648, lr=0.000018
Epoch: 4: Step: 1101/4907, loss=0.156648, lr=0.000018
Train batch 1200
Avg. loss per last 100 batches: 0.100159
Train batch 1200
Avg. loss per last 100 batches: 0.100159
Epoch: 4: Step: 1201/4907, loss=0.063571, lr=0.000018
Epoch: 4: Step: 1201/4907, loss=0.063571, lr=0.000018
Train batch 1300
Avg. loss per last 100 batches: 0.102197
Train batch 1300
Avg. loss per last 100 batches: 0.102197
Epoch: 4: Step: 1301/4907, loss=0.032887, lr=0.000018
Epoch: 4: Step: 1301/4907, loss=0.032887, lr=0.000018
Train batch 1400
Avg. loss per last 100 batches: 0.111199
Train batch 1400
Avg. loss per last 100 batches: 0.111199
Epoch: 4: Step: 1401/4907, loss=0.083662, lr=0.000018
Epoch: 4: Step: 1401/4907, loss=0.083662, lr=0.000018
Train batch 1500
Avg. loss per last 100 batches: 0.081717
Train batch 1500
Avg. loss per last 100 batches: 0.081717
Epoch: 4: Step: 1501/4907, loss=0.078136, lr=0.000018
Epoch: 4: Step: 1501/4907, loss=0.078136, lr=0.000018
Train batch 1600
Avg. loss per last 100 batches: 0.096330
Train batch 1600
Avg. loss per last 100 batches: 0.096330
Epoch: 4: Step: 1601/4907, loss=0.046333, lr=0.000018
Epoch: 4: Step: 1601/4907, loss=0.046333, lr=0.000018
Train batch 1700
Avg. loss per last 100 batches: 0.138445
Train batch 1700
Avg. loss per last 100 batches: 0.138445
Epoch: 4: Step: 1701/4907, loss=0.022784, lr=0.000018
Epoch: 4: Step: 1701/4907, loss=0.022784, lr=0.000018
Train batch 1800
Avg. loss per last 100 batches: 0.112658
Train batch 1800
Avg. loss per last 100 batches: 0.112658
Epoch: 4: Step: 1801/4907, loss=0.113855, lr=0.000018
Epoch: 4: Step: 1801/4907, loss=0.113855, lr=0.000018
Train batch 1900
Avg. loss per last 100 batches: 0.081977
Train batch 1900
Avg. loss per last 100 batches: 0.081977
Epoch: 4: Step: 1901/4907, loss=0.077979, lr=0.000018
Epoch: 4: Step: 1901/4907, loss=0.077979, lr=0.000018
Train batch 2000
Avg. loss per last 100 batches: 0.098182
Train batch 2000
Avg. loss per last 100 batches: 0.098182
Epoch: 4: Step: 2001/4907, loss=0.415406, lr=0.000018
Epoch: 4: Step: 2001/4907, loss=0.415406, lr=0.000018
Train batch 2100
Train batch 2100
Avg. loss per last 100 batches: 0.114798
Avg. loss per last 100 batches: 0.114798
Epoch: 4: Step: 2101/4907, loss=0.009244, lr=0.000018
Epoch: 4: Step: 2101/4907, loss=0.009244, lr=0.000018
Train batch 2200
Avg. loss per last 100 batches: 0.130390
Train batch 2200
Avg. loss per last 100 batches: 0.130390
Epoch: 4: Step: 2201/4907, loss=0.000306, lr=0.000018
Epoch: 4: Step: 2201/4907, loss=0.000306, lr=0.000018
Train batch 2300
Avg. loss per last 100 batches: 0.159396
Train batch 2300
Avg. loss per last 100 batches: 0.159396
Epoch: 4: Step: 2301/4907, loss=0.209806, lr=0.000018
Epoch: 4: Step: 2301/4907, loss=0.209806, lr=0.000018
Train batch 2400
Avg. loss per last 100 batches: 0.112158
Train batch 2400
Avg. loss per last 100 batches: 0.112158
Epoch: 4: Step: 2401/4907, loss=0.048326, lr=0.000018
Epoch: 4: Step: 2401/4907, loss=0.048326, lr=0.000018
Train batch 2500
Avg. loss per last 100 batches: 0.092262
Train batch 2500
Avg. loss per last 100 batches: 0.092262
Epoch: 4: Step: 2501/4907, loss=0.080409, lr=0.000018
Epoch: 4: Step: 2501/4907, loss=0.080409, lr=0.000018
Train batch 2600
Avg. loss per last 100 batches: 0.091421
Train batch 2600
Avg. loss per last 100 batches: 0.091421
Epoch: 4: Step: 2601/4907, loss=0.188629, lr=0.000018
Epoch: 4: Step: 2601/4907, loss=0.188629, lr=0.000018
Train batch 2700
Avg. loss per last 100 batches: 0.091957
Train batch 2700
Avg. loss per last 100 batches: 0.091957
Epoch: 4: Step: 2701/4907, loss=0.204399, lr=0.000018
Epoch: 4: Step: 2701/4907, loss=0.204399, lr=0.000018
Train batch 2800
Avg. loss per last 100 batches: 0.101089
Train batch 2800
Avg. loss per last 100 batches: 0.101089
Epoch: 4: Step: 2801/4907, loss=0.001440, lr=0.000018
Epoch: 4: Step: 2801/4907, loss=0.001440, lr=0.000018
Train batch 2900
Avg. loss per last 100 batches: 0.073587
Train batch 2900
Avg. loss per last 100 batches: 0.073587
Epoch: 4: Step: 2901/4907, loss=1.299675, lr=0.000018
Epoch: 4: Step: 2901/4907, loss=1.299675, lr=0.000018
Train batch 3000
Avg. loss per last 100 batches: 0.109971
Train batch 3000
Avg. loss per last 100 batches: 0.109971
Epoch: 4: Step: 3001/4907, loss=0.035141, lr=0.000018
Epoch: 4: Step: 3001/4907, loss=0.035141, lr=0.000018
Train batch 3100
Avg. loss per last 100 batches: 0.108660
Train batch 3100
Avg. loss per last 100 batches: 0.108660
Epoch: 4: Step: 3101/4907, loss=0.395609, lr=0.000018
Epoch: 4: Step: 3101/4907, loss=0.395609, lr=0.000018
Train batch 3200
Avg. loss per last 100 batches: 0.082484
Train batch 3200
Avg. loss per last 100 batches: 0.082484
Epoch: 4: Step: 3201/4907, loss=0.055622, lr=0.000018
Epoch: 4: Step: 3201/4907, loss=0.055622, lr=0.000018
Train batch 3300
Avg. loss per last 100 batches: 0.121746
Train batch 3300
Avg. loss per last 100 batches: 0.121746
Epoch: 4: Step: 3301/4907, loss=0.026974, lr=0.000018
Epoch: 4: Step: 3301/4907, loss=0.026974, lr=0.000018
Train batch 3400
Avg. loss per last 100 batches: 0.104950
Train batch 3400
Avg. loss per last 100 batches: 0.104950
Epoch: 4: Step: 3401/4907, loss=0.034076, lr=0.000018
Epoch: 4: Step: 3401/4907, loss=0.034076, lr=0.000018
Train batch 3500
Avg. loss per last 100 batches: 0.111844
Train batch 3500
Avg. loss per last 100 batches: 0.111844
Epoch: 4: Step: 3501/4907, loss=0.001391, lr=0.000018
Epoch: 4: Step: 3501/4907, loss=0.001391, lr=0.000018
Train batch 3600
Avg. loss per last 100 batches: 0.101029
Train batch 3600
Avg. loss per last 100 batches: 0.101029
Epoch: 4: Step: 3601/4907, loss=0.454105, lr=0.000018
Epoch: 4: Step: 3601/4907, loss=0.454105, lr=0.000018
Train batch 3700
Avg. loss per last 100 batches: 0.106764
Train batch 3700
Avg. loss per last 100 batches: 0.106764
Epoch: 4: Step: 3701/4907, loss=0.000600, lr=0.000018
Epoch: 4: Step: 3701/4907, loss=0.000600, lr=0.000018
Train batch 3800
Avg. loss per last 100 batches: 0.124726
Train batch 3800
Avg. loss per last 100 batches: 0.124726
Epoch: 4: Step: 3801/4907, loss=0.352924, lr=0.000018
Epoch: 4: Step: 3801/4907, loss=0.352924, lr=0.000018
Train batch 3900
Avg. loss per last 100 batches: 0.094739
Train batch 3900
Avg. loss per last 100 batches: 0.094739
Epoch: 4: Step: 3901/4907, loss=0.003445, lr=0.000018
Epoch: 4: Step: 3901/4907, loss=0.003445, lr=0.000018
Train batch 4000
Avg. loss per last 100 batches: 0.090152
Train batch 4000
Avg. loss per last 100 batches: 0.090152
Epoch: 4: Step: 4001/4907, loss=0.000464, lr=0.000018
Epoch: 4: Step: 4001/4907, loss=0.000464, lr=0.000018
Train batch 4100
Avg. loss per last 100 batches: 0.083771
Train batch 4100
Avg. loss per last 100 batches: 0.083771
Epoch: 4: Step: 4101/4907, loss=0.126245, lr=0.000018
Epoch: 4: Step: 4101/4907, loss=0.126245, lr=0.000018
Train batch 4200
Avg. loss per last 100 batches: 0.127945
Train batch 4200
Avg. loss per last 100 batches: 0.127945
Epoch: 4: Step: 4201/4907, loss=0.084400, lr=0.000018
Epoch: 4: Step: 4201/4907, loss=0.084400, lr=0.000018
Train batch 4300
Avg. loss per last 100 batches: 0.128402
Train batch 4300
Avg. loss per last 100 batches: 0.128402
Epoch: 4: Step: 4301/4907, loss=0.015249, lr=0.000018
Epoch: 4: Step: 4301/4907, loss=0.015249, lr=0.000018
Train batch 4400
Avg. loss per last 100 batches: 0.100540
Train batch 4400
Avg. loss per last 100 batches: 0.100540
Epoch: 4: Step: 4401/4907, loss=0.011383, lr=0.000018
Epoch: 4: Step: 4401/4907, loss=0.011383, lr=0.000018
Train batch 4500
Avg. loss per last 100 batches: 0.123554
Train batch 4500
Avg. loss per last 100 batches: 0.123554
Epoch: 4: Step: 4501/4907, loss=0.188090, lr=0.000018
Epoch: 4: Step: 4501/4907, loss=0.188090, lr=0.000018
Train batch 4600
Avg. loss per last 100 batches: 0.125397
Train batch 4600
Avg. loss per last 100 batches: 0.125397
Epoch: 4: Step: 4601/4907, loss=0.305750, lr=0.000018
Epoch: 4: Step: 4601/4907, loss=0.305750, lr=0.000018
Train batch 4700
Avg. loss per last 100 batches: 0.135577
Train batch 4700
Avg. loss per last 100 batches: 0.135577
Epoch: 4: Step: 4701/4907, loss=0.001432, lr=0.000018
Epoch: 4: Step: 4701/4907, loss=0.001432, lr=0.000018
Train batch 4800
Avg. loss per last 100 batches: 0.085784
Train batch 4800
Avg. loss per last 100 batches: 0.085784
Epoch: 4: Step: 4801/4907, loss=0.012584, lr=0.000018
Epoch: 4: Step: 4801/4907, loss=0.012584, lr=0.000018
Train batch 4900
Avg. loss per last 100 batches: 0.095357
Train batch 4900
Avg. loss per last 100 batches: 0.095357
Epoch: 4: Step: 4901/4907, loss=0.000192, lr=0.000018
Epoch: 4: Step: 4901/4907, loss=0.000192, lr=0.000018
Validation: Epoch: 4 Step: 4907/4907
NLL validation ...
Validation: Epoch: 4 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.584026 sec., loss=0.306627 
Eval step: 99 , used_time=28.454802 sec., loss=0.306627 
Eval step: 199 , used_time=57.577928 sec., loss=0.128771 
Eval step: 199 , used_time=57.448717 sec., loss=0.128771 
Eval step: 299 , used_time=86.590071 sec., loss=0.459040 
Eval step: 299 , used_time=86.460883 sec., loss=0.459040 
Eval step: 399 , used_time=115.629482 sec., loss=0.677000 
Eval step: 399 , used_time=115.500278 sec., loss=0.677000 
Eval step: 499 , used_time=144.501823 sec., loss=0.497888 
Eval step: 499 , used_time=144.631071 sec., loss=0.497888 
NLL Validation: loss = 0.515717. correct prediction ratio  5732/6516 ~  0.879681
NLL Validation: loss = 0.515717. correct prediction ratio  5732/6516 ~  0.879681
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.4.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.4.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.848343 sec., loss=0.306627 
Eval step: 99 , used_time=36.264167 sec., loss=0.306627 
Eval step: 199 , used_time=56.854526 sec., loss=0.128771 
Eval step: 199 , used_time=65.270329 sec., loss=0.128771 
Eval step: 299 , used_time=94.264285 sec., loss=0.459040 
Eval step: 299 , used_time=85.848584 sec., loss=0.459040 
Eval step: 399 , used_time=114.952577 sec., loss=0.677000 
Eval step: 399 , used_time=123.368394 sec., loss=0.677000 
Eval step: 499 , used_time=143.947957 sec., loss=0.497888 
Eval step: 499 , used_time=152.363837 sec., loss=0.497888 
NLL Validation: loss = 0.515717. correct prediction ratio  5732/6516 ~  0.879681
NLL Validation: loss = 0.515717. correct prediction ratio  5732/6516 ~  0.879681
Av Loss per epoch=0.107302
epoch total correct predictions=56947
***** Epoch 5 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.4.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.4.4907
Av Loss per epoch=0.107302
epoch total correct predictions=56947
***** Epoch 5 *****
Epoch: 5: Step: 1/4907, loss=0.101990, lr=0.000018
Epoch: 5: Step: 1/4907, loss=0.101990, lr=0.000018
Train batch 100
Avg. loss per last 100 batches: 0.065418
Train batch 100
Avg. loss per last 100 batches: 0.065418
Epoch: 5: Step: 101/4907, loss=0.001004, lr=0.000018
Epoch: 5: Step: 101/4907, loss=0.001004, lr=0.000018
Train batch 200
Avg. loss per last 100 batches: 0.063759
Train batch 200
Avg. loss per last 100 batches: 0.063759
Epoch: 5: Step: 201/4907, loss=0.000130, lr=0.000018
Epoch: 5: Step: 201/4907, loss=0.000130, lr=0.000018
Train batch 300
Avg. loss per last 100 batches: 0.090234
Train batch 300
Avg. loss per last 100 batches: 0.090234
Epoch: 5: Step: 301/4907, loss=0.010588, lr=0.000018
Epoch: 5: Step: 301/4907, loss=0.010588, lr=0.000018
Train batch 400
Avg. loss per last 100 batches: 0.088437
Train batch 400
Avg. loss per last 100 batches: 0.088437
Epoch: 5: Step: 401/4907, loss=0.063138, lr=0.000018
Epoch: 5: Step: 401/4907, loss=0.063138, lr=0.000018
Train batch 500
Avg. loss per last 100 batches: 0.106693
Train batch 500
Avg. loss per last 100 batches: 0.106693
Epoch: 5: Step: 501/4907, loss=0.225402, lr=0.000018
Epoch: 5: Step: 501/4907, loss=0.225402, lr=0.000018
Train batch 600
Avg. loss per last 100 batches: 0.129924
Train batch 600
Avg. loss per last 100 batches: 0.129924
Epoch: 5: Step: 601/4907, loss=0.037995, lr=0.000018
Epoch: 5: Step: 601/4907, loss=0.037995, lr=0.000018
Train batch 700
Avg. loss per last 100 batches: 0.101150
Train batch 700
Avg. loss per last 100 batches: 0.101150
Epoch: 5: Step: 701/4907, loss=0.002110, lr=0.000018
Epoch: 5: Step: 701/4907, loss=0.002110, lr=0.000018
Train batch 800
Avg. loss per last 100 batches: 0.103602
Train batch 800
Avg. loss per last 100 batches: 0.103602
Epoch: 5: Step: 801/4907, loss=0.042831, lr=0.000018
Epoch: 5: Step: 801/4907, loss=0.042831, lr=0.000018
Train batch 900
Train batch 900
Avg. loss per last 100 batches: 0.089868
Avg. loss per last 100 batches: 0.089868
Epoch: 5: Step: 901/4907, loss=0.086378, lr=0.000018
Epoch: 5: Step: 901/4907, loss=0.086378, lr=0.000018
Train batch 1000
Avg. loss per last 100 batches: 0.080897
Train batch 1000
Avg. loss per last 100 batches: 0.080897
Epoch: 5: Step: 1001/4907, loss=0.012027, lr=0.000018
Epoch: 5: Step: 1001/4907, loss=0.012027, lr=0.000018
Train batch 1100
Avg. loss per last 100 batches: 0.072062
Train batch 1100
Avg. loss per last 100 batches: 0.072062
Epoch: 5: Step: 1101/4907, loss=0.189358, lr=0.000017
Epoch: 5: Step: 1101/4907, loss=0.189358, lr=0.000017
Train batch 1200
Avg. loss per last 100 batches: 0.134849
Train batch 1200
Avg. loss per last 100 batches: 0.134849
Epoch: 5: Step: 1201/4907, loss=0.106454, lr=0.000017
Epoch: 5: Step: 1201/4907, loss=0.106454, lr=0.000017
Train batch 1300
Avg. loss per last 100 batches: 0.103551
Train batch 1300
Avg. loss per last 100 batches: 0.103551
Epoch: 5: Step: 1301/4907, loss=0.005159, lr=0.000017
Epoch: 5: Step: 1301/4907, loss=0.005159, lr=0.000017
Train batch 1400
Avg. loss per last 100 batches: 0.088496
Train batch 1400
Avg. loss per last 100 batches: 0.088496
Epoch: 5: Step: 1401/4907, loss=0.014665, lr=0.000017
Epoch: 5: Step: 1401/4907, loss=0.014665, lr=0.000017
Train batch 1500
Avg. loss per last 100 batches: 0.106988
Train batch 1500
Avg. loss per last 100 batches: 0.106988
Epoch: 5: Step: 1501/4907, loss=0.002730, lr=0.000017
Epoch: 5: Step: 1501/4907, loss=0.002730, lr=0.000017
Train batch 1600
Avg. loss per last 100 batches: 0.097487
Train batch 1600
Avg. loss per last 100 batches: 0.097487
Epoch: 5: Step: 1601/4907, loss=0.001664, lr=0.000017
Epoch: 5: Step: 1601/4907, loss=0.001664, lr=0.000017
Train batch 1700
Avg. loss per last 100 batches: 0.085656
Train batch 1700
Avg. loss per last 100 batches: 0.085656
Epoch: 5: Step: 1701/4907, loss=0.001579, lr=0.000017
Epoch: 5: Step: 1701/4907, loss=0.001579, lr=0.000017
Train batch 1800
Avg. loss per last 100 batches: 0.107013
Train batch 1800
Avg. loss per last 100 batches: 0.107013
Epoch: 5: Step: 1801/4907, loss=0.001013, lr=0.000017
Epoch: 5: Step: 1801/4907, loss=0.001013, lr=0.000017
Train batch 1900
Avg. loss per last 100 batches: 0.091308
Train batch 1900
Avg. loss per last 100 batches: 0.091308
Epoch: 5: Step: 1901/4907, loss=0.034151, lr=0.000017
Epoch: 5: Step: 1901/4907, loss=0.034151, lr=0.000017
Train batch 2000
Avg. loss per last 100 batches: 0.090414
Train batch 2000
Avg. loss per last 100 batches: 0.090414
Epoch: 5: Step: 2001/4907, loss=0.090644, lr=0.000017
Epoch: 5: Step: 2001/4907, loss=0.090644, lr=0.000017
Train batch 2100
Avg. loss per last 100 batches: 0.087799
Train batch 2100
Avg. loss per last 100 batches: 0.087799
Epoch: 5: Step: 2101/4907, loss=0.045329, lr=0.000017
Epoch: 5: Step: 2101/4907, loss=0.045329, lr=0.000017
Train batch 2200
Avg. loss per last 100 batches: 0.104915
Train batch 2200
Avg. loss per last 100 batches: 0.104915
Epoch: 5: Step: 2201/4907, loss=0.003649, lr=0.000017
Epoch: 5: Step: 2201/4907, loss=0.003649, lr=0.000017
Train batch 2300
Avg. loss per last 100 batches: 0.068121
Train batch 2300
Avg. loss per last 100 batches: 0.068121
Epoch: 5: Step: 2301/4907, loss=0.001796, lr=0.000017
Epoch: 5: Step: 2301/4907, loss=0.001796, lr=0.000017
Train batch 2400
Avg. loss per last 100 batches: 0.079499
Train batch 2400
Avg. loss per last 100 batches: 0.079499
Epoch: 5: Step: 2401/4907, loss=0.305339, lr=0.000017
Epoch: 5: Step: 2401/4907, loss=0.305339, lr=0.000017
Train batch 2500
Avg. loss per last 100 batches: 0.083937
Train batch 2500
Avg. loss per last 100 batches: 0.083937
Epoch: 5: Step: 2501/4907, loss=0.118258, lr=0.000017
Epoch: 5: Step: 2501/4907, loss=0.118258, lr=0.000017
Train batch 2600
Avg. loss per last 100 batches: 0.066320
Train batch 2600
Avg. loss per last 100 batches: 0.066320
Epoch: 5: Step: 2601/4907, loss=0.001160, lr=0.000017
Epoch: 5: Step: 2601/4907, loss=0.001160, lr=0.000017
Train batch 2700
Avg. loss per last 100 batches: 0.124512
Train batch 2700
Avg. loss per last 100 batches: 0.124512
Epoch: 5: Step: 2701/4907, loss=0.015503, lr=0.000017
Epoch: 5: Step: 2701/4907, loss=0.015503, lr=0.000017
Train batch 2800
Avg. loss per last 100 batches: 0.094713
Train batch 2800
Avg. loss per last 100 batches: 0.094713
Epoch: 5: Step: 2801/4907, loss=0.121204, lr=0.000017
Epoch: 5: Step: 2801/4907, loss=0.121204, lr=0.000017
Train batch 2900
Train batch 2900
Avg. loss per last 100 batches: 0.088459
Avg. loss per last 100 batches: 0.088459
Epoch: 5: Step: 2901/4907, loss=0.266181, lr=0.000017
Epoch: 5: Step: 2901/4907, loss=0.266181, lr=0.000017
Train batch 3000
Avg. loss per last 100 batches: 0.101703
Train batch 3000
Avg. loss per last 100 batches: 0.101703
Epoch: 5: Step: 3001/4907, loss=0.043851, lr=0.000017
Epoch: 5: Step: 3001/4907, loss=0.043851, lr=0.000017
Train batch 3100
Avg. loss per last 100 batches: 0.082841
Train batch 3100
Avg. loss per last 100 batches: 0.082841
Epoch: 5: Step: 3101/4907, loss=0.041893, lr=0.000017
Epoch: 5: Step: 3101/4907, loss=0.041893, lr=0.000017
Train batch 3200
Avg. loss per last 100 batches: 0.146831
Train batch 3200
Avg. loss per last 100 batches: 0.146831
Epoch: 5: Step: 3201/4907, loss=0.039111, lr=0.000017
Epoch: 5: Step: 3201/4907, loss=0.039111, lr=0.000017
Train batch 3300
Avg. loss per last 100 batches: 0.103579
Train batch 3300
Avg. loss per last 100 batches: 0.103579
Epoch: 5: Step: 3301/4907, loss=0.015124, lr=0.000017
Epoch: 5: Step: 3301/4907, loss=0.015124, lr=0.000017
Train batch 3400
Avg. loss per last 100 batches: 0.096956
Train batch 3400
Avg. loss per last 100 batches: 0.096956
Epoch: 5: Step: 3401/4907, loss=0.015886, lr=0.000017
Epoch: 5: Step: 3401/4907, loss=0.015886, lr=0.000017
Train batch 3500
Avg. loss per last 100 batches: 0.102263
Train batch 3500
Avg. loss per last 100 batches: 0.102263
Epoch: 5: Step: 3501/4907, loss=0.010814, lr=0.000017
Epoch: 5: Step: 3501/4907, loss=0.010814, lr=0.000017
Train batch 3600
Avg. loss per last 100 batches: 0.093490
Train batch 3600
Avg. loss per last 100 batches: 0.093490
Epoch: 5: Step: 3601/4907, loss=0.299317, lr=0.000017
Epoch: 5: Step: 3601/4907, loss=0.299317, lr=0.000017
Train batch 3700
Avg. loss per last 100 batches: 0.130583
Train batch 3700
Avg. loss per last 100 batches: 0.130583
Epoch: 5: Step: 3701/4907, loss=0.011145, lr=0.000017
Epoch: 5: Step: 3701/4907, loss=0.011145, lr=0.000017
Train batch 3800
Avg. loss per last 100 batches: 0.101610
Train batch 3800
Avg. loss per last 100 batches: 0.101610
Epoch: 5: Step: 3801/4907, loss=0.023747, lr=0.000017
Epoch: 5: Step: 3801/4907, loss=0.023747, lr=0.000017
Train batch 3900
Avg. loss per last 100 batches: 0.088222
Train batch 3900
Avg. loss per last 100 batches: 0.088222
Epoch: 5: Step: 3901/4907, loss=0.000257, lr=0.000017
Epoch: 5: Step: 3901/4907, loss=0.000257, lr=0.000017
Train batch 4000
Avg. loss per last 100 batches: 0.114950
Train batch 4000
Avg. loss per last 100 batches: 0.114950
Epoch: 5: Step: 4001/4907, loss=0.124926, lr=0.000017
Epoch: 5: Step: 4001/4907, loss=0.124926, lr=0.000017
Train batch 4100
Avg. loss per last 100 batches: 0.083862
Train batch 4100
Avg. loss per last 100 batches: 0.083862
Epoch: 5: Step: 4101/4907, loss=0.001152, lr=0.000017
Epoch: 5: Step: 4101/4907, loss=0.001152, lr=0.000017
Train batch 4200
Avg. loss per last 100 batches: 0.113725
Train batch 4200
Avg. loss per last 100 batches: 0.113725
Epoch: 5: Step: 4201/4907, loss=0.254693, lr=0.000017
Epoch: 5: Step: 4201/4907, loss=0.254693, lr=0.000017
Train batch 4300
Avg. loss per last 100 batches: 0.091935
Train batch 4300
Avg. loss per last 100 batches: 0.091935
Epoch: 5: Step: 4301/4907, loss=0.332722, lr=0.000017
Epoch: 5: Step: 4301/4907, loss=0.332722, lr=0.000017
Train batch 4400
Avg. loss per last 100 batches: 0.109474
Train batch 4400
Avg. loss per last 100 batches: 0.109474
Epoch: 5: Step: 4401/4907, loss=0.040131, lr=0.000017
Epoch: 5: Step: 4401/4907, loss=0.040131, lr=0.000017
Train batch 4500
Avg. loss per last 100 batches: 0.105834
Train batch 4500
Avg. loss per last 100 batches: 0.105834
Epoch: 5: Step: 4501/4907, loss=0.464607, lr=0.000017
Epoch: 5: Step: 4501/4907, loss=0.464607, lr=0.000017
Train batch 4600
Avg. loss per last 100 batches: 0.127676
Train batch 4600
Avg. loss per last 100 batches: 0.127676
Epoch: 5: Step: 4601/4907, loss=0.302285, lr=0.000017
Epoch: 5: Step: 4601/4907, loss=0.302285, lr=0.000017
Train batch 4700
Avg. loss per last 100 batches: 0.107954
Train batch 4700
Avg. loss per last 100 batches: 0.107954
Epoch: 5: Step: 4701/4907, loss=0.038110, lr=0.000017
Epoch: 5: Step: 4701/4907, loss=0.038110, lr=0.000017
Train batch 4800
Avg. loss per last 100 batches: 0.085489
Train batch 4800
Avg. loss per last 100 batches: 0.085489
Epoch: 5: Step: 4801/4907, loss=0.045731, lr=0.000017
Epoch: 5: Step: 4801/4907, loss=0.045731, lr=0.000017
Train batch 4900
Avg. loss per last 100 batches: 0.096616
Train batch 4900
Avg. loss per last 100 batches: 0.096616
Epoch: 5: Step: 4901/4907, loss=0.003633, lr=0.000017
Epoch: 5: Step: 4901/4907, loss=0.003633, lr=0.000017
Validation: Epoch: 5 Step: 4907/4907
NLL validation ...
Validation: Epoch: 5 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=29.714611 sec., loss=0.010149 
Eval step: 99 , used_time=28.326646 sec., loss=0.010149 
Eval step: 199 , used_time=57.394906 sec., loss=0.786330 
Eval step: 199 , used_time=58.782848 sec., loss=0.786330 
Eval step: 299 , used_time=87.867280 sec., loss=1.009255 
Eval step: 299 , used_time=86.479336 sec., loss=1.009255 
Eval step: 399 , used_time=115.560994 sec., loss=0.176336 
Eval step: 399 , used_time=116.948940 sec., loss=0.176336 
Eval step: 499 , used_time=146.061239 sec., loss=0.323168 
Eval step: 499 , used_time=144.673275 sec., loss=0.323168 
NLL Validation: loss = 0.560177. correct prediction ratio  5739/6516 ~  0.880755
NLL Validation: loss = 0.560177. correct prediction ratio  5739/6516 ~  0.880755
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.5.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.5.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.960869 sec., loss=0.010149 
Eval step: 99 , used_time=36.223844 sec., loss=0.010149 
Eval step: 199 , used_time=65.289461 sec., loss=0.786330 
Eval step: 199 , used_time=57.026514 sec., loss=0.786330 
Eval step: 299 , used_time=86.106433 sec., loss=1.009255 
Eval step: 299 , used_time=94.369379 sec., loss=1.009255 
Eval step: 399 , used_time=115.200358 sec., loss=0.176336 
Eval step: 399 , used_time=123.463305 sec., loss=0.176336 
Eval step: 499 , used_time=152.524302 sec., loss=0.323168 
Eval step: 499 , used_time=144.261366 sec., loss=0.323168 
NLL Validation: loss = 0.560177. correct prediction ratio  5739/6516 ~  0.880755
NLL Validation: loss = 0.560177. correct prediction ratio  5739/6516 ~  0.880755
Av Loss per epoch=0.097641
epoch total correct predictions=57142
***** Epoch 6 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.5.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.5.4907
Av Loss per epoch=0.097641
epoch total correct predictions=57142
***** Epoch 6 *****
Epoch: 6: Step: 1/4907, loss=0.378725, lr=0.000017
Epoch: 6: Step: 1/4907, loss=0.378725, lr=0.000017
Train batch 100
Avg. loss per last 100 batches: 0.096737
Train batch 100
Avg. loss per last 100 batches: 0.096737
Epoch: 6: Step: 101/4907, loss=0.000051, lr=0.000017
Epoch: 6: Step: 101/4907, loss=0.000051, lr=0.000017
Train batch 200
Avg. loss per last 100 batches: 0.101894
Train batch 200
Avg. loss per last 100 batches: 0.101894
Epoch: 6: Step: 201/4907, loss=0.277093, lr=0.000017
Epoch: 6: Step: 201/4907, loss=0.277093, lr=0.000017
Train batch 300
Avg. loss per last 100 batches: 0.105500
Train batch 300
Avg. loss per last 100 batches: 0.105500
Epoch: 6: Step: 301/4907, loss=0.146528, lr=0.000017
Epoch: 6: Step: 301/4907, loss=0.146528, lr=0.000017
Train batch 400
Avg. loss per last 100 batches: 0.073031
Train batch 400
Avg. loss per last 100 batches: 0.073031
Epoch: 6: Step: 401/4907, loss=0.000348, lr=0.000017
Epoch: 6: Step: 401/4907, loss=0.000348, lr=0.000017
Train batch 500
Avg. loss per last 100 batches: 0.100630
Train batch 500
Avg. loss per last 100 batches: 0.100630
Epoch: 6: Step: 501/4907, loss=0.000050, lr=0.000017
Epoch: 6: Step: 501/4907, loss=0.000050, lr=0.000017
Train batch 600
Avg. loss per last 100 batches: 0.090587
Train batch 600
Avg. loss per last 100 batches: 0.090587
Epoch: 6: Step: 601/4907, loss=0.012344, lr=0.000017
Epoch: 6: Step: 601/4907, loss=0.012344, lr=0.000017
Train batch 700
Avg. loss per last 100 batches: 0.099817
Train batch 700
Avg. loss per last 100 batches: 0.099817
Epoch: 6: Step: 701/4907, loss=0.001838, lr=0.000017
Epoch: 6: Step: 701/4907, loss=0.001838, lr=0.000017
Train batch 800
Avg. loss per last 100 batches: 0.082657
Train batch 800
Avg. loss per last 100 batches: 0.082657
Epoch: 6: Step: 801/4907, loss=0.000067, lr=0.000017
Epoch: 6: Step: 801/4907, loss=0.000067, lr=0.000017
Train batch 900
Train batch 900
Avg. loss per last 100 batches: 0.086710
Avg. loss per last 100 batches: 0.086710
Epoch: 6: Step: 901/4907, loss=0.451775, lr=0.000017
Epoch: 6: Step: 901/4907, loss=0.451775, lr=0.000017
Train batch 1000
Avg. loss per last 100 batches: 0.068726
Train batch 1000
Avg. loss per last 100 batches: 0.068726
Epoch: 6: Step: 1001/4907, loss=0.004435, lr=0.000017
Epoch: 6: Step: 1001/4907, loss=0.004435, lr=0.000017
Train batch 1100
Avg. loss per last 100 batches: 0.075515
Train batch 1100
Avg. loss per last 100 batches: 0.075515
Epoch: 6: Step: 1101/4907, loss=0.000037, lr=0.000017
Epoch: 6: Step: 1101/4907, loss=0.000037, lr=0.000017
Train batch 1200
Avg. loss per last 100 batches: 0.060234
Train batch 1200
Avg. loss per last 100 batches: 0.060234
Epoch: 6: Step: 1201/4907, loss=0.000031, lr=0.000017
Epoch: 6: Step: 1201/4907, loss=0.000031, lr=0.000017
Train batch 1300
Avg. loss per last 100 batches: 0.097634
Train batch 1300
Avg. loss per last 100 batches: 0.097634
Epoch: 6: Step: 1301/4907, loss=0.000418, lr=0.000017
Epoch: 6: Step: 1301/4907, loss=0.000418, lr=0.000017
Train batch 1400
Avg. loss per last 100 batches: 0.081273
Train batch 1400
Avg. loss per last 100 batches: 0.081273
Epoch: 6: Step: 1401/4907, loss=0.000054, lr=0.000017
Epoch: 6: Step: 1401/4907, loss=0.000054, lr=0.000017
Train batch 1500
Avg. loss per last 100 batches: 0.094356
Train batch 1500
Avg. loss per last 100 batches: 0.094356
Epoch: 6: Step: 1501/4907, loss=0.368932, lr=0.000017
Epoch: 6: Step: 1501/4907, loss=0.368932, lr=0.000017
Train batch 1600
Avg. loss per last 100 batches: 0.072749
Train batch 1600
Avg. loss per last 100 batches: 0.072749
Epoch: 6: Step: 1601/4907, loss=0.001388, lr=0.000017
Epoch: 6: Step: 1601/4907, loss=0.001388, lr=0.000017
Train batch 1700
Avg. loss per last 100 batches: 0.094738
Train batch 1700
Avg. loss per last 100 batches: 0.094738
Epoch: 6: Step: 1701/4907, loss=0.009944, lr=0.000017
Epoch: 6: Step: 1701/4907, loss=0.009944, lr=0.000017
Train batch 1800
Avg. loss per last 100 batches: 0.092816
Train batch 1800
Avg. loss per last 100 batches: 0.092816
Epoch: 6: Step: 1801/4907, loss=0.097529, lr=0.000017
Epoch: 6: Step: 1801/4907, loss=0.097529, lr=0.000017
Train batch 1900
Avg. loss per last 100 batches: 0.081563
Train batch 1900
Avg. loss per last 100 batches: 0.081563
Epoch: 6: Step: 1901/4907, loss=0.001177, lr=0.000017
Epoch: 6: Step: 1901/4907, loss=0.001177, lr=0.000017
Train batch 2000
Avg. loss per last 100 batches: 0.068301
Train batch 2000
Avg. loss per last 100 batches: 0.068301
Epoch: 6: Step: 2001/4907, loss=0.585829, lr=0.000017
Epoch: 6: Step: 2001/4907, loss=0.585829, lr=0.000017
Train batch 2100
Avg. loss per last 100 batches: 0.112286
Train batch 2100
Avg. loss per last 100 batches: 0.112286
Epoch: 6: Step: 2101/4907, loss=0.000595, lr=0.000017
Epoch: 6: Step: 2101/4907, loss=0.000595, lr=0.000017
Train batch 2200
Avg. loss per last 100 batches: 0.134307
Train batch 2200
Avg. loss per last 100 batches: 0.134307
Epoch: 6: Step: 2201/4907, loss=0.000854, lr=0.000017
Epoch: 6: Step: 2201/4907, loss=0.000854, lr=0.000017
Train batch 2300
Avg. loss per last 100 batches: 0.085777
Train batch 2300
Avg. loss per last 100 batches: 0.085777
Epoch: 6: Step: 2301/4907, loss=0.032577, lr=0.000017
Epoch: 6: Step: 2301/4907, loss=0.032577, lr=0.000017
Train batch 2400
Avg. loss per last 100 batches: 0.071501
Train batch 2400
Avg. loss per last 100 batches: 0.071501
Epoch: 6: Step: 2401/4907, loss=0.002471, lr=0.000017
Epoch: 6: Step: 2401/4907, loss=0.002471, lr=0.000017
Train batch 2500
Avg. loss per last 100 batches: 0.077398
Train batch 2500
Avg. loss per last 100 batches: 0.077398
Epoch: 6: Step: 2501/4907, loss=0.000190, lr=0.000017
Epoch: 6: Step: 2501/4907, loss=0.000190, lr=0.000017
Train batch 2600
Train batch 2600
Avg. loss per last 100 batches: 0.072408
Avg. loss per last 100 batches: 0.072408
Epoch: 6: Step: 2601/4907, loss=0.000737, lr=0.000017
Epoch: 6: Step: 2601/4907, loss=0.000737, lr=0.000017
Train batch 2700
Avg. loss per last 100 batches: 0.090639
Train batch 2700
Avg. loss per last 100 batches: 0.090639
Epoch: 6: Step: 2701/4907, loss=0.000766, lr=0.000017
Epoch: 6: Step: 2701/4907, loss=0.000766, lr=0.000017
Train batch 2800
Avg. loss per last 100 batches: 0.099195
Train batch 2800
Avg. loss per last 100 batches: 0.099195
Epoch: 6: Step: 2801/4907, loss=0.012895, lr=0.000017
Epoch: 6: Step: 2801/4907, loss=0.012895, lr=0.000017
Train batch 2900
Avg. loss per last 100 batches: 0.097264
Train batch 2900
Avg. loss per last 100 batches: 0.097264
Epoch: 6: Step: 2901/4907, loss=0.272441, lr=0.000017
Epoch: 6: Step: 2901/4907, loss=0.272441, lr=0.000017
Train batch 3000
Avg. loss per last 100 batches: 0.100313
Train batch 3000
Avg. loss per last 100 batches: 0.100313
Epoch: 6: Step: 3001/4907, loss=0.133421, lr=0.000017
Epoch: 6: Step: 3001/4907, loss=0.133421, lr=0.000017
Train batch 3100
Avg. loss per last 100 batches: 0.102799
Train batch 3100
Avg. loss per last 100 batches: 0.102799
Epoch: 6: Step: 3101/4907, loss=0.000444, lr=0.000017
Epoch: 6: Step: 3101/4907, loss=0.000444, lr=0.000017
Train batch 3200
Avg. loss per last 100 batches: 0.070003
Train batch 3200
Avg. loss per last 100 batches: 0.070003
Epoch: 6: Step: 3201/4907, loss=0.000113, lr=0.000017
Epoch: 6: Step: 3201/4907, loss=0.000113, lr=0.000017
Train batch 3300
Avg. loss per last 100 batches: 0.079333
Train batch 3300
Avg. loss per last 100 batches: 0.079333
Epoch: 6: Step: 3301/4907, loss=0.002829, lr=0.000017
Epoch: 6: Step: 3301/4907, loss=0.002829, lr=0.000017
Train batch 3400
Avg. loss per last 100 batches: 0.077684
Train batch 3400
Avg. loss per last 100 batches: 0.077684
Epoch: 6: Step: 3401/4907, loss=0.396310, lr=0.000017
Epoch: 6: Step: 3401/4907, loss=0.396310, lr=0.000017
Train batch 3500
Avg. loss per last 100 batches: 0.086425
Train batch 3500
Avg. loss per last 100 batches: 0.086425
Epoch: 6: Step: 3501/4907, loss=0.010679, lr=0.000017
Epoch: 6: Step: 3501/4907, loss=0.010679, lr=0.000017
Train batch 3600
Avg. loss per last 100 batches: 0.092924
Train batch 3600
Avg. loss per last 100 batches: 0.092924
Epoch: 6: Step: 3601/4907, loss=0.190662, lr=0.000017
Epoch: 6: Step: 3601/4907, loss=0.190662, lr=0.000017
Train batch 3700
Avg. loss per last 100 batches: 0.069668
Train batch 3700
Avg. loss per last 100 batches: 0.069668
Epoch: 6: Step: 3701/4907, loss=0.000005, lr=0.000017
Epoch: 6: Step: 3701/4907, loss=0.000005, lr=0.000017
Train batch 3800
Avg. loss per last 100 batches: 0.058090
Train batch 3800
Avg. loss per last 100 batches: 0.058090
Epoch: 6: Step: 3801/4907, loss=0.000964, lr=0.000017
Epoch: 6: Step: 3801/4907, loss=0.000964, lr=0.000017
Train batch 3900
Avg. loss per last 100 batches: 0.087477
Train batch 3900
Avg. loss per last 100 batches: 0.087477
Epoch: 6: Step: 3901/4907, loss=0.002059, lr=0.000017
Epoch: 6: Step: 3901/4907, loss=0.002059, lr=0.000017
Train batch 4000
Avg. loss per last 100 batches: 0.098058
Train batch 4000
Avg. loss per last 100 batches: 0.098058
Epoch: 6: Step: 4001/4907, loss=0.000003, lr=0.000017
Epoch: 6: Step: 4001/4907, loss=0.000003, lr=0.000017
Train batch 4100
Avg. loss per last 100 batches: 0.076271
Train batch 4100
Avg. loss per last 100 batches: 0.076271
Epoch: 6: Step: 4101/4907, loss=0.010690, lr=0.000017
Epoch: 6: Step: 4101/4907, loss=0.010690, lr=0.000017
Train batch 4200
Avg. loss per last 100 batches: 0.094515
Train batch 4200
Avg. loss per last 100 batches: 0.094515
Epoch: 6: Step: 4201/4907, loss=0.003195, lr=0.000017
Epoch: 6: Step: 4201/4907, loss=0.003195, lr=0.000017
Train batch 4300
Avg. loss per last 100 batches: 0.091310
Train batch 4300
Avg. loss per last 100 batches: 0.091310
Epoch: 6: Step: 4301/4907, loss=0.007376, lr=0.000017
Epoch: 6: Step: 4301/4907, loss=0.007376, lr=0.000017
Train batch 4400
Avg. loss per last 100 batches: 0.074341
Train batch 4400
Avg. loss per last 100 batches: 0.074341
Epoch: 6: Step: 4401/4907, loss=0.000826, lr=0.000017
Epoch: 6: Step: 4401/4907, loss=0.000826, lr=0.000017
Train batch 4500
Avg. loss per last 100 batches: 0.082182
Train batch 4500
Avg. loss per last 100 batches: 0.082182
Epoch: 6: Step: 4501/4907, loss=0.000173, lr=0.000017
Epoch: 6: Step: 4501/4907, loss=0.000173, lr=0.000017
Train batch 4600
Avg. loss per last 100 batches: 0.066895
Train batch 4600
Avg. loss per last 100 batches: 0.066895
Epoch: 6: Step: 4601/4907, loss=0.000008, lr=0.000017
Epoch: 6: Step: 4601/4907, loss=0.000008, lr=0.000017
Train batch 4700
Avg. loss per last 100 batches: 0.101400
Train batch 4700
Avg. loss per last 100 batches: 0.101400
Epoch: 6: Step: 4701/4907, loss=0.000179, lr=0.000017
Epoch: 6: Step: 4701/4907, loss=0.000179, lr=0.000017
Train batch 4800
Avg. loss per last 100 batches: 0.092067
Train batch 4800
Avg. loss per last 100 batches: 0.092067
Epoch: 6: Step: 4801/4907, loss=0.102185, lr=0.000017
Epoch: 6: Step: 4801/4907, loss=0.102185, lr=0.000017
Train batch 4900
Avg. loss per last 100 batches: 0.129384
Train batch 4900
Avg. loss per last 100 batches: 0.129384
Epoch: 6: Step: 4901/4907, loss=0.002799, lr=0.000017
Epoch: 6: Step: 4901/4907, loss=0.002799, lr=0.000017
Validation: Epoch: 6 Step: 4907/4907
NLL validation ...
Validation: Epoch: 6 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=29.344657 sec., loss=0.016341 
Eval step: 99 , used_time=28.310895 sec., loss=0.016341 
Eval step: 199 , used_time=58.375244 sec., loss=0.184667 
Eval step: 199 , used_time=57.341444 sec., loss=0.184667 
Eval step: 299 , used_time=86.379302 sec., loss=0.769093 
Eval step: 299 , used_time=87.413107 sec., loss=0.769093 
Eval step: 399 , used_time=116.474859 sec., loss=0.430086 
Eval step: 399 , used_time=115.441031 sec., loss=0.430086 
Eval step: 499 , used_time=144.482489 sec., loss=0.480656 
Eval step: 499 , used_time=145.516335 sec., loss=0.480656 
NLL Validation: loss = 0.489618. correct prediction ratio  5847/6516 ~  0.897330
NLL Validation: loss = 0.489618. correct prediction ratio  5847/6516 ~  0.897330
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.6.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.6.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.899105 sec., loss=0.016341 
Eval step: 99 , used_time=33.901947 sec., loss=0.016341 
Eval step: 199 , used_time=62.931384 sec., loss=0.184667 
Eval step: 199 , used_time=56.928526 sec., loss=0.184667 
Eval step: 299 , used_time=85.980535 sec., loss=0.769093 
Eval step: 299 , used_time=91.983396 sec., loss=0.769093 
Eval step: 399 , used_time=121.032466 sec., loss=0.430086 
Eval step: 399 , used_time=115.029665 sec., loss=0.430086 
Eval step: 499 , used_time=144.116852 sec., loss=0.480656 
Eval step: 499 , used_time=150.119714 sec., loss=0.480656 
NLL Validation: loss = 0.489618. correct prediction ratio  5847/6516 ~  0.897330
NLL Validation: loss = 0.489618. correct prediction ratio  5847/6516 ~  0.897330
Av Loss per epoch=0.087754
epoch total correct predictions=57317
***** Epoch 7 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.6.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.6.4907
Av Loss per epoch=0.087754
epoch total correct predictions=57317
***** Epoch 7 *****
Epoch: 7: Step: 1/4907, loss=0.000480, lr=0.000017
Epoch: 7: Step: 1/4907, loss=0.000480, lr=0.000017
Train batch 100
Avg. loss per last 100 batches: 0.065679
Train batch 100
Avg. loss per last 100 batches: 0.065679
Epoch: 7: Step: 101/4907, loss=0.000625, lr=0.000017
Epoch: 7: Step: 101/4907, loss=0.000625, lr=0.000017
Train batch 200
Avg. loss per last 100 batches: 0.114472
Train batch 200
Avg. loss per last 100 batches: 0.114472
Epoch: 7: Step: 201/4907, loss=0.000214, lr=0.000017
Epoch: 7: Step: 201/4907, loss=0.000214, lr=0.000017
Train batch 300
Avg. loss per last 100 batches: 0.065210
Train batch 300
Avg. loss per last 100 batches: 0.065210
Epoch: 7: Step: 301/4907, loss=0.000094, lr=0.000017
Epoch: 7: Step: 301/4907, loss=0.000094, lr=0.000017
Train batch 400
Avg. loss per last 100 batches: 0.080226
Train batch 400
Avg. loss per last 100 batches: 0.080226
Epoch: 7: Step: 401/4907, loss=0.113557, lr=0.000017
Epoch: 7: Step: 401/4907, loss=0.113557, lr=0.000017
Train batch 500
Avg. loss per last 100 batches: 0.063933
Train batch 500
Avg. loss per last 100 batches: 0.063933
Epoch: 7: Step: 501/4907, loss=0.000842, lr=0.000017
Epoch: 7: Step: 501/4907, loss=0.000842, lr=0.000017
Train batch 600
Avg. loss per last 100 batches: 0.096065
Train batch 600
Avg. loss per last 100 batches: 0.096065
Epoch: 7: Step: 601/4907, loss=0.022144, lr=0.000017
Epoch: 7: Step: 601/4907, loss=0.022144, lr=0.000017
Train batch 700
Avg. loss per last 100 batches: 0.086925
Train batch 700
Avg. loss per last 100 batches: 0.086925
Epoch: 7: Step: 701/4907, loss=0.000666, lr=0.000017
Epoch: 7: Step: 701/4907, loss=0.000666, lr=0.000017
Train batch 800
Avg. loss per last 100 batches: 0.065456
Train batch 800
Avg. loss per last 100 batches: 0.065456
Epoch: 7: Step: 801/4907, loss=0.002340, lr=0.000017
Epoch: 7: Step: 801/4907, loss=0.002340, lr=0.000017
Train batch 900
Avg. loss per last 100 batches: 0.076763
Train batch 900
Avg. loss per last 100 batches: 0.076763
Epoch: 7: Step: 901/4907, loss=0.000115, lr=0.000017
Epoch: 7: Step: 901/4907, loss=0.000115, lr=0.000017
Train batch 1000
Avg. loss per last 100 batches: 0.078586
Train batch 1000
Avg. loss per last 100 batches: 0.078586
Epoch: 7: Step: 1001/4907, loss=0.000090, lr=0.000017
Epoch: 7: Step: 1001/4907, loss=0.000090, lr=0.000017
Train batch 1100
Avg. loss per last 100 batches: 0.080666
Train batch 1100
Avg. loss per last 100 batches: 0.080666
Epoch: 7: Step: 1101/4907, loss=0.025755, lr=0.000016
Epoch: 7: Step: 1101/4907, loss=0.025755, lr=0.000016
Train batch 1200
Avg. loss per last 100 batches: 0.071405
Train batch 1200
Avg. loss per last 100 batches: 0.071405
Epoch: 7: Step: 1201/4907, loss=0.007574, lr=0.000016
Epoch: 7: Step: 1201/4907, loss=0.007574, lr=0.000016
Train batch 1300
Avg. loss per last 100 batches: 0.106337
Train batch 1300
Avg. loss per last 100 batches: 0.106337
Epoch: 7: Step: 1301/4907, loss=0.523521, lr=0.000016
Epoch: 7: Step: 1301/4907, loss=0.523521, lr=0.000016
Train batch 1400
Avg. loss per last 100 batches: 0.097387
Train batch 1400
Avg. loss per last 100 batches: 0.097387
Epoch: 7: Step: 1401/4907, loss=0.073359, lr=0.000016
Epoch: 7: Step: 1401/4907, loss=0.073359, lr=0.000016
Train batch 1500
Avg. loss per last 100 batches: 0.075762
Train batch 1500
Avg. loss per last 100 batches: 0.075762
Epoch: 7: Step: 1501/4907, loss=0.011146, lr=0.000016
Epoch: 7: Step: 1501/4907, loss=0.011146, lr=0.000016
Train batch 1600
Avg. loss per last 100 batches: 0.080228
Train batch 1600
Avg. loss per last 100 batches: 0.080228
Epoch: 7: Step: 1601/4907, loss=0.000459, lr=0.000016
Epoch: 7: Step: 1601/4907, loss=0.000459, lr=0.000016
Train batch 1700
Train batch 1700
Avg. loss per last 100 batches: 0.067484
Avg. loss per last 100 batches: 0.067484
Epoch: 7: Step: 1701/4907, loss=0.000870, lr=0.000016
Epoch: 7: Step: 1701/4907, loss=0.000870, lr=0.000016
Train batch 1800
Train batch 1800
Avg. loss per last 100 batches: 0.088977
Avg. loss per last 100 batches: 0.088977
Epoch: 7: Step: 1801/4907, loss=0.000340, lr=0.000016
Epoch: 7: Step: 1801/4907, loss=0.000340, lr=0.000016
Train batch 1900
Avg. loss per last 100 batches: 0.098549
Train batch 1900
Avg. loss per last 100 batches: 0.098549
Epoch: 7: Step: 1901/4907, loss=0.000484, lr=0.000016
Epoch: 7: Step: 1901/4907, loss=0.000484, lr=0.000016
Train batch 2000
Avg. loss per last 100 batches: 0.111791
Train batch 2000
Avg. loss per last 100 batches: 0.111791
Epoch: 7: Step: 2001/4907, loss=0.048905, lr=0.000016
Epoch: 7: Step: 2001/4907, loss=0.048905, lr=0.000016
Train batch 2100
Avg. loss per last 100 batches: 0.081453
Train batch 2100
Avg. loss per last 100 batches: 0.081453
Epoch: 7: Step: 2101/4907, loss=0.001518, lr=0.000016
Epoch: 7: Step: 2101/4907, loss=0.001518, lr=0.000016
Train batch 2200
Avg. loss per last 100 batches: 0.095524
Train batch 2200
Avg. loss per last 100 batches: 0.095524
Epoch: 7: Step: 2201/4907, loss=0.000670, lr=0.000016
Epoch: 7: Step: 2201/4907, loss=0.000670, lr=0.000016
Train batch 2300
Avg. loss per last 100 batches: 0.085113
Train batch 2300
Avg. loss per last 100 batches: 0.085113
Epoch: 7: Step: 2301/4907, loss=0.002080, lr=0.000016
Epoch: 7: Step: 2301/4907, loss=0.002080, lr=0.000016
Train batch 2400
Avg. loss per last 100 batches: 0.093796
Train batch 2400
Avg. loss per last 100 batches: 0.093796
Epoch: 7: Step: 2401/4907, loss=0.043784, lr=0.000016
Epoch: 7: Step: 2401/4907, loss=0.043784, lr=0.000016
Train batch 2500
Avg. loss per last 100 batches: 0.096450
Train batch 2500
Avg. loss per last 100 batches: 0.096450
Epoch: 7: Step: 2501/4907, loss=0.003208, lr=0.000016
Epoch: 7: Step: 2501/4907, loss=0.003208, lr=0.000016
Train batch 2600
Avg. loss per last 100 batches: 0.076486
Train batch 2600
Avg. loss per last 100 batches: 0.076486
Epoch: 7: Step: 2601/4907, loss=0.007242, lr=0.000016
Epoch: 7: Step: 2601/4907, loss=0.007242, lr=0.000016
Train batch 2700
Avg. loss per last 100 batches: 0.078602
Train batch 2700
Avg. loss per last 100 batches: 0.078602
Epoch: 7: Step: 2701/4907, loss=0.000387, lr=0.000016
Epoch: 7: Step: 2701/4907, loss=0.000387, lr=0.000016
Train batch 2800
Avg. loss per last 100 batches: 0.110889
Train batch 2800
Avg. loss per last 100 batches: 0.110889
Epoch: 7: Step: 2801/4907, loss=0.009572, lr=0.000016
Epoch: 7: Step: 2801/4907, loss=0.009572, lr=0.000016
Train batch 2900
Avg. loss per last 100 batches: 0.108971
Train batch 2900
Avg. loss per last 100 batches: 0.108971
Epoch: 7: Step: 2901/4907, loss=0.000282, lr=0.000016
Epoch: 7: Step: 2901/4907, loss=0.000282, lr=0.000016
Train batch 3000
Avg. loss per last 100 batches: 0.064337
Train batch 3000
Avg. loss per last 100 batches: 0.064337
Epoch: 7: Step: 3001/4907, loss=0.000014, lr=0.000016
Epoch: 7: Step: 3001/4907, loss=0.000014, lr=0.000016
Train batch 3100
Avg. loss per last 100 batches: 0.082370
Train batch 3100
Avg. loss per last 100 batches: 0.082370
Epoch: 7: Step: 3101/4907, loss=0.028245, lr=0.000016
Epoch: 7: Step: 3101/4907, loss=0.028245, lr=0.000016
Train batch 3200
Avg. loss per last 100 batches: 0.082281
Train batch 3200
Avg. loss per last 100 batches: 0.082281
Epoch: 7: Step: 3201/4907, loss=0.015354, lr=0.000016
Epoch: 7: Step: 3201/4907, loss=0.015354, lr=0.000016
Train batch 3300
Avg. loss per last 100 batches: 0.106675
Train batch 3300
Avg. loss per last 100 batches: 0.106675
Epoch: 7: Step: 3301/4907, loss=0.003004, lr=0.000016
Epoch: 7: Step: 3301/4907, loss=0.003004, lr=0.000016
Train batch 3400
Avg. loss per last 100 batches: 0.075201
Train batch 3400
Avg. loss per last 100 batches: 0.075201
Epoch: 7: Step: 3401/4907, loss=0.094503, lr=0.000016
Epoch: 7: Step: 3401/4907, loss=0.094503, lr=0.000016
Train batch 3500
Avg. loss per last 100 batches: 0.083243
Train batch 3500
Avg. loss per last 100 batches: 0.083243
Epoch: 7: Step: 3501/4907, loss=0.000035, lr=0.000016
Epoch: 7: Step: 3501/4907, loss=0.000035, lr=0.000016
Train batch 3600
Avg. loss per last 100 batches: 0.063061
Train batch 3600
Avg. loss per last 100 batches: 0.063061
Epoch: 7: Step: 3601/4907, loss=0.000185, lr=0.000016
Epoch: 7: Step: 3601/4907, loss=0.000185, lr=0.000016
Train batch 3700
Avg. loss per last 100 batches: 0.094305
Train batch 3700
Avg. loss per last 100 batches: 0.094305
Epoch: 7: Step: 3701/4907, loss=0.000308, lr=0.000016
Epoch: 7: Step: 3701/4907, loss=0.000308, lr=0.000016
Train batch 3800
Avg. loss per last 100 batches: 0.089041
Train batch 3800
Avg. loss per last 100 batches: 0.089041
Epoch: 7: Step: 3801/4907, loss=0.663883, lr=0.000016
Epoch: 7: Step: 3801/4907, loss=0.663883, lr=0.000016
Train batch 3900
Avg. loss per last 100 batches: 0.098057
Train batch 3900
Avg. loss per last 100 batches: 0.098057
Epoch: 7: Step: 3901/4907, loss=0.000014, lr=0.000016
Epoch: 7: Step: 3901/4907, loss=0.000014, lr=0.000016
Train batch 4000
Avg. loss per last 100 batches: 0.081666
Train batch 4000
Avg. loss per last 100 batches: 0.081666
Epoch: 7: Step: 4001/4907, loss=0.037900, lr=0.000016
Epoch: 7: Step: 4001/4907, loss=0.037900, lr=0.000016
Train batch 4100
Avg. loss per last 100 batches: 0.107468
Train batch 4100
Avg. loss per last 100 batches: 0.107468
Epoch: 7: Step: 4101/4907, loss=0.000017, lr=0.000016
Epoch: 7: Step: 4101/4907, loss=0.000017, lr=0.000016
Train batch 4200
Avg. loss per last 100 batches: 0.061198
Train batch 4200
Avg. loss per last 100 batches: 0.061198
Epoch: 7: Step: 4201/4907, loss=0.000002, lr=0.000016
Epoch: 7: Step: 4201/4907, loss=0.000002, lr=0.000016
Train batch 4300
Avg. loss per last 100 batches: 0.076402
Train batch 4300
Avg. loss per last 100 batches: 0.076402
Epoch: 7: Step: 4301/4907, loss=0.033236, lr=0.000016
Epoch: 7: Step: 4301/4907, loss=0.033236, lr=0.000016
Train batch 4400
Avg. loss per last 100 batches: 0.072593
Train batch 4400
Avg. loss per last 100 batches: 0.072593
Epoch: 7: Step: 4401/4907, loss=0.106432, lr=0.000016
Epoch: 7: Step: 4401/4907, loss=0.106432, lr=0.000016
Train batch 4500
Avg. loss per last 100 batches: 0.061877
Train batch 4500
Avg. loss per last 100 batches: 0.061877
Epoch: 7: Step: 4501/4907, loss=0.012983, lr=0.000016
Epoch: 7: Step: 4501/4907, loss=0.012983, lr=0.000016
Train batch 4600
Avg. loss per last 100 batches: 0.074350
Train batch 4600
Avg. loss per last 100 batches: 0.074350
Epoch: 7: Step: 4601/4907, loss=0.056296, lr=0.000016
Epoch: 7: Step: 4601/4907, loss=0.056296, lr=0.000016
Train batch 4700
Avg. loss per last 100 batches: 0.078976
Train batch 4700
Avg. loss per last 100 batches: 0.078976
Epoch: 7: Step: 4701/4907, loss=0.230718, lr=0.000016
Epoch: 7: Step: 4701/4907, loss=0.230718, lr=0.000016
Train batch 4800
Avg. loss per last 100 batches: 0.074102
Train batch 4800
Avg. loss per last 100 batches: 0.074102
Epoch: 7: Step: 4801/4907, loss=0.000803, lr=0.000016
Epoch: 7: Step: 4801/4907, loss=0.000803, lr=0.000016
Train batch 4900
Avg. loss per last 100 batches: 0.086608
Train batch 4900
Avg. loss per last 100 batches: 0.086608
Epoch: 7: Step: 4901/4907, loss=0.093994, lr=0.000016
Epoch: 7: Step: 4901/4907, loss=0.093994, lr=0.000016
Validation: Epoch: 7 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 7 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.579347 sec., loss=0.504641 
Eval step: 99 , used_time=28.546266 sec., loss=0.504641 
Eval step: 199 , used_time=57.656549 sec., loss=0.280401 
Eval step: 199 , used_time=57.623450 sec., loss=0.280401 
Eval step: 299 , used_time=86.705496 sec., loss=0.915160 
Eval step: 299 , used_time=86.672453 sec., loss=0.915160 
Eval step: 399 , used_time=115.778207 sec., loss=1.006705 
Eval step: 399 , used_time=115.745107 sec., loss=1.006705 
Eval step: 499 , used_time=144.824808 sec., loss=0.148476 
Eval step: 499 , used_time=144.791716 sec., loss=0.148476 
NLL Validation: loss = 0.555214. correct prediction ratio  5765/6516 ~  0.884745
NLL Validation: loss = 0.555214. correct prediction ratio  5765/6516 ~  0.884745
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.7.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.7.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=27.675543 sec., loss=0.504641 
Eval step: 99 , used_time=38.213223 sec., loss=0.504641 
Eval step: 199 , used_time=67.261878 sec., loss=0.280401 
Eval step: 199 , used_time=56.724257 sec., loss=0.280401 
Eval step: 299 , used_time=96.308991 sec., loss=0.915160 
Eval step: 299 , used_time=85.771489 sec., loss=0.915160 
Eval step: 399 , used_time=114.843019 sec., loss=1.006705 
Eval step: 399 , used_time=125.380852 sec., loss=1.006705 
Eval step: 499 , used_time=154.545474 sec., loss=0.148476 
Eval step: 499 , used_time=144.007942 sec., loss=0.148476 
NLL Validation: loss = 0.555214. correct prediction ratio  5765/6516 ~  0.884745
NLL Validation: loss = 0.555214. correct prediction ratio  5765/6516 ~  0.884745
Av Loss per epoch=0.083858
epoch total correct predictions=57437
***** Epoch 8 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.7.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.7.4907
Av Loss per epoch=0.083858
epoch total correct predictions=57437
***** Epoch 8 *****
Epoch: 8: Step: 1/4907, loss=0.070257, lr=0.000016
Epoch: 8: Step: 1/4907, loss=0.070257, lr=0.000016
Train batch 100
Avg. loss per last 100 batches: 0.079939
Train batch 100
Avg. loss per last 100 batches: 0.079939
Epoch: 8: Step: 101/4907, loss=0.000678, lr=0.000016
Epoch: 8: Step: 101/4907, loss=0.000678, lr=0.000016
Train batch 200
Avg. loss per last 100 batches: 0.091158
Train batch 200
Avg. loss per last 100 batches: 0.091158
Epoch: 8: Step: 201/4907, loss=0.000032, lr=0.000016
Epoch: 8: Step: 201/4907, loss=0.000032, lr=0.000016
Train batch 300
Avg. loss per last 100 batches: 0.091427
Train batch 300
Avg. loss per last 100 batches: 0.091427
Epoch: 8: Step: 301/4907, loss=0.180556, lr=0.000016
Epoch: 8: Step: 301/4907, loss=0.180556, lr=0.000016
Train batch 400
Avg. loss per last 100 batches: 0.051529
Train batch 400
Avg. loss per last 100 batches: 0.051529
Epoch: 8: Step: 401/4907, loss=0.000031, lr=0.000016
Epoch: 8: Step: 401/4907, loss=0.000031, lr=0.000016
Train batch 500
Avg. loss per last 100 batches: 0.045360
Train batch 500
Avg. loss per last 100 batches: 0.045360
Epoch: 8: Step: 501/4907, loss=0.000952, lr=0.000016
Epoch: 8: Step: 501/4907, loss=0.000952, lr=0.000016
Train batch 600
Avg. loss per last 100 batches: 0.082107
Train batch 600
Avg. loss per last 100 batches: 0.082107
Epoch: 8: Step: 601/4907, loss=0.005888, lr=0.000016
Epoch: 8: Step: 601/4907, loss=0.005888, lr=0.000016
Train batch 700
Avg. loss per last 100 batches: 0.092970
Train batch 700
Avg. loss per last 100 batches: 0.092970
Epoch: 8: Step: 701/4907, loss=0.014154, lr=0.000016
Epoch: 8: Step: 701/4907, loss=0.014154, lr=0.000016
Train batch 800
Avg. loss per last 100 batches: 0.063562
Train batch 800
Avg. loss per last 100 batches: 0.063562
Epoch: 8: Step: 801/4907, loss=0.172330, lr=0.000016
Epoch: 8: Step: 801/4907, loss=0.172330, lr=0.000016
Train batch 900
Avg. loss per last 100 batches: 0.095049
Train batch 900
Avg. loss per last 100 batches: 0.095049
Epoch: 8: Step: 901/4907, loss=0.082524, lr=0.000016
Epoch: 8: Step: 901/4907, loss=0.082524, lr=0.000016
Train batch 1000
Avg. loss per last 100 batches: 0.053242
Train batch 1000
Avg. loss per last 100 batches: 0.053242
Epoch: 8: Step: 1001/4907, loss=0.012699, lr=0.000016
Epoch: 8: Step: 1001/4907, loss=0.012699, lr=0.000016
Train batch 1100
Avg. loss per last 100 batches: 0.052201
Train batch 1100
Avg. loss per last 100 batches: 0.052201
Epoch: 8: Step: 1101/4907, loss=0.000523, lr=0.000016
Epoch: 8: Step: 1101/4907, loss=0.000523, lr=0.000016
Train batch 1200
Avg. loss per last 100 batches: 0.072569
Train batch 1200
Avg. loss per last 100 batches: 0.072569
Epoch: 8: Step: 1201/4907, loss=0.071539, lr=0.000016
Epoch: 8: Step: 1201/4907, loss=0.071539, lr=0.000016
Train batch 1300
Avg. loss per last 100 batches: 0.053351
Train batch 1300
Avg. loss per last 100 batches: 0.053351
Epoch: 8: Step: 1301/4907, loss=0.007190, lr=0.000016
Epoch: 8: Step: 1301/4907, loss=0.007190, lr=0.000016
Train batch 1400
Avg. loss per last 100 batches: 0.065817
Train batch 1400
Avg. loss per last 100 batches: 0.065817
Epoch: 8: Step: 1401/4907, loss=0.000029, lr=0.000016
Epoch: 8: Step: 1401/4907, loss=0.000029, lr=0.000016
Train batch 1500
Avg. loss per last 100 batches: 0.110809
Train batch 1500
Avg. loss per last 100 batches: 0.110809
Epoch: 8: Step: 1501/4907, loss=0.000100, lr=0.000016
Epoch: 8: Step: 1501/4907, loss=0.000100, lr=0.000016
Train batch 1600
Avg. loss per last 100 batches: 0.079748
Train batch 1600
Avg. loss per last 100 batches: 0.079748
Epoch: 8: Step: 1601/4907, loss=0.007258, lr=0.000016
Epoch: 8: Step: 1601/4907, loss=0.007258, lr=0.000016
Train batch 1700
Avg. loss per last 100 batches: 0.097599
Train batch 1700
Avg. loss per last 100 batches: 0.097599
Epoch: 8: Step: 1701/4907, loss=0.011171, lr=0.000016
Epoch: 8: Step: 1701/4907, loss=0.011171, lr=0.000016
Train batch 1800
Avg. loss per last 100 batches: 0.057102
Train batch 1800
Avg. loss per last 100 batches: 0.057102
Epoch: 8: Step: 1801/4907, loss=0.000010, lr=0.000016
Epoch: 8: Step: 1801/4907, loss=0.000010, lr=0.000016
Train batch 1900
Avg. loss per last 100 batches: 0.088025
Train batch 1900
Avg. loss per last 100 batches: 0.088025
Epoch: 8: Step: 1901/4907, loss=0.177887, lr=0.000016
Epoch: 8: Step: 1901/4907, loss=0.177887, lr=0.000016
Train batch 2000
Avg. loss per last 100 batches: 0.075209
Train batch 2000
Avg. loss per last 100 batches: 0.075209
Epoch: 8: Step: 2001/4907, loss=0.000195, lr=0.000016
Epoch: 8: Step: 2001/4907, loss=0.000195, lr=0.000016
Train batch 2100
Avg. loss per last 100 batches: 0.056331
Train batch 2100
Avg. loss per last 100 batches: 0.056331
Epoch: 8: Step: 2101/4907, loss=0.000522, lr=0.000016
Epoch: 8: Step: 2101/4907, loss=0.000522, lr=0.000016
Train batch 2200
Avg. loss per last 100 batches: 0.082153
Train batch 2200
Avg. loss per last 100 batches: 0.082153
Epoch: 8: Step: 2201/4907, loss=0.014669, lr=0.000016
Epoch: 8: Step: 2201/4907, loss=0.014669, lr=0.000016
Train batch 2300
Avg. loss per last 100 batches: 0.090084
Train batch 2300
Avg. loss per last 100 batches: 0.090084
Epoch: 8: Step: 2301/4907, loss=0.000086, lr=0.000016
Epoch: 8: Step: 2301/4907, loss=0.000086, lr=0.000016
Train batch 2400
Avg. loss per last 100 batches: 0.069294
Train batch 2400
Avg. loss per last 100 batches: 0.069294
Epoch: 8: Step: 2401/4907, loss=0.000360, lr=0.000016
Epoch: 8: Step: 2401/4907, loss=0.000360, lr=0.000016
Train batch 2500
Avg. loss per last 100 batches: 0.092288
Train batch 2500
Avg. loss per last 100 batches: 0.092288
Epoch: 8: Step: 2501/4907, loss=0.000070, lr=0.000016
Epoch: 8: Step: 2501/4907, loss=0.000070, lr=0.000016
Train batch 2600
Avg. loss per last 100 batches: 0.066027
Train batch 2600
Avg. loss per last 100 batches: 0.066027
Epoch: 8: Step: 2601/4907, loss=0.006723, lr=0.000016
Epoch: 8: Step: 2601/4907, loss=0.006723, lr=0.000016
Train batch 2700
Avg. loss per last 100 batches: 0.097288
Train batch 2700
Avg. loss per last 100 batches: 0.097288
Epoch: 8: Step: 2701/4907, loss=0.271489, lr=0.000016
Epoch: 8: Step: 2701/4907, loss=0.271489, lr=0.000016
Train batch 2800
Avg. loss per last 100 batches: 0.081208
Train batch 2800
Avg. loss per last 100 batches: 0.081208
Epoch: 8: Step: 2801/4907, loss=0.000062, lr=0.000016
Epoch: 8: Step: 2801/4907, loss=0.000062, lr=0.000016
Train batch 2900
Avg. loss per last 100 batches: 0.062278
Train batch 2900
Avg. loss per last 100 batches: 0.062278
Epoch: 8: Step: 2901/4907, loss=0.000288, lr=0.000016
Epoch: 8: Step: 2901/4907, loss=0.000288, lr=0.000016
Train batch 3000
Avg. loss per last 100 batches: 0.094584
Train batch 3000
Avg. loss per last 100 batches: 0.094584
Epoch: 8: Step: 3001/4907, loss=0.324464, lr=0.000016
Epoch: 8: Step: 3001/4907, loss=0.324464, lr=0.000016
Train batch 3100
Avg. loss per last 100 batches: 0.064599
Train batch 3100
Avg. loss per last 100 batches: 0.064599
Epoch: 8: Step: 3101/4907, loss=0.002729, lr=0.000016
Epoch: 8: Step: 3101/4907, loss=0.002729, lr=0.000016
Train batch 3200
Avg. loss per last 100 batches: 0.076925
Train batch 3200
Avg. loss per last 100 batches: 0.076925
Epoch: 8: Step: 3201/4907, loss=0.156578, lr=0.000016
Epoch: 8: Step: 3201/4907, loss=0.156578, lr=0.000016
Train batch 3300
Avg. loss per last 100 batches: 0.096871
Train batch 3300
Avg. loss per last 100 batches: 0.096871
Epoch: 8: Step: 3301/4907, loss=0.031046, lr=0.000016
Epoch: 8: Step: 3301/4907, loss=0.031046, lr=0.000016
Train batch 3400
Avg. loss per last 100 batches: 0.078630
Train batch 3400
Avg. loss per last 100 batches: 0.078630
Epoch: 8: Step: 3401/4907, loss=0.027878, lr=0.000016
Epoch: 8: Step: 3401/4907, loss=0.027878, lr=0.000016
Train batch 3500
Avg. loss per last 100 batches: 0.088615
Train batch 3500
Avg. loss per last 100 batches: 0.088615
Epoch: 8: Step: 3501/4907, loss=0.000286, lr=0.000016
Epoch: 8: Step: 3501/4907, loss=0.000286, lr=0.000016
Train batch 3600
Avg. loss per last 100 batches: 0.068922
Train batch 3600
Avg. loss per last 100 batches: 0.068922
Epoch: 8: Step: 3601/4907, loss=0.029098, lr=0.000016
Epoch: 8: Step: 3601/4907, loss=0.029098, lr=0.000016
Train batch 3700
Avg. loss per last 100 batches: 0.053638
Train batch 3700
Avg. loss per last 100 batches: 0.053638
Epoch: 8: Step: 3701/4907, loss=0.000996, lr=0.000016
Epoch: 8: Step: 3701/4907, loss=0.000996, lr=0.000016
Train batch 3800
Avg. loss per last 100 batches: 0.079211
Train batch 3800
Avg. loss per last 100 batches: 0.079211
Epoch: 8: Step: 3801/4907, loss=0.005629, lr=0.000016
Epoch: 8: Step: 3801/4907, loss=0.005629, lr=0.000016
Train batch 3900
Avg. loss per last 100 batches: 0.083883
Train batch 3900
Avg. loss per last 100 batches: 0.083883
Epoch: 8: Step: 3901/4907, loss=0.003567, lr=0.000016
Epoch: 8: Step: 3901/4907, loss=0.003567, lr=0.000016
Train batch 4000
Avg. loss per last 100 batches: 0.088798
Train batch 4000
Avg. loss per last 100 batches: 0.088798
Epoch: 8: Step: 4001/4907, loss=0.000329, lr=0.000016
Epoch: 8: Step: 4001/4907, loss=0.000329, lr=0.000016
Train batch 4100
Avg. loss per last 100 batches: 0.080580
Train batch 4100
Avg. loss per last 100 batches: 0.080580
Epoch: 8: Step: 4101/4907, loss=0.000002, lr=0.000016
Epoch: 8: Step: 4101/4907, loss=0.000002, lr=0.000016
Train batch 4200
Avg. loss per last 100 batches: 0.084776
Train batch 4200
Avg. loss per last 100 batches: 0.084776
Epoch: 8: Step: 4201/4907, loss=0.000351, lr=0.000016
Epoch: 8: Step: 4201/4907, loss=0.000351, lr=0.000016
Train batch 4300
Avg. loss per last 100 batches: 0.049539
Train batch 4300
Avg. loss per last 100 batches: 0.049539
Epoch: 8: Step: 4301/4907, loss=0.014014, lr=0.000016
Epoch: 8: Step: 4301/4907, loss=0.014014, lr=0.000016
Train batch 4400
Avg. loss per last 100 batches: 0.145195
Train batch 4400
Avg. loss per last 100 batches: 0.145195
Epoch: 8: Step: 4401/4907, loss=0.002478, lr=0.000016
Epoch: 8: Step: 4401/4907, loss=0.002478, lr=0.000016
Train batch 4500
Avg. loss per last 100 batches: 0.064997
Train batch 4500
Avg. loss per last 100 batches: 0.064997
Epoch: 8: Step: 4501/4907, loss=0.000702, lr=0.000016
Epoch: 8: Step: 4501/4907, loss=0.000702, lr=0.000016
Train batch 4600
Avg. loss per last 100 batches: 0.082679
Train batch 4600
Avg. loss per last 100 batches: 0.082679
Epoch: 8: Step: 4601/4907, loss=0.224568, lr=0.000016
Epoch: 8: Step: 4601/4907, loss=0.224568, lr=0.000016
Train batch 4700
Avg. loss per last 100 batches: 0.079916
Train batch 4700
Avg. loss per last 100 batches: 0.079916
Epoch: 8: Step: 4701/4907, loss=0.051505, lr=0.000016
Epoch: 8: Step: 4701/4907, loss=0.051505, lr=0.000016
Train batch 4800
Avg. loss per last 100 batches: 0.094487
Train batch 4800
Avg. loss per last 100 batches: 0.094487
Epoch: 8: Step: 4801/4907, loss=0.000089, lr=0.000016
Epoch: 8: Step: 4801/4907, loss=0.000089, lr=0.000016
Train batch 4900
Avg. loss per last 100 batches: 0.053240
Train batch 4900
Avg. loss per last 100 batches: 0.053240
Epoch: 8: Step: 4901/4907, loss=0.073132, lr=0.000016
Epoch: 8: Step: 4901/4907, loss=0.073132, lr=0.000016
Validation: Epoch: 8 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 8 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.518122 sec., loss=0.165052 
Eval step: 99 , used_time=28.560748 sec., loss=0.165052 
Eval step: 199 , used_time=57.614597 sec., loss=0.815722 
Eval step: 199 , used_time=57.571932 sec., loss=0.815722 
Eval step: 299 , used_time=86.608543 sec., loss=0.439045 
Eval step: 299 , used_time=86.651236 sec., loss=0.439045 
Eval step: 399 , used_time=115.678103 sec., loss=0.456287 
Eval step: 399 , used_time=115.635420 sec., loss=0.456287 
Eval step: 499 , used_time=144.698786 sec., loss=0.261196 
Eval step: 499 , used_time=144.656055 sec., loss=0.261196 
NLL Validation: loss = 0.536356. correct prediction ratio  5832/6516 ~  0.895028
NLL Validation: loss = 0.536356. correct prediction ratio  5832/6516 ~  0.895028
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.8.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.8.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=38.717816 sec., loss=0.165052 
Eval step: 99 , used_time=27.831742 sec., loss=0.165052 
Eval step: 199 , used_time=67.766044 sec., loss=0.815722 
Eval step: 199 , used_time=56.879966 sec., loss=0.815722 
Eval step: 299 , used_time=96.788867 sec., loss=0.439045 
Eval step: 299 , used_time=85.902771 sec., loss=0.439045 
Eval step: 399 , used_time=114.978999 sec., loss=0.456287 
Eval step: 399 , used_time=125.865098 sec., loss=0.456287 
Eval step: 499 , used_time=154.946917 sec., loss=0.261196 
Eval step: 499 , used_time=144.061025 sec., loss=0.261196 
NLL Validation: loss = 0.536356. correct prediction ratio  5832/6516 ~  0.895028
NLL Validation: loss = 0.536356. correct prediction ratio  5832/6516 ~  0.895028
Av Loss per epoch=0.077725
epoch total correct predictions=57505
***** Epoch 9 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.8.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.8.4907
Av Loss per epoch=0.077725
epoch total correct predictions=57505
***** Epoch 9 *****
Epoch: 9: Step: 1/4907, loss=0.000180, lr=0.000016
Epoch: 9: Step: 1/4907, loss=0.000180, lr=0.000016
Train batch 100
Avg. loss per last 100 batches: 0.092055
Train batch 100
Avg. loss per last 100 batches: 0.092055
Epoch: 9: Step: 101/4907, loss=0.000053, lr=0.000016
Epoch: 9: Step: 101/4907, loss=0.000053, lr=0.000016
Train batch 200
Avg. loss per last 100 batches: 0.095327
Train batch 200
Avg. loss per last 100 batches: 0.095327
Epoch: 9: Step: 201/4907, loss=0.000219, lr=0.000016
Epoch: 9: Step: 201/4907, loss=0.000219, lr=0.000016
Train batch 300
Avg. loss per last 100 batches: 0.068487
Train batch 300
Avg. loss per last 100 batches: 0.068487
Epoch: 9: Step: 301/4907, loss=0.003441, lr=0.000016
Epoch: 9: Step: 301/4907, loss=0.003441, lr=0.000016
Train batch 400
Avg. loss per last 100 batches: 0.070286
Train batch 400
Avg. loss per last 100 batches: 0.070286
Epoch: 9: Step: 401/4907, loss=0.005840, lr=0.000016
Epoch: 9: Step: 401/4907, loss=0.005840, lr=0.000016
Train batch 500
Avg. loss per last 100 batches: 0.053985
Train batch 500
Avg. loss per last 100 batches: 0.053985
Epoch: 9: Step: 501/4907, loss=0.014112, lr=0.000016
Epoch: 9: Step: 501/4907, loss=0.014112, lr=0.000016
Train batch 600
Avg. loss per last 100 batches: 0.079526
Train batch 600
Avg. loss per last 100 batches: 0.079526
Epoch: 9: Step: 601/4907, loss=0.000268, lr=0.000016
Epoch: 9: Step: 601/4907, loss=0.000268, lr=0.000016
Train batch 700
Avg. loss per last 100 batches: 0.053199
Train batch 700
Avg. loss per last 100 batches: 0.053199
Epoch: 9: Step: 701/4907, loss=0.297094, lr=0.000016
Epoch: 9: Step: 701/4907, loss=0.297094, lr=0.000016
Train batch 800
Avg. loss per last 100 batches: 0.082671
Train batch 800
Avg. loss per last 100 batches: 0.082671
Epoch: 9: Step: 801/4907, loss=0.000058, lr=0.000016
Epoch: 9: Step: 801/4907, loss=0.000058, lr=0.000016
Train batch 900
Avg. loss per last 100 batches: 0.053164
Train batch 900
Avg. loss per last 100 batches: 0.053164
Epoch: 9: Step: 901/4907, loss=0.175520, lr=0.000016
Epoch: 9: Step: 901/4907, loss=0.175520, lr=0.000016
Train batch 1000
Avg. loss per last 100 batches: 0.085332
Train batch 1000
Avg. loss per last 100 batches: 0.085332
Epoch: 9: Step: 1001/4907, loss=0.000508, lr=0.000015
Epoch: 9: Step: 1001/4907, loss=0.000508, lr=0.000015
Train batch 1100
Avg. loss per last 100 batches: 0.071163
Train batch 1100
Avg. loss per last 100 batches: 0.071163
Epoch: 9: Step: 1101/4907, loss=0.000062, lr=0.000015
Epoch: 9: Step: 1101/4907, loss=0.000062, lr=0.000015
Train batch 1200
Avg. loss per last 100 batches: 0.043143
Train batch 1200
Avg. loss per last 100 batches: 0.043143
Epoch: 9: Step: 1201/4907, loss=0.006720, lr=0.000015
Epoch: 9: Step: 1201/4907, loss=0.006720, lr=0.000015
Train batch 1300
Avg. loss per last 100 batches: 0.060981
Train batch 1300
Avg. loss per last 100 batches: 0.060981
Epoch: 9: Step: 1301/4907, loss=0.143115, lr=0.000015
Epoch: 9: Step: 1301/4907, loss=0.143115, lr=0.000015
Train batch 1400
Avg. loss per last 100 batches: 0.099989
Train batch 1400
Avg. loss per last 100 batches: 0.099989
Epoch: 9: Step: 1401/4907, loss=0.005054, lr=0.000015
Epoch: 9: Step: 1401/4907, loss=0.005054, lr=0.000015
Train batch 1500
Avg. loss per last 100 batches: 0.038679
Train batch 1500
Avg. loss per last 100 batches: 0.038679
Epoch: 9: Step: 1501/4907, loss=0.013143, lr=0.000015
Epoch: 9: Step: 1501/4907, loss=0.013143, lr=0.000015
Train batch 1600
Avg. loss per last 100 batches: 0.055712
Train batch 1600
Avg. loss per last 100 batches: 0.055712
Epoch: 9: Step: 1601/4907, loss=0.000021, lr=0.000015
Epoch: 9: Step: 1601/4907, loss=0.000021, lr=0.000015
Train batch 1700
Avg. loss per last 100 batches: 0.083167
Train batch 1700
Avg. loss per last 100 batches: 0.083167
Epoch: 9: Step: 1701/4907, loss=0.000332, lr=0.000015
Epoch: 9: Step: 1701/4907, loss=0.000332, lr=0.000015
Train batch 1800
Avg. loss per last 100 batches: 0.062022
Train batch 1800
Avg. loss per last 100 batches: 0.062022
Epoch: 9: Step: 1801/4907, loss=0.001443, lr=0.000015
Epoch: 9: Step: 1801/4907, loss=0.001443, lr=0.000015
Train batch 1900
Avg. loss per last 100 batches: 0.039011
Train batch 1900
Avg. loss per last 100 batches: 0.039011
Epoch: 9: Step: 1901/4907, loss=0.001032, lr=0.000015
Epoch: 9: Step: 1901/4907, loss=0.001032, lr=0.000015
Train batch 2000
Avg. loss per last 100 batches: 0.096983
Train batch 2000
Avg. loss per last 100 batches: 0.096983
Epoch: 9: Step: 2001/4907, loss=0.000126, lr=0.000015
Epoch: 9: Step: 2001/4907, loss=0.000126, lr=0.000015
Train batch 2100
Avg. loss per last 100 batches: 0.118146
Train batch 2100
Avg. loss per last 100 batches: 0.118146
Epoch: 9: Step: 2101/4907, loss=0.000041, lr=0.000015
Epoch: 9: Step: 2101/4907, loss=0.000041, lr=0.000015
Train batch 2200
Avg. loss per last 100 batches: 0.079496
Train batch 2200
Avg. loss per last 100 batches: 0.079496
Epoch: 9: Step: 2201/4907, loss=0.000073, lr=0.000015
Epoch: 9: Step: 2201/4907, loss=0.000073, lr=0.000015
Train batch 2300
Avg. loss per last 100 batches: 0.093427
Train batch 2300
Avg. loss per last 100 batches: 0.093427
Epoch: 9: Step: 2301/4907, loss=0.000566, lr=0.000015
Epoch: 9: Step: 2301/4907, loss=0.000566, lr=0.000015
Train batch 2400
Avg. loss per last 100 batches: 0.063792
Train batch 2400
Avg. loss per last 100 batches: 0.063792
Epoch: 9: Step: 2401/4907, loss=0.415041, lr=0.000015
Epoch: 9: Step: 2401/4907, loss=0.415041, lr=0.000015
Train batch 2500
Avg. loss per last 100 batches: 0.105930
Train batch 2500
Avg. loss per last 100 batches: 0.105930
Epoch: 9: Step: 2501/4907, loss=0.000313, lr=0.000015
Epoch: 9: Step: 2501/4907, loss=0.000313, lr=0.000015
Train batch 2600
Avg. loss per last 100 batches: 0.098088
Train batch 2600
Avg. loss per last 100 batches: 0.098088
Epoch: 9: Step: 2601/4907, loss=0.196144, lr=0.000015
Epoch: 9: Step: 2601/4907, loss=0.196144, lr=0.000015
Train batch 2700
Avg. loss per last 100 batches: 0.053122
Train batch 2700
Avg. loss per last 100 batches: 0.053122
Epoch: 9: Step: 2701/4907, loss=0.000810, lr=0.000015
Epoch: 9: Step: 2701/4907, loss=0.000810, lr=0.000015
Train batch 2800
Avg. loss per last 100 batches: 0.055527
Train batch 2800
Avg. loss per last 100 batches: 0.055527
Epoch: 9: Step: 2801/4907, loss=0.000016, lr=0.000015
Epoch: 9: Step: 2801/4907, loss=0.000016, lr=0.000015
Train batch 2900
Avg. loss per last 100 batches: 0.060963
Train batch 2900
Avg. loss per last 100 batches: 0.060963
Epoch: 9: Step: 2901/4907, loss=0.000001, lr=0.000015
Epoch: 9: Step: 2901/4907, loss=0.000001, lr=0.000015
Train batch 3000
Avg. loss per last 100 batches: 0.064744
Train batch 3000
Avg. loss per last 100 batches: 0.064744
Epoch: 9: Step: 3001/4907, loss=0.010129, lr=0.000015
Epoch: 9: Step: 3001/4907, loss=0.010129, lr=0.000015
Train batch 3100
Avg. loss per last 100 batches: 0.057724
Train batch 3100
Avg. loss per last 100 batches: 0.057724
Epoch: 9: Step: 3101/4907, loss=0.000060, lr=0.000015
Epoch: 9: Step: 3101/4907, loss=0.000060, lr=0.000015
Train batch 3200
Avg. loss per last 100 batches: 0.093083
Train batch 3200
Avg. loss per last 100 batches: 0.093083
Epoch: 9: Step: 3201/4907, loss=0.000221, lr=0.000015
Epoch: 9: Step: 3201/4907, loss=0.000221, lr=0.000015
Train batch 3300
Avg. loss per last 100 batches: 0.073371
Train batch 3300
Avg. loss per last 100 batches: 0.073371
Epoch: 9: Step: 3301/4907, loss=0.000134, lr=0.000015
Epoch: 9: Step: 3301/4907, loss=0.000134, lr=0.000015
Train batch 3400
Avg. loss per last 100 batches: 0.107307
Train batch 3400
Avg. loss per last 100 batches: 0.107307
Epoch: 9: Step: 3401/4907, loss=0.002184, lr=0.000015
Epoch: 9: Step: 3401/4907, loss=0.002184, lr=0.000015
Train batch 3500
Avg. loss per last 100 batches: 0.079961
Train batch 3500
Avg. loss per last 100 batches: 0.079961
Epoch: 9: Step: 3501/4907, loss=0.010111, lr=0.000015
Epoch: 9: Step: 3501/4907, loss=0.010111, lr=0.000015
Train batch 3600
Avg. loss per last 100 batches: 0.063268
Train batch 3600
Avg. loss per last 100 batches: 0.063268
Epoch: 9: Step: 3601/4907, loss=0.155614, lr=0.000015
Epoch: 9: Step: 3601/4907, loss=0.155614, lr=0.000015
Train batch 3700
Avg. loss per last 100 batches: 0.082149
Train batch 3700
Avg. loss per last 100 batches: 0.082149
Epoch: 9: Step: 3701/4907, loss=0.000155, lr=0.000015
Epoch: 9: Step: 3701/4907, loss=0.000155, lr=0.000015
Train batch 3800
Avg. loss per last 100 batches: 0.077359
Train batch 3800
Avg. loss per last 100 batches: 0.077359
Epoch: 9: Step: 3801/4907, loss=0.019452, lr=0.000015
Epoch: 9: Step: 3801/4907, loss=0.019452, lr=0.000015
Train batch 3900
Avg. loss per last 100 batches: 0.069099
Train batch 3900
Avg. loss per last 100 batches: 0.069099
Epoch: 9: Step: 3901/4907, loss=0.055285, lr=0.000015
Epoch: 9: Step: 3901/4907, loss=0.055285, lr=0.000015
Train batch 4000
Avg. loss per last 100 batches: 0.076267
Train batch 4000
Avg. loss per last 100 batches: 0.076267
Epoch: 9: Step: 4001/4907, loss=0.001843, lr=0.000015
Epoch: 9: Step: 4001/4907, loss=0.001843, lr=0.000015
Train batch 4100
Avg. loss per last 100 batches: 0.092053
Train batch 4100
Avg. loss per last 100 batches: 0.092053
Epoch: 9: Step: 4101/4907, loss=0.296710, lr=0.000015
Epoch: 9: Step: 4101/4907, loss=0.296710, lr=0.000015
Train batch 4200
Avg. loss per last 100 batches: 0.062896
Train batch 4200
Avg. loss per last 100 batches: 0.062896
Epoch: 9: Step: 4201/4907, loss=0.001777, lr=0.000015
Epoch: 9: Step: 4201/4907, loss=0.001777, lr=0.000015
Train batch 4300
Avg. loss per last 100 batches: 0.067501
Train batch 4300
Avg. loss per last 100 batches: 0.067501
Epoch: 9: Step: 4301/4907, loss=0.073721, lr=0.000015
Epoch: 9: Step: 4301/4907, loss=0.073721, lr=0.000015
Train batch 4400
Avg. loss per last 100 batches: 0.082231
Train batch 4400
Avg. loss per last 100 batches: 0.082231
Epoch: 9: Step: 4401/4907, loss=0.000537, lr=0.000015
Epoch: 9: Step: 4401/4907, loss=0.000537, lr=0.000015
Train batch 4500
Avg. loss per last 100 batches: 0.059495
Train batch 4500
Avg. loss per last 100 batches: 0.059495
Epoch: 9: Step: 4501/4907, loss=0.407325, lr=0.000015
Epoch: 9: Step: 4501/4907, loss=0.407325, lr=0.000015
Train batch 4600
Avg. loss per last 100 batches: 0.086401
Train batch 4600
Avg. loss per last 100 batches: 0.086401
Epoch: 9: Step: 4601/4907, loss=0.000136, lr=0.000015
Epoch: 9: Step: 4601/4907, loss=0.000136, lr=0.000015
Train batch 4700
Avg. loss per last 100 batches: 0.072732
Train batch 4700
Avg. loss per last 100 batches: 0.072732
Epoch: 9: Step: 4701/4907, loss=0.003944, lr=0.000015
Epoch: 9: Step: 4701/4907, loss=0.003944, lr=0.000015
Train batch 4800
Avg. loss per last 100 batches: 0.055341
Train batch 4800
Avg. loss per last 100 batches: 0.055341
Epoch: 9: Step: 4801/4907, loss=0.000027, lr=0.000015
Epoch: 9: Step: 4801/4907, loss=0.000027, lr=0.000015
Train batch 4900
Avg. loss per last 100 batches: 0.056554
Train batch 4900
Avg. loss per last 100 batches: 0.056554
Epoch: 9: Step: 4901/4907, loss=0.000005, lr=0.000015
Epoch: 9: Step: 4901/4907, loss=0.000005, lr=0.000015
Validation: Epoch: 9 Step: 4907/4907
NLL validation ...
Validation: Epoch: 9 Step: 4907/4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=28.232532 sec., loss=0.174056 
Eval step: 99 , used_time=28.263746 sec., loss=0.174056 
Eval step: 199 , used_time=57.301748 sec., loss=0.475807 
Eval step: 199 , used_time=57.332971 sec., loss=0.475807 
Eval step: 299 , used_time=86.369617 sec., loss=0.530662 
Eval step: 299 , used_time=86.400835 sec., loss=0.530662 
Eval step: 399 , used_time=115.494409 sec., loss=0.383181 
Eval step: 399 , used_time=115.525627 sec., loss=0.383181 
Eval step: 499 , used_time=144.655092 sec., loss=0.128535 
Eval step: 499 , used_time=144.623964 sec., loss=0.128535 
NLL Validation: loss = 0.552753. correct prediction ratio  5830/6516 ~  0.894721
NLL Validation: loss = 0.552753. correct prediction ratio  5830/6516 ~  0.894721
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.9.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.9.4907
NLL validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Eval step: 99 , used_time=38.537642 sec., loss=0.174056 
Eval step: 99 , used_time=27.821020 sec., loss=0.174056 
Eval step: 199 , used_time=67.601649 sec., loss=0.475807 
Eval step: 199 , used_time=56.885020 sec., loss=0.475807 
Eval step: 299 , used_time=85.965075 sec., loss=0.530662 
Eval step: 299 , used_time=96.681709 sec., loss=0.530662 
Eval step: 399 , used_time=125.757493 sec., loss=0.383181 
Eval step: 399 , used_time=115.040880 sec., loss=0.383181 
Eval step: 499 , used_time=154.833003 sec., loss=0.128535 
Eval step: 499 , used_time=144.116390 sec., loss=0.128535 
NLL Validation: loss = 0.552753. correct prediction ratio  5830/6516 ~  0.894721
NLL Validation: loss = 0.552753. correct prediction ratio  5830/6516 ~  0.894721
Av Loss per epoch=0.073376
epoch total correct predictions=57644
***** Epoch 10 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.9.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.9.4907
Av Loss per epoch=0.073376
epoch total correct predictions=57644
***** Epoch 10 *****
Epoch: 10: Step: 1/4907, loss=0.000987, lr=0.000015
Epoch: 10: Step: 1/4907, loss=0.000987, lr=0.000015
Train batch 100
Avg. loss per last 100 batches: 0.047294
Train batch 100
Avg. loss per last 100 batches: 0.047294
Epoch: 10: Step: 101/4907, loss=0.000003, lr=0.000015
Epoch: 10: Step: 101/4907, loss=0.000003, lr=0.000015
Train batch 200
Avg. loss per last 100 batches: 0.071296
Train batch 200
Avg. loss per last 100 batches: 0.071296
Epoch: 10: Step: 201/4907, loss=0.000116, lr=0.000015
Epoch: 10: Step: 201/4907, loss=0.000116, lr=0.000015
Train batch 300
Avg. loss per last 100 batches: 0.027456
Train batch 300
Avg. loss per last 100 batches: 0.027456
Epoch: 10: Step: 301/4907, loss=0.000109, lr=0.000015
Epoch: 10: Step: 301/4907, loss=0.000109, lr=0.000015
Train batch 400
Avg. loss per last 100 batches: 0.074147
Train batch 400
Avg. loss per last 100 batches: 0.074147
Epoch: 10: Step: 401/4907, loss=0.000014, lr=0.000015
Epoch: 10: Step: 401/4907, loss=0.000014, lr=0.000015
Train batch 500
Avg. loss per last 100 batches: 0.075777
Train batch 500
Avg. loss per last 100 batches: 0.075777
Epoch: 10: Step: 501/4907, loss=0.012963, lr=0.000015
Epoch: 10: Step: 501/4907, loss=0.012963, lr=0.000015
Train batch 600
Avg. loss per last 100 batches: 0.080881
Train batch 600
Avg. loss per last 100 batches: 0.080881
Epoch: 10: Step: 601/4907, loss=0.000284, lr=0.000015
Epoch: 10: Step: 601/4907, loss=0.000284, lr=0.000015
Train batch 700
Avg. loss per last 100 batches: 0.092570
Train batch 700
Avg. loss per last 100 batches: 0.092570
Epoch: 10: Step: 701/4907, loss=0.000002, lr=0.000015
Epoch: 10: Step: 701/4907, loss=0.000002, lr=0.000015
Train batch 800
Avg. loss per last 100 batches: 0.076907
Train batch 800
Avg. loss per last 100 batches: 0.076907
Epoch: 10: Step: 801/4907, loss=0.002904, lr=0.000015
Epoch: 10: Step: 801/4907, loss=0.002904, lr=0.000015
Train batch 900
Avg. loss per last 100 batches: 0.044300
Train batch 900
Avg. loss per last 100 batches: 0.044300
Epoch: 10: Step: 901/4907, loss=0.112001, lr=0.000015
Epoch: 10: Step: 901/4907, loss=0.112001, lr=0.000015
Train batch 1000
Avg. loss per last 100 batches: 0.075018
Train batch 1000
Avg. loss per last 100 batches: 0.075018
Epoch: 10: Step: 1001/4907, loss=0.019157, lr=0.000015
Epoch: 10: Step: 1001/4907, loss=0.019157, lr=0.000015
Train batch 1100
Avg. loss per last 100 batches: 0.076574
Train batch 1100
Avg. loss per last 100 batches: 0.076574
Epoch: 10: Step: 1101/4907, loss=0.000010, lr=0.000015
Epoch: 10: Step: 1101/4907, loss=0.000010, lr=0.000015
Train batch 1200
Avg. loss per last 100 batches: 0.047168
Train batch 1200
Avg. loss per last 100 batches: 0.047168
Epoch: 10: Step: 1201/4907, loss=0.000007, lr=0.000015
Epoch: 10: Step: 1201/4907, loss=0.000007, lr=0.000015
Train batch 1300
Avg. loss per last 100 batches: 0.076987
Train batch 1300
Avg. loss per last 100 batches: 0.076987
Epoch: 10: Step: 1301/4907, loss=0.000347, lr=0.000015
Epoch: 10: Step: 1301/4907, loss=0.000347, lr=0.000015
Train batch 1400
Avg. loss per last 100 batches: 0.076074
Train batch 1400
Avg. loss per last 100 batches: 0.076074
Epoch: 10: Step: 1401/4907, loss=0.000601, lr=0.000015
Epoch: 10: Step: 1401/4907, loss=0.000601, lr=0.000015
Train batch 1500
Avg. loss per last 100 batches: 0.070065
Train batch 1500
Avg. loss per last 100 batches: 0.070065
Epoch: 10: Step: 1501/4907, loss=0.000036, lr=0.000015
Epoch: 10: Step: 1501/4907, loss=0.000036, lr=0.000015
Train batch 1600
Avg. loss per last 100 batches: 0.070114
Train batch 1600
Avg. loss per last 100 batches: 0.070114
Epoch: 10: Step: 1601/4907, loss=0.031496, lr=0.000015
Epoch: 10: Step: 1601/4907, loss=0.031496, lr=0.000015
Train batch 1700
Avg. loss per last 100 batches: 0.075231
Train batch 1700
Avg. loss per last 100 batches: 0.075231
Epoch: 10: Step: 1701/4907, loss=0.089258, lr=0.000015
Epoch: 10: Step: 1701/4907, loss=0.089258, lr=0.000015
Train batch 1800
Avg. loss per last 100 batches: 0.094266
Train batch 1800
Avg. loss per last 100 batches: 0.094266
Epoch: 10: Step: 1801/4907, loss=0.000013, lr=0.000015
Epoch: 10: Step: 1801/4907, loss=0.000013, lr=0.000015
Train batch 1900
Avg. loss per last 100 batches: 0.037046
Train batch 1900
Avg. loss per last 100 batches: 0.037046
Epoch: 10: Step: 1901/4907, loss=0.000099, lr=0.000015
Epoch: 10: Step: 1901/4907, loss=0.000099, lr=0.000015
Train batch 2000
Avg. loss per last 100 batches: 0.079096
Train batch 2000
Avg. loss per last 100 batches: 0.079096
Epoch: 10: Step: 2001/4907, loss=0.037162, lr=0.000015
Epoch: 10: Step: 2001/4907, loss=0.037162, lr=0.000015
Train batch 2100
Avg. loss per last 100 batches: 0.041409
Train batch 2100
Avg. loss per last 100 batches: 0.041409
Epoch: 10: Step: 2101/4907, loss=0.143008, lr=0.000015
Epoch: 10: Step: 2101/4907, loss=0.143008, lr=0.000015
Train batch 2200
Avg. loss per last 100 batches: 0.066911
Train batch 2200
Avg. loss per last 100 batches: 0.066911
Epoch: 10: Step: 2201/4907, loss=0.141689, lr=0.000015
Epoch: 10: Step: 2201/4907, loss=0.141689, lr=0.000015
Train batch 2300
Avg. loss per last 100 batches: 0.088802
Train batch 2300
Avg. loss per last 100 batches: 0.088802
Epoch: 10: Step: 2301/4907, loss=0.343575, lr=0.000015
Epoch: 10: Step: 2301/4907, loss=0.343575, lr=0.000015
Train batch 2400
Avg. loss per last 100 batches: 0.056024
Train batch 2400
Avg. loss per last 100 batches: 0.056024
Epoch: 10: Step: 2401/4907, loss=0.000904, lr=0.000015
Epoch: 10: Step: 2401/4907, loss=0.000904, lr=0.000015
Train batch 2500
Avg. loss per last 100 batches: 0.082424
Train batch 2500
Avg. loss per last 100 batches: 0.082424
Epoch: 10: Step: 2501/4907, loss=0.005260, lr=0.000015
Epoch: 10: Step: 2501/4907, loss=0.005260, lr=0.000015
Train batch 2600
Avg. loss per last 100 batches: 0.087347
Train batch 2600
Avg. loss per last 100 batches: 0.087347
Epoch: 10: Step: 2601/4907, loss=0.005381, lr=0.000015
Epoch: 10: Step: 2601/4907, loss=0.005381, lr=0.000015
Train batch 2700
Avg. loss per last 100 batches: 0.052543
Train batch 2700
Avg. loss per last 100 batches: 0.052543
Epoch: 10: Step: 2701/4907, loss=0.000306, lr=0.000015
Epoch: 10: Step: 2701/4907, loss=0.000306, lr=0.000015
Train batch 2800
Avg. loss per last 100 batches: 0.062820
Train batch 2800
Avg. loss per last 100 batches: 0.062820
Epoch: 10: Step: 2801/4907, loss=0.254281, lr=0.000015
Epoch: 10: Step: 2801/4907, loss=0.254281, lr=0.000015
Train batch 2900
Avg. loss per last 100 batches: 0.054382
Train batch 2900
Avg. loss per last 100 batches: 0.054382
Epoch: 10: Step: 2901/4907, loss=0.190167, lr=0.000015
Epoch: 10: Step: 2901/4907, loss=0.190167, lr=0.000015
Train batch 3000
Avg. loss per last 100 batches: 0.060719
Train batch 3000
Avg. loss per last 100 batches: 0.060719
Epoch: 10: Step: 3001/4907, loss=0.198898, lr=0.000015
Epoch: 10: Step: 3001/4907, loss=0.198898, lr=0.000015
Train batch 3100
Avg. loss per last 100 batches: 0.045563
Train batch 3100
Avg. loss per last 100 batches: 0.045563
Epoch: 10: Step: 3101/4907, loss=0.000863, lr=0.000015
Epoch: 10: Step: 3101/4907, loss=0.000863, lr=0.000015
Train batch 3200
Avg. loss per last 100 batches: 0.054740
Train batch 3200
Avg. loss per last 100 batches: 0.054740
Epoch: 10: Step: 3201/4907, loss=0.002378, lr=0.000015
Epoch: 10: Step: 3201/4907, loss=0.002378, lr=0.000015
Train batch 3300
Avg. loss per last 100 batches: 0.077398
Train batch 3300
Avg. loss per last 100 batches: 0.077398
Epoch: 10: Step: 3301/4907, loss=0.610407, lr=0.000015
Epoch: 10: Step: 3301/4907, loss=0.610407, lr=0.000015
Train batch 3400
Avg. loss per last 100 batches: 0.079666
Train batch 3400
Avg. loss per last 100 batches: 0.079666
Epoch: 10: Step: 3401/4907, loss=0.053906, lr=0.000015
Epoch: 10: Step: 3401/4907, loss=0.053906, lr=0.000015
Train batch 3500
Avg. loss per last 100 batches: 0.091298
Train batch 3500
Avg. loss per last 100 batches: 0.091298
Epoch: 10: Step: 3501/4907, loss=0.004024, lr=0.000015
Epoch: 10: Step: 3501/4907, loss=0.004024, lr=0.000015
Train batch 3600
Avg. loss per last 100 batches: 0.069476
Train batch 3600
Avg. loss per last 100 batches: 0.069476
Epoch: 10: Step: 3601/4907, loss=0.000145, lr=0.000015
Epoch: 10: Step: 3601/4907, loss=0.000145, lr=0.000015
Train batch 3700
Avg. loss per last 100 batches: 0.078743
Train batch 3700
Avg. loss per last 100 batches: 0.078743
Epoch: 10: Step: 3701/4907, loss=0.003354, lr=0.000015
Epoch: 10: Step: 3701/4907, loss=0.003354, lr=0.000015
Train batch 3800
Avg. loss per last 100 batches: 0.072705
Train batch 3800
Avg. loss per last 100 batches: 0.072705
Epoch: 10: Step: 3801/4907, loss=0.335031, lr=0.000015
Epoch: 10: Step: 3801/4907, loss=0.335031, lr=0.000015
Train batch 3900
Avg. loss per last 100 batches: 0.067039
Train batch 3900
Avg. loss per last 100 batches: 0.067039
Epoch: 10: Step: 3901/4907, loss=0.007218, lr=0.000015
Epoch: 10: Step: 3901/4907, loss=0.007218, lr=0.000015
Train batch 4000
Avg. loss per last 100 batches: 0.080921
Train batch 4000
Avg. loss per last 100 batches: 0.080921
Epoch: 10: Step: 4001/4907, loss=0.000576, lr=0.000015
Epoch: 10: Step: 4001/4907, loss=0.000576, lr=0.000015
Train batch 4100
Avg. loss per last 100 batches: 0.078751
Train batch 4100
Avg. loss per last 100 batches: 0.078751
Epoch: 10: Step: 4101/4907, loss=0.616859, lr=0.000015
Epoch: 10: Step: 4101/4907, loss=0.616859, lr=0.000015
Train batch 4200
Avg. loss per last 100 batches: 0.086800
Train batch 4200
Avg. loss per last 100 batches: 0.086800
Epoch: 10: Step: 4201/4907, loss=0.069466, lr=0.000015
Epoch: 10: Step: 4201/4907, loss=0.069466, lr=0.000015
Train batch 4300
Avg. loss per last 100 batches: 0.076331
Train batch 4300
Avg. loss per last 100 batches: 0.076331
Epoch: 10: Step: 4301/4907, loss=0.000287, lr=0.000015
Epoch: 10: Step: 4301/4907, loss=0.000287, lr=0.000015
Train batch 4400
Avg. loss per last 100 batches: 0.046416
Train batch 4400
Avg. loss per last 100 batches: 0.046416
Epoch: 10: Step: 4401/4907, loss=0.000002, lr=0.000015
Epoch: 10: Step: 4401/4907, loss=0.000002, lr=0.000015
Train batch 4500
Avg. loss per last 100 batches: 0.088773
Train batch 4500
Avg. loss per last 100 batches: 0.088773
Epoch: 10: Step: 4501/4907, loss=0.000464, lr=0.000015
Epoch: 10: Step: 4501/4907, loss=0.000464, lr=0.000015
Train batch 4600
Avg. loss per last 100 batches: 0.051550
Train batch 4600
Avg. loss per last 100 batches: 0.051550
Epoch: 10: Step: 4601/4907, loss=0.000862, lr=0.000015
Epoch: 10: Step: 4601/4907, loss=0.000862, lr=0.000015
Train batch 4700
Avg. loss per last 100 batches: 0.059316
Train batch 4700
Avg. loss per last 100 batches: 0.059316
Epoch: 10: Step: 4701/4907, loss=0.264175, lr=0.000015
Epoch: 10: Step: 4701/4907, loss=0.264175, lr=0.000015
Train batch 4800
Avg. loss per last 100 batches: 0.077372
Train batch 4800
Avg. loss per last 100 batches: 0.077372
Epoch: 10: Step: 4801/4907, loss=0.001017, lr=0.000015
Epoch: 10: Step: 4801/4907, loss=0.001017, lr=0.000015
Train batch 4900
Avg. loss per last 100 batches: 0.057812
Train batch 4900
Avg. loss per last 100 batches: 0.057812
Epoch: 10: Step: 4901/4907, loss=0.000707, lr=0.000015
Epoch: 10: Step: 4901/4907, loss=0.000707, lr=0.000015
Validation: Epoch: 10 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 10 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
../baselines/DPR/train_dense_encoder.py:282: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)
  gold_idx = (indices[i] == idx).nonzero()
../baselines/DPR/train_dense_encoder.py:282: UserWarning: This overload of nonzero is deprecated:
	nonzero()
Consider using one of the following signatures instead:
	nonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)
  gold_idx = (indices[i] == idx).nonzero()
Av.rank validation: average rank 193.34745242480048, total questions=6516
Av.rank validation: average rank 193.34745242480048, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.10.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.10.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.10.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 193.34745242480048, total questions=6516
Av.rank validation: average rank 193.34745242480048, total questions=6516
Av Loss per epoch=0.068525
epoch total correct predictions=57729
***** Epoch 11 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.10.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.10.4907
New Best validation checkpoint ../biencoders/trained/B+A/dpr_biencoder.10.4907
Av Loss per epoch=0.068525
epoch total correct predictions=57729
***** Epoch 11 *****
Epoch: 11: Step: 1/4907, loss=0.000007, lr=0.000015
Epoch: 11: Step: 1/4907, loss=0.000007, lr=0.000015
Train batch 100
Avg. loss per last 100 batches: 0.044466
Train batch 100
Avg. loss per last 100 batches: 0.044466
Epoch: 11: Step: 101/4907, loss=0.503454, lr=0.000015
Epoch: 11: Step: 101/4907, loss=0.503454, lr=0.000015
Train batch 200
Avg. loss per last 100 batches: 0.046913
Train batch 200
Avg. loss per last 100 batches: 0.046913
Epoch: 11: Step: 201/4907, loss=0.845312, lr=0.000015
Epoch: 11: Step: 201/4907, loss=0.845312, lr=0.000015
Train batch 300
Avg. loss per last 100 batches: 0.069361
Train batch 300
Avg. loss per last 100 batches: 0.069361
Epoch: 11: Step: 301/4907, loss=0.000000, lr=0.000015
Epoch: 11: Step: 301/4907, loss=0.000000, lr=0.000015
Train batch 400
Avg. loss per last 100 batches: 0.051020
Train batch 400
Avg. loss per last 100 batches: 0.051020
Epoch: 11: Step: 401/4907, loss=0.000030, lr=0.000015
Epoch: 11: Step: 401/4907, loss=0.000030, lr=0.000015
Train batch 500
Avg. loss per last 100 batches: 0.042606
Train batch 500
Avg. loss per last 100 batches: 0.042606
Epoch: 11: Step: 501/4907, loss=0.358192, lr=0.000015
Epoch: 11: Step: 501/4907, loss=0.358192, lr=0.000015
Train batch 600
Avg. loss per last 100 batches: 0.076132
Train batch 600
Avg. loss per last 100 batches: 0.076132
Epoch: 11: Step: 601/4907, loss=0.000001, lr=0.000015
Epoch: 11: Step: 601/4907, loss=0.000001, lr=0.000015
Train batch 700
Avg. loss per last 100 batches: 0.071068
Train batch 700
Avg. loss per last 100 batches: 0.071068
Epoch: 11: Step: 701/4907, loss=0.000321, lr=0.000015
Epoch: 11: Step: 701/4907, loss=0.000321, lr=0.000015
Train batch 800
Avg. loss per last 100 batches: 0.043192
Train batch 800
Avg. loss per last 100 batches: 0.043192
Epoch: 11: Step: 801/4907, loss=0.277084, lr=0.000015
Epoch: 11: Step: 801/4907, loss=0.277084, lr=0.000015
Train batch 900
Avg. loss per last 100 batches: 0.040193
Train batch 900
Avg. loss per last 100 batches: 0.040193
Epoch: 11: Step: 901/4907, loss=0.001065, lr=0.000014
Epoch: 11: Step: 901/4907, loss=0.001065, lr=0.000014
Train batch 1000
Avg. loss per last 100 batches: 0.052884
Train batch 1000
Avg. loss per last 100 batches: 0.052884
Epoch: 11: Step: 1001/4907, loss=0.000041, lr=0.000014
Epoch: 11: Step: 1001/4907, loss=0.000041, lr=0.000014
Train batch 1100
Avg. loss per last 100 batches: 0.043680
Train batch 1100
Avg. loss per last 100 batches: 0.043680
Epoch: 11: Step: 1101/4907, loss=0.000494, lr=0.000014
Epoch: 11: Step: 1101/4907, loss=0.000494, lr=0.000014
Train batch 1200
Avg. loss per last 100 batches: 0.078244
Train batch 1200
Avg. loss per last 100 batches: 0.078244
Epoch: 11: Step: 1201/4907, loss=0.000594, lr=0.000014
Epoch: 11: Step: 1201/4907, loss=0.000594, lr=0.000014
Train batch 1300
Avg. loss per last 100 batches: 0.062604
Train batch 1300
Avg. loss per last 100 batches: 0.062604
Epoch: 11: Step: 1301/4907, loss=0.148166, lr=0.000014
Epoch: 11: Step: 1301/4907, loss=0.148166, lr=0.000014
Train batch 1400
Avg. loss per last 100 batches: 0.052381
Train batch 1400
Avg. loss per last 100 batches: 0.052381
Epoch: 11: Step: 1401/4907, loss=0.017911, lr=0.000014
Epoch: 11: Step: 1401/4907, loss=0.017911, lr=0.000014
Train batch 1500
Avg. loss per last 100 batches: 0.076602
Train batch 1500
Avg. loss per last 100 batches: 0.076602
Epoch: 11: Step: 1501/4907, loss=0.000000, lr=0.000014
Epoch: 11: Step: 1501/4907, loss=0.000000, lr=0.000014
Train batch 1600
Avg. loss per last 100 batches: 0.045208
Train batch 1600
Avg. loss per last 100 batches: 0.045208
Epoch: 11: Step: 1601/4907, loss=0.002329, lr=0.000014
Epoch: 11: Step: 1601/4907, loss=0.002329, lr=0.000014
Train batch 1700
Avg. loss per last 100 batches: 0.053554
Train batch 1700
Avg. loss per last 100 batches: 0.053554
Epoch: 11: Step: 1701/4907, loss=0.000002, lr=0.000014
Epoch: 11: Step: 1701/4907, loss=0.000002, lr=0.000014
Train batch 1800
Avg. loss per last 100 batches: 0.076491
Train batch 1800
Avg. loss per last 100 batches: 0.076491
Epoch: 11: Step: 1801/4907, loss=0.002316, lr=0.000014
Epoch: 11: Step: 1801/4907, loss=0.002316, lr=0.000014
Train batch 1900
Avg. loss per last 100 batches: 0.073875
Train batch 1900
Avg. loss per last 100 batches: 0.073875
Epoch: 11: Step: 1901/4907, loss=0.000960, lr=0.000014
Epoch: 11: Step: 1901/4907, loss=0.000960, lr=0.000014
Train batch 2000
Avg. loss per last 100 batches: 0.073785
Train batch 2000
Avg. loss per last 100 batches: 0.073785
Epoch: 11: Step: 2001/4907, loss=0.004562, lr=0.000014
Epoch: 11: Step: 2001/4907, loss=0.004562, lr=0.000014
Train batch 2100
Avg. loss per last 100 batches: 0.063725
Train batch 2100
Avg. loss per last 100 batches: 0.063725
Epoch: 11: Step: 2101/4907, loss=0.002623, lr=0.000014
Epoch: 11: Step: 2101/4907, loss=0.002623, lr=0.000014
Train batch 2200
Avg. loss per last 100 batches: 0.066446
Train batch 2200
Avg. loss per last 100 batches: 0.066446
Epoch: 11: Step: 2201/4907, loss=0.000036, lr=0.000014
Epoch: 11: Step: 2201/4907, loss=0.000036, lr=0.000014
Train batch 2300
Avg. loss per last 100 batches: 0.074770
Train batch 2300
Avg. loss per last 100 batches: 0.074770
Epoch: 11: Step: 2301/4907, loss=0.000034, lr=0.000014
Epoch: 11: Step: 2301/4907, loss=0.000034, lr=0.000014
Train batch 2400
Avg. loss per last 100 batches: 0.045657
Train batch 2400
Avg. loss per last 100 batches: 0.045657
Epoch: 11: Step: 2401/4907, loss=0.000861, lr=0.000014
Epoch: 11: Step: 2401/4907, loss=0.000861, lr=0.000014
Train batch 2500
Avg. loss per last 100 batches: 0.037811
Train batch 2500
Avg. loss per last 100 batches: 0.037811
Epoch: 11: Step: 2501/4907, loss=0.031501, lr=0.000014
Epoch: 11: Step: 2501/4907, loss=0.031501, lr=0.000014
Train batch 2600
Avg. loss per last 100 batches: 0.064771
Train batch 2600
Avg. loss per last 100 batches: 0.064771
Epoch: 11: Step: 2601/4907, loss=0.071353, lr=0.000014
Epoch: 11: Step: 2601/4907, loss=0.071353, lr=0.000014
Train batch 2700
Avg. loss per last 100 batches: 0.058731
Train batch 2700
Avg. loss per last 100 batches: 0.058731
Epoch: 11: Step: 2701/4907, loss=0.033143, lr=0.000014
Epoch: 11: Step: 2701/4907, loss=0.033143, lr=0.000014
Train batch 2800
Avg. loss per last 100 batches: 0.060452
Train batch 2800
Avg. loss per last 100 batches: 0.060452
Epoch: 11: Step: 2801/4907, loss=0.019707, lr=0.000014
Epoch: 11: Step: 2801/4907, loss=0.019707, lr=0.000014
Train batch 2900
Avg. loss per last 100 batches: 0.062219
Train batch 2900
Avg. loss per last 100 batches: 0.062219
Epoch: 11: Step: 2901/4907, loss=0.005429, lr=0.000014
Epoch: 11: Step: 2901/4907, loss=0.005429, lr=0.000014
Train batch 3000
Avg. loss per last 100 batches: 0.058062
Train batch 3000
Avg. loss per last 100 batches: 0.058062
Epoch: 11: Step: 3001/4907, loss=0.000016, lr=0.000014
Epoch: 11: Step: 3001/4907, loss=0.000016, lr=0.000014
Train batch 3100
Avg. loss per last 100 batches: 0.060968
Train batch 3100
Avg. loss per last 100 batches: 0.060968
Epoch: 11: Step: 3101/4907, loss=0.038155, lr=0.000014
Epoch: 11: Step: 3101/4907, loss=0.038155, lr=0.000014
Train batch 3200
Avg. loss per last 100 batches: 0.048979
Train batch 3200
Avg. loss per last 100 batches: 0.048979
Epoch: 11: Step: 3201/4907, loss=0.055738, lr=0.000014
Epoch: 11: Step: 3201/4907, loss=0.055738, lr=0.000014
Train batch 3300
Avg. loss per last 100 batches: 0.078374
Train batch 3300
Avg. loss per last 100 batches: 0.078374
Epoch: 11: Step: 3301/4907, loss=0.000077, lr=0.000014
Epoch: 11: Step: 3301/4907, loss=0.000077, lr=0.000014
Train batch 3400
Avg. loss per last 100 batches: 0.053694
Train batch 3400
Avg. loss per last 100 batches: 0.053694
Epoch: 11: Step: 3401/4907, loss=0.004131, lr=0.000014
Epoch: 11: Step: 3401/4907, loss=0.004131, lr=0.000014
Train batch 3500
Avg. loss per last 100 batches: 0.065531
Train batch 3500
Avg. loss per last 100 batches: 0.065531
Epoch: 11: Step: 3501/4907, loss=0.000168, lr=0.000014
Epoch: 11: Step: 3501/4907, loss=0.000168, lr=0.000014
Train batch 3600
Avg. loss per last 100 batches: 0.102978
Train batch 3600
Avg. loss per last 100 batches: 0.102978
Epoch: 11: Step: 3601/4907, loss=0.000513, lr=0.000014
Epoch: 11: Step: 3601/4907, loss=0.000513, lr=0.000014
Train batch 3700
Avg. loss per last 100 batches: 0.079212
Train batch 3700
Avg. loss per last 100 batches: 0.079212
Epoch: 11: Step: 3701/4907, loss=0.111071, lr=0.000014
Epoch: 11: Step: 3701/4907, loss=0.111071, lr=0.000014
Train batch 3800
Avg. loss per last 100 batches: 0.064209
Train batch 3800
Avg. loss per last 100 batches: 0.064209
Epoch: 11: Step: 3801/4907, loss=0.000385, lr=0.000014
Epoch: 11: Step: 3801/4907, loss=0.000385, lr=0.000014
Train batch 3900
Avg. loss per last 100 batches: 0.056603
Train batch 3900
Avg. loss per last 100 batches: 0.056603
Epoch: 11: Step: 3901/4907, loss=0.002479, lr=0.000014
Epoch: 11: Step: 3901/4907, loss=0.002479, lr=0.000014
Train batch 4000
Avg. loss per last 100 batches: 0.044380
Train batch 4000
Avg. loss per last 100 batches: 0.044380
Epoch: 11: Step: 4001/4907, loss=0.000003, lr=0.000014
Epoch: 11: Step: 4001/4907, loss=0.000003, lr=0.000014
Train batch 4100
Avg. loss per last 100 batches: 0.049807
Train batch 4100
Avg. loss per last 100 batches: 0.049807
Epoch: 11: Step: 4101/4907, loss=0.000089, lr=0.000014
Epoch: 11: Step: 4101/4907, loss=0.000089, lr=0.000014
Train batch 4200
Avg. loss per last 100 batches: 0.032936
Train batch 4200
Avg. loss per last 100 batches: 0.032936
Epoch: 11: Step: 4201/4907, loss=0.141587, lr=0.000014
Epoch: 11: Step: 4201/4907, loss=0.141587, lr=0.000014
Train batch 4300
Avg. loss per last 100 batches: 0.085931
Train batch 4300
Avg. loss per last 100 batches: 0.085931
Epoch: 11: Step: 4301/4907, loss=0.116308, lr=0.000014
Epoch: 11: Step: 4301/4907, loss=0.116308, lr=0.000014
Train batch 4400
Avg. loss per last 100 batches: 0.061514
Train batch 4400
Avg. loss per last 100 batches: 0.061514
Epoch: 11: Step: 4401/4907, loss=0.013392, lr=0.000014
Epoch: 11: Step: 4401/4907, loss=0.013392, lr=0.000014
Train batch 4500
Avg. loss per last 100 batches: 0.068784
Train batch 4500
Avg. loss per last 100 batches: 0.068784
Epoch: 11: Step: 4501/4907, loss=0.000007, lr=0.000014
Epoch: 11: Step: 4501/4907, loss=0.000007, lr=0.000014
Train batch 4600
Avg. loss per last 100 batches: 0.065190
Train batch 4600
Avg. loss per last 100 batches: 0.065190
Epoch: 11: Step: 4601/4907, loss=0.334085, lr=0.000014
Epoch: 11: Step: 4601/4907, loss=0.334085, lr=0.000014
Train batch 4700
Avg. loss per last 100 batches: 0.066530
Train batch 4700
Avg. loss per last 100 batches: 0.066530
Epoch: 11: Step: 4701/4907, loss=0.000012, lr=0.000014
Epoch: 11: Step: 4701/4907, loss=0.000012, lr=0.000014
Train batch 4800
Avg. loss per last 100 batches: 0.061669
Train batch 4800
Avg. loss per last 100 batches: 0.061669
Epoch: 11: Step: 4801/4907, loss=0.000024, lr=0.000014
Epoch: 11: Step: 4801/4907, loss=0.000024, lr=0.000014
Train batch 4900
Avg. loss per last 100 batches: 0.057194
Train batch 4900
Avg. loss per last 100 batches: 0.057194
Epoch: 11: Step: 4901/4907, loss=0.009478, lr=0.000014
Epoch: 11: Step: 4901/4907, loss=0.009478, lr=0.000014
Validation: Epoch: 11 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Validation: Epoch: 11 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 199.71132596685084, total questions=6516
Av.rank validation: average rank 199.71132596685084, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.11.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.11.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 199.71132596685084, total questions=6516
Av.rank validation: average rank 199.71132596685084, total questions=6516
Av Loss per epoch=0.060582
epoch total correct predictions=57850
***** Epoch 12 *****
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.11.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.11.4907
Av Loss per epoch=0.060582
epoch total correct predictions=57850
***** Epoch 12 *****
Epoch: 12: Step: 1/4907, loss=0.000859, lr=0.000014
Epoch: 12: Step: 1/4907, loss=0.000859, lr=0.000014
Train batch 100
Avg. loss per last 100 batches: 0.030073
Train batch 100
Avg. loss per last 100 batches: 0.030073
Epoch: 12: Step: 101/4907, loss=0.000005, lr=0.000014
Epoch: 12: Step: 101/4907, loss=0.000005, lr=0.000014
Train batch 200
Avg. loss per last 100 batches: 0.069324
Train batch 200
Avg. loss per last 100 batches: 0.069324
Epoch: 12: Step: 201/4907, loss=0.000000, lr=0.000014
Epoch: 12: Step: 201/4907, loss=0.000000, lr=0.000014
Train batch 300
Avg. loss per last 100 batches: 0.060460
Train batch 300
Avg. loss per last 100 batches: 0.060460
Epoch: 12: Step: 301/4907, loss=0.002744, lr=0.000014
Epoch: 12: Step: 301/4907, loss=0.002744, lr=0.000014
Train batch 400
Avg. loss per last 100 batches: 0.057361
Train batch 400
Avg. loss per last 100 batches: 0.057361
Epoch: 12: Step: 401/4907, loss=0.080886, lr=0.000014
Epoch: 12: Step: 401/4907, loss=0.080886, lr=0.000014
Train batch 500
Avg. loss per last 100 batches: 0.052697
Train batch 500
Avg. loss per last 100 batches: 0.052697
Epoch: 12: Step: 501/4907, loss=0.265885, lr=0.000014
Epoch: 12: Step: 501/4907, loss=0.265885, lr=0.000014
Train batch 600
Avg. loss per last 100 batches: 0.077685
Train batch 600
Avg. loss per last 100 batches: 0.077685
Epoch: 12: Step: 601/4907, loss=0.004843, lr=0.000014
Epoch: 12: Step: 601/4907, loss=0.004843, lr=0.000014
Train batch 700
Avg. loss per last 100 batches: 0.032966
Train batch 700
Avg. loss per last 100 batches: 0.032966
Epoch: 12: Step: 701/4907, loss=0.000098, lr=0.000014
Epoch: 12: Step: 701/4907, loss=0.000098, lr=0.000014
Train batch 800
Avg. loss per last 100 batches: 0.051106
Train batch 800
Avg. loss per last 100 batches: 0.051106
Epoch: 12: Step: 801/4907, loss=0.000708, lr=0.000014
Epoch: 12: Step: 801/4907, loss=0.000708, lr=0.000014
Train batch 900
Avg. loss per last 100 batches: 0.062672
Train batch 900
Avg. loss per last 100 batches: 0.062672
Epoch: 12: Step: 901/4907, loss=0.000267, lr=0.000014
Epoch: 12: Step: 901/4907, loss=0.000267, lr=0.000014
Train batch 1000
Avg. loss per last 100 batches: 0.050067
Train batch 1000
Avg. loss per last 100 batches: 0.050067
Epoch: 12: Step: 1001/4907, loss=0.000000, lr=0.000014
Epoch: 12: Step: 1001/4907, loss=0.000000, lr=0.000014
Train batch 1100
Avg. loss per last 100 batches: 0.049415
Train batch 1100
Avg. loss per last 100 batches: 0.049415
Epoch: 12: Step: 1101/4907, loss=0.019301, lr=0.000014
Epoch: 12: Step: 1101/4907, loss=0.019301, lr=0.000014
Train batch 1200
Avg. loss per last 100 batches: 0.045503
Train batch 1200
Avg. loss per last 100 batches: 0.045503
Epoch: 12: Step: 1201/4907, loss=0.036965, lr=0.000014
Epoch: 12: Step: 1201/4907, loss=0.036965, lr=0.000014
Train batch 1300
Avg. loss per last 100 batches: 0.071991
Train batch 1300
Avg. loss per last 100 batches: 0.071991
Epoch: 12: Step: 1301/4907, loss=0.000083, lr=0.000014
Epoch: 12: Step: 1301/4907, loss=0.000083, lr=0.000014
Train batch 1400
Avg. loss per last 100 batches: 0.073930
Train batch 1400
Avg. loss per last 100 batches: 0.073930
Epoch: 12: Step: 1401/4907, loss=0.521888, lr=0.000014
Epoch: 12: Step: 1401/4907, loss=0.521888, lr=0.000014
Train batch 1500
Avg. loss per last 100 batches: 0.069398
Train batch 1500
Avg. loss per last 100 batches: 0.069398
Epoch: 12: Step: 1501/4907, loss=0.000410, lr=0.000014
Epoch: 12: Step: 1501/4907, loss=0.000410, lr=0.000014
Train batch 1600
Avg. loss per last 100 batches: 0.065550
Train batch 1600
Avg. loss per last 100 batches: 0.065550
Epoch: 12: Step: 1601/4907, loss=0.000002, lr=0.000014
Epoch: 12: Step: 1601/4907, loss=0.000002, lr=0.000014
Train batch 1700
Avg. loss per last 100 batches: 0.047882
Train batch 1700
Avg. loss per last 100 batches: 0.047882
Epoch: 12: Step: 1701/4907, loss=0.000025, lr=0.000014
Epoch: 12: Step: 1701/4907, loss=0.000025, lr=0.000014
Train batch 1800
Avg. loss per last 100 batches: 0.089684
Train batch 1800
Avg. loss per last 100 batches: 0.089684
Epoch: 12: Step: 1801/4907, loss=0.000002, lr=0.000014
Epoch: 12: Step: 1801/4907, loss=0.000002, lr=0.000014
Train batch 1900
Avg. loss per last 100 batches: 0.042367
Train batch 1900
Avg. loss per last 100 batches: 0.042367
Epoch: 12: Step: 1901/4907, loss=0.000141, lr=0.000014
Epoch: 12: Step: 1901/4907, loss=0.000141, lr=0.000014
Train batch 2000
Avg. loss per last 100 batches: 0.071275
Train batch 2000
Avg. loss per last 100 batches: 0.071275
Epoch: 12: Step: 2001/4907, loss=0.000073, lr=0.000014
Epoch: 12: Step: 2001/4907, loss=0.000073, lr=0.000014
Train batch 2100
Avg. loss per last 100 batches: 0.046989
Train batch 2100
Avg. loss per last 100 batches: 0.046989
Epoch: 12: Step: 2101/4907, loss=0.000023, lr=0.000014
Epoch: 12: Step: 2101/4907, loss=0.000023, lr=0.000014
Train batch 2200
Avg. loss per last 100 batches: 0.024631
Train batch 2200
Avg. loss per last 100 batches: 0.024631
Epoch: 12: Step: 2201/4907, loss=0.000021, lr=0.000014
Epoch: 12: Step: 2201/4907, loss=0.000021, lr=0.000014
Train batch 2300
Avg. loss per last 100 batches: 0.077975
Train batch 2300
Avg. loss per last 100 batches: 0.077975
Epoch: 12: Step: 2301/4907, loss=0.000000, lr=0.000014
Epoch: 12: Step: 2301/4907, loss=0.000000, lr=0.000014
Train batch 2400
Avg. loss per last 100 batches: 0.056872
Train batch 2400
Avg. loss per last 100 batches: 0.056872
Epoch: 12: Step: 2401/4907, loss=0.000939, lr=0.000014
Epoch: 12: Step: 2401/4907, loss=0.000939, lr=0.000014
Train batch 2500
Avg. loss per last 100 batches: 0.035800
Train batch 2500
Avg. loss per last 100 batches: 0.035800
Epoch: 12: Step: 2501/4907, loss=0.000116, lr=0.000014
Epoch: 12: Step: 2501/4907, loss=0.000116, lr=0.000014
Train batch 2600
Avg. loss per last 100 batches: 0.041026
Train batch 2600
Avg. loss per last 100 batches: 0.041026
Epoch: 12: Step: 2601/4907, loss=0.000003, lr=0.000014
Epoch: 12: Step: 2601/4907, loss=0.000003, lr=0.000014
Train batch 2700
Avg. loss per last 100 batches: 0.059638
Train batch 2700
Avg. loss per last 100 batches: 0.059638
Epoch: 12: Step: 2701/4907, loss=0.002911, lr=0.000014
Epoch: 12: Step: 2701/4907, loss=0.002911, lr=0.000014
Train batch 2800
Avg. loss per last 100 batches: 0.061307
Train batch 2800
Avg. loss per last 100 batches: 0.061307
Epoch: 12: Step: 2801/4907, loss=0.087639, lr=0.000014
Epoch: 12: Step: 2801/4907, loss=0.087639, lr=0.000014
Train batch 2900
Avg. loss per last 100 batches: 0.061501
Train batch 2900
Avg. loss per last 100 batches: 0.061501
Epoch: 12: Step: 2901/4907, loss=0.052640, lr=0.000014
Epoch: 12: Step: 2901/4907, loss=0.052640, lr=0.000014
Train batch 3000
Avg. loss per last 100 batches: 0.056937
Train batch 3000
Avg. loss per last 100 batches: 0.056937
Epoch: 12: Step: 3001/4907, loss=0.075360, lr=0.000014
Epoch: 12: Step: 3001/4907, loss=0.075360, lr=0.000014
Train batch 3100
Avg. loss per last 100 batches: 0.045703
Train batch 3100
Avg. loss per last 100 batches: 0.045703
Epoch: 12: Step: 3101/4907, loss=0.036153, lr=0.000014
Epoch: 12: Step: 3101/4907, loss=0.036153, lr=0.000014
Train batch 3200
Avg. loss per last 100 batches: 0.063043
Train batch 3200
Avg. loss per last 100 batches: 0.063043
Epoch: 12: Step: 3201/4907, loss=0.000109, lr=0.000014
Epoch: 12: Step: 3201/4907, loss=0.000109, lr=0.000014
Train batch 3300
Avg. loss per last 100 batches: 0.079993
Train batch 3300
Avg. loss per last 100 batches: 0.079993
Epoch: 12: Step: 3301/4907, loss=0.153865, lr=0.000014
Epoch: 12: Step: 3301/4907, loss=0.153865, lr=0.000014
Train batch 3400
Avg. loss per last 100 batches: 0.089038
Train batch 3400
Avg. loss per last 100 batches: 0.089038
Epoch: 12: Step: 3401/4907, loss=0.163339, lr=0.000014
Epoch: 12: Step: 3401/4907, loss=0.163339, lr=0.000014
Train batch 3500
Avg. loss per last 100 batches: 0.103295
Train batch 3500
Avg. loss per last 100 batches: 0.103295
Epoch: 12: Step: 3501/4907, loss=0.022000, lr=0.000014
Epoch: 12: Step: 3501/4907, loss=0.022000, lr=0.000014
Train batch 3600
Avg. loss per last 100 batches: 0.044664
Train batch 3600
Avg. loss per last 100 batches: 0.044664
Epoch: 12: Step: 3601/4907, loss=0.564945, lr=0.000014
Epoch: 12: Step: 3601/4907, loss=0.564945, lr=0.000014
Train batch 3700
Avg. loss per last 100 batches: 0.069840
Train batch 3700
Avg. loss per last 100 batches: 0.069840
Epoch: 12: Step: 3701/4907, loss=0.051575, lr=0.000014
Epoch: 12: Step: 3701/4907, loss=0.051575, lr=0.000014
Train batch 3800
Avg. loss per last 100 batches: 0.058523
Train batch 3800
Avg. loss per last 100 batches: 0.058523
Epoch: 12: Step: 3801/4907, loss=0.000500, lr=0.000014
Epoch: 12: Step: 3801/4907, loss=0.000500, lr=0.000014
Train batch 3900
Avg. loss per last 100 batches: 0.053695
Train batch 3900
Avg. loss per last 100 batches: 0.053695
Epoch: 12: Step: 3901/4907, loss=0.028615, lr=0.000014
Epoch: 12: Step: 3901/4907, loss=0.028615, lr=0.000014
Train batch 4000
Avg. loss per last 100 batches: 0.070014
Train batch 4000
Avg. loss per last 100 batches: 0.070014
Epoch: 12: Step: 4001/4907, loss=0.000153, lr=0.000014
Epoch: 12: Step: 4001/4907, loss=0.000153, lr=0.000014
Train batch 4100
Avg. loss per last 100 batches: 0.051709
Train batch 4100
Avg. loss per last 100 batches: 0.051709
Epoch: 12: Step: 4101/4907, loss=0.000002, lr=0.000014
Epoch: 12: Step: 4101/4907, loss=0.000002, lr=0.000014
Train batch 4200
Avg. loss per last 100 batches: 0.050759
Train batch 4200
Avg. loss per last 100 batches: 0.050759
Epoch: 12: Step: 4201/4907, loss=0.000085, lr=0.000014
Epoch: 12: Step: 4201/4907, loss=0.000085, lr=0.000014
Train batch 4300
Avg. loss per last 100 batches: 0.043624
Train batch 4300
Avg. loss per last 100 batches: 0.043624
Epoch: 12: Step: 4301/4907, loss=0.000002, lr=0.000014
Epoch: 12: Step: 4301/4907, loss=0.000002, lr=0.000014
Train batch 4400
Avg. loss per last 100 batches: 0.043285
Train batch 4400
Avg. loss per last 100 batches: 0.043285
Epoch: 12: Step: 4401/4907, loss=0.584432, lr=0.000014
Epoch: 12: Step: 4401/4907, loss=0.584432, lr=0.000014
Train batch 4500
Avg. loss per last 100 batches: 0.071847
Train batch 4500
Avg. loss per last 100 batches: 0.071847
Epoch: 12: Step: 4501/4907, loss=0.000019, lr=0.000014
Epoch: 12: Step: 4501/4907, loss=0.000019, lr=0.000014
Train batch 4600
Avg. loss per last 100 batches: 0.060204
Train batch 4600
Avg. loss per last 100 batches: 0.060204
Epoch: 12: Step: 4601/4907, loss=0.160092, lr=0.000014
Epoch: 12: Step: 4601/4907, loss=0.160092, lr=0.000014
Train batch 4700
Avg. loss per last 100 batches: 0.058352
Train batch 4700
Avg. loss per last 100 batches: 0.058352
Epoch: 12: Step: 4701/4907, loss=0.074308, lr=0.000014
Epoch: 12: Step: 4701/4907, loss=0.074308, lr=0.000014
Train batch 4800
Avg. loss per last 100 batches: 0.053635
Train batch 4800
Avg. loss per last 100 batches: 0.053635
Epoch: 12: Step: 4801/4907, loss=0.000080, lr=0.000014
Epoch: 12: Step: 4801/4907, loss=0.000080, lr=0.000014
Train batch 4900
Avg. loss per last 100 batches: 0.062613
Train batch 4900
Avg. loss per last 100 batches: 0.062613
Epoch: 12: Step: 4901/4907, loss=0.016783, lr=0.000014
Epoch: 12: Step: 4901/4907, loss=0.016783, lr=0.000014
Validation: Epoch: 12 Step: 4907/4907
Average rank validation ...
Validation: Epoch: 12 Step: 4907/4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
Av.rank validation: step 399, computed ctx_vectors 145881, q_vectors 2400
Av.rank validation: step 399, computed ctx_vectors 146005, q_vectors 2400
Av.rank validation: step 499, computed ctx_vectors 182372, q_vectors 3000
Av.rank validation: step 499, computed ctx_vectors 182509, q_vectors 3000
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198062, 768])
Av.rank validation: total q_vectors size=torch.Size([3258, 768])
Av.rank validation: total ctx_vectors size=torch.Size([198157, 768])
Av.rank validation: average rank 216.97022713321056, total questions=6516
Av.rank validation: average rank 216.97022713321056, total questions=6516
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Saved checkpoint at ../biencoders/trained/B+A/dpr_biencoder.12.4907
Saved checkpoint to ../biencoders/trained/B+A/dpr_biencoder.12.4907
Average rank validation ...
Reading file ../data/NQ/biencoder-nq-dev.json
Aggregated data size: 6515
Total cleaned data size: 6515
Av.rank validation: step 99, computed ctx_vectors 36506, q_vectors 600
Av.rank validation: step 99, computed ctx_vectors 36512, q_vectors 600
Av.rank validation: step 199, computed ctx_vectors 72987, q_vectors 1200
Av.rank validation: step 199, computed ctx_vectors 72968, q_vectors 1200
Av.rank validation: step 299, computed ctx_vectors 109390, q_vectors 1800
Av.rank validation: step 299, computed ctx_vectors 109425, q_vectors 1800
